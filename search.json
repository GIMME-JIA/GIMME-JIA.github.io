[{"title":"Docker","path":"/2024/03/06/Docker/","content":"基本介绍 Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的 Linux或Windows操作系统的机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。 就拿 MySQL 安装来举例，如果不是用 Docker，我们需要下载安装以及各种配置等繁琐的步骤；而使用 Docker 的话，当我们执行命令后，Docker 做的第一件事情，是去自动搜索并下载了MySQL，然后会自动运行MySQL，我们完全不用插手，非常方便。 而且，这种安装方式完全不用考虑运行的操作系统环境，它不仅仅在CentOS系统是这样，在Ubuntu系统、macOS系统、甚至是装了WSL的Windows下，都可以使用这条命令来安装MySQL。 要知道，不同操作系统下其安装包、运行环境是都不相同的！如果是手动安装，必须手动解决安装包不同、环境不同的、配置不同的问题！ 而使用Docker，这些完全不用考虑。就是因为Docker会自动搜索并下载MySQL。 注意：这里下载的不是安装包，而是镜像（image）。镜像中不仅包含了MySQL本身，还包含了其运行所需要的环境、配置、系统级函数库。因此它在运行时就有自己独立的环境，就可以跨系统运行，也不需要手动再次配置环境了。这套独立运行的隔离环境我们称为容器（container）。 常见命令 官方文档： https://docs.docker.com/engine/reference/commandline/cli/ 比较常见的命令有： 命令 说明 文档地址 docker pull 拉取镜像 docker pull docker push 推送镜像到DockerRegistry docker push docker images 查看本地镜像 docker images docker rmi 删除本地镜像 docker rmi docker run 创建并运行容器（不能重复创建） docker run docker stop 停止指定容器 docker stop docker start 启动指定容器 docker start docker restart 重新启动容器 docker restart docker rm 删除指定容器 docs.docker.com docker ps 查看容器 docker ps docker logs 查看容器运行日志 docker logs docker exec 进入容器 docker exec docker save 保存镜像到本地压缩文件 docker save docker load 加载本地压缩文件到镜像 docker load docker inspect 查看容器详细信息 docker inspect 用一副图来表示这些命令的关系： 以Nginx为例演示上述命令： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 第1步，去DockerHub查看nginx镜像仓库及相关信息# 第2步，拉取Nginx镜像docker pull nginx# 第3步，查看镜像docker images# 结果如下：REPOSITORY TAG IMAGE ID CREATED SIZEnginx latest 605c77e624dd 16 months ago 141MBmysql latest 3218b38490ce 17 months ago 516MB# 第4步，创建并允许Nginx容器docker run -d --name nginx -p 80:80 nginx# 第5步，查看运行中容器docker ps# 也可以加格式化方式访问，格式会更加清爽docker ps --format &quot;table &#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Image&#125;&#125;\\t&#123;&#123;.Ports&#125;&#125;\\t&#123;&#123;.Status&#125;&#125;\\t&#123;&#123;.Names&#125;&#125;&quot;# 第6步，访问网页，地址：http://虚拟机地址# 第7步，停止容器docker stop nginx# 第8步，查看所有容器docker ps -a --format &quot;table &#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Image&#125;&#125;\\t&#123;&#123;.Ports&#125;&#125;\\t&#123;&#123;.Status&#125;&#125;\\t&#123;&#123;.Names&#125;&#125;&quot;# 第9步，再次启动nginx容器docker start nginx# 第10步，再次查看容器docker ps --format &quot;table &#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Image&#125;&#125;\\t&#123;&#123;.Ports&#125;&#125;\\t&#123;&#123;.Status&#125;&#125;\\t&#123;&#123;.Names&#125;&#125;&quot;# 第11步，查看容器详细信息docker inspect nginx# 第12步，进入容器,查看容器内目录docker exec -it nginx bash# 或者，可以进入MySQLdocker exec -it mysql mysql -uroot -p# 第13步，删除容器docker rm nginx# 发现无法删除，因为容器运行中，强制删除容器docker rm -f nginx Docker 命令通常比较长，我们可以给常用Docker命令起别名，方便我们访问： 1234567891011121314151617# 修改/root/.bashrc文件vi /root/.bashrc内容如下：# .bashrc# User specific aliases and functionsalias rm=&#x27;rm -i&#x27;alias cp=&#x27;cp -i&#x27;alias mv=&#x27;mv -i&#x27;alias dps=&#x27;docker ps --format &quot;table &#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Image&#125;&#125;\\t&#123;&#123;.Ports&#125;&#125;\\t&#123;&#123;.Status&#125;&#125;\\t&#123;&#123;.Names&#125;&#125;&quot;&#x27;alias dis=&#x27;docker images&#x27;# Source global definitionsif [ -f /etc/bashrc ]; then . /etc/bashrcfi 然后，执行命令使别名生效： 1source /root/.bashrc 默认情况下，每次重启虚拟机我们都需要手动启动Docker和Docker中的容器。通过命令可以实现开机自启： 12345# Docker开机自启systemctl enable docker# Docker容器开机自启docker update --restart=always [容器名/容器id] 数据卷 概述 容器是隔离环境，容器内程序的文件、配置、运行时产生的容器都在容器内部，我们要读写容器内的文件非常不方便。大家思考几个问题： 如果要升级MySQL版本，需要销毁旧容器，那么数据岂不是跟着被销毁了？ MySQL、Nginx容器运行后，如果我要修改其中的某些配置该怎么办？ 我想要让Nginx代理我的静态资源怎么办？ 因此，容器提供程序的运行环境，但是程序运行产生的数据、程序运行依赖的配置都应该与容器解耦。 数据卷（volume）是一个虚拟目录，是容器内目录与宿主机目录之间映射的桥梁。 以Nginx为例，我们知道Nginx中有两个关键的目录： html：放置一些静态资源 conf：放置配置文件 如果我们要让Nginx代理我们的静态资源，最好是放到html目录；如果我们要修改Nginx的配置，最好是找到conf下的nginx.conf文件。 但遗憾的是，容器运行的Nginx所有的文件都在容器内部。所以我们必须利用数据卷将两个目录与宿主机目录关联，方便我们操作。如图： 在上图中： 我们创建了两个数据卷：conf、html Nginx容器内部的conf目录和html目录分别与两个数据卷关联。 而数据卷conf和html分别指向了宿主机的/var/lib/docker/volumes/conf/_data目录和/var/lib/docker/volumes/html/_data目录 这样一来，容器内的conf和html目录就 与宿主机的conf和html目录关联起来，我们称为挂载。此时，我们操作宿主机的 /var/lib/docker/volumes/html/_data就是在操作容器内的/usr/share/nginx/html/_data目录。只要我们将静态资源放入宿主机对应目录，就可以被Nginx代理了。 注意： /var/lib/docker/volumes这个目录就是默认的存放所有容器数据卷的目录，其下再根据数据卷名称创建新目录，格式为/数据卷名/_data。 为什么不让容器目录直接指向宿主机目录呢？ 因为直接指向宿主机目录就与宿主机强耦合了，如果切换了环境，宿主机目录就可能发生改变了。由于容器一旦创建，目录挂载就无法修改，这样容器就无法正常工作了。 但是容器指向数据卷，一个逻辑名称，而数据卷再指向宿主机目录，就不存在强耦合。如果宿主机目录发生改变，只要改变数据卷与宿主机目录之间的映射关系即可 不过，我们通过由于数据卷目录比较深，不好寻找，通常我们也允许让容器直接与宿主机目录挂载而不使用数据卷，具体参考挂载本地目录或文件部分。 常见命令 数据卷的相关命令有： 命令 说明 文档地址 docker volume create 创建数据卷 docker volume create docker volume ls 查看所有数据卷 docs.docker.com docker volume rm 删除指定数据卷 docs.docker.com docker volume inspect 查看某个数据卷的详情 docs.docker.com docker volume prune 清除数据卷 docker volume prune 注意：容器与数据卷的挂载要在创建容器时配置，对于创建好的容器，是不能设置数据卷的。而且创建容器的过程中，数据卷会自动创建。 案例演示： nginx的html目录挂载 12345678910111213141516171819202122232425262728293031323334353637383940# 1.首先创建容器并指定数据卷，注意通过 -v 参数来指定数据卷docker run -d --name nginx -p 80:80 -v html:/usr/share/nginx/html nginx# 2.然后查看数据卷docker volume ls# 结果DRIVER VOLUME NAMElocal 29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459flocal html# 3.查看数据卷详情docker volume inspect html# 结果[ &#123; &quot;CreatedAt&quot;: &quot;2024-05-17T19:57:08+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/html/_data&quot;, &quot;Name&quot;: &quot;html&quot;, &quot;Options&quot;: null, &quot;Scope&quot;: &quot;local&quot; &#125;]# 4.查看/var/lib/docker/volumes/html/_data目录ll /var/lib/docker/volumes/html/_data# 可以看到与nginx的html目录内容一样，结果如下：总用量 8-rw-r--r--. 1 root root 497 12月 28 2021 50x.html-rw-r--r--. 1 root root 615 12月 28 2021 index.html# 5.进入该目录，并随意修改index.html内容cd /var/lib/docker/volumes/html/_datavi index.html# 6.打开页面，查看效果# 7.进入容器内部，查看/usr/share/nginx/html目录内的文件是否变化docker exec -it nginx bash MySQL的匿名数据卷 123# 1.查看MySQL容器详细信息docker inspect mysql# 关注其中.Config.Volumes部分和.Mounts部分 关注两部分内容，第一是.Config.Volumes部分： 123456789&#123; &quot;Config&quot;: &#123; // ... 略 &quot;Volumes&quot;: &#123; &quot;/var/lib/mysql&quot;: &#123;&#125; &#125; // ... 略 &#125;&#125; 可以发现这个容器声明了一个本地目录，需要挂载数据卷，但是数据卷未定义。这就是匿名卷。 然后，我们再看结果中的.Mounts部分： 1234567891011&#123; &quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data&quot;, &quot;Destination&quot;: &quot;/var/lib/mysql&quot;, &quot;Driver&quot;: &quot;local&quot;, &#125; ]&#125; 可以发现，其中有几个关键属性： Name：数据卷名称。由于定义容器未设置容器名，这里的就是匿名卷自动生成的名字，一串hash值。 Source：宿主机目录 Destination : 容器内的目录 上述配置是将容器内的/var/lib/mysql这个目录，与数据卷29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f挂载。于是在宿主机中就有了/var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data这个目录。这就是匿名数据卷对应的目录，其使用方式与普通数据卷没有差别。 接下来，可以查看该目录下的MySQL的data文件： 1ls -l /var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data 注意：每一个不同的镜像，将来创建容器后内部有哪些目录可以挂载，可以参考DockerHub对应的页面。 挂载本地目录或文件 可以发现，数据卷的目录结构较深，如果我们去操作数据卷目录会不太方便。在很多情况下，我们会直接将容器目录与宿主机指定目录挂载。挂载语法与数据卷类似： 1234# 挂载本地目录-v 本地目录:容器内目录# 挂载本地文件-v 本地文件:容器内文件 注意：本地目录或文件必须以 / 或 ./开头，如果直接以名字开头，会被识别为数据卷名而非本地目录名。 例如： 12-v mysql:/var/lib/mysql # 会被识别为一个数据卷叫mysql，运行时会自动创建这个数据卷-v ./mysql:/var/lib/mysql # 会被识别为当前目录下的mysql目录，运行时如果不存在会创建目录 案例演示，删除并重新创建mysql容器，并完成本地目录挂载： 挂载/root/mysql/data到容器内的/var/lib/mysql目录 挂载/root/mysql/init到容器内的/docker-entrypoint-initdb.d目录（初始化的SQL脚本目录） 挂载/root/mysql/conf到容器内的/etc/mysql/conf.d目录（这个是MySQL配置文件目录） 在课前资料中已经准备好了mysql的init目录和conf目录： 以及对应的初始化SQL脚本和配置文件： 其中，hm.cnf主要是配置了MySQL的默认编码，改为utf8mb4；而hmall.sql则是后面我们要用到的黑马商城项目的初始化SQL脚本。 我们直接将整个mysql目录上传至虚拟机的/root目录下： 接下来，我们演示本地目录挂载： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# 1.删除原来的MySQL容器docker rm -f mysql# 2.进入root目录cd ~# 3.创建并运行新mysql容器，挂载本地目录docker run -d \\ --name mysql \\ -p 3306:3306 \\ -e TZ=Asia/Shanghai \\ -e MYSQL_ROOT_PASSWORD=123 \\ -v ./mysql/data:/var/lib/mysql \\ -v ./mysql/conf:/etc/mysql/conf.d \\ -v ./mysql/init:/docker-entrypoint-initdb.d \\ mysql# 4.查看root目录，可以发现~/mysql/data目录已经自动创建好了ls -l mysql# 结果：总用量 4drwxr-xr-x. 2 root root 20 5月 19 15:11 confdrwxr-xr-x. 7 polkitd root 4096 5月 19 15:11 datadrwxr-xr-x. 2 root root 23 5月 19 15:11 init# 查看data目录，会发现里面有大量数据库数据，说明数据库完成了初始化ls -l data# 5.查看MySQL容器内数据# 5.1.进入MySQLdocker exec -it mysql mysql -uroot -p123# 5.2.查看编码表show variables like &quot;%char%&quot;;# 5.3.结果，发现编码是utf8mb4没有问题+--------------------------+--------------------------------+| Variable_name | Value |+--------------------------+--------------------------------+| character_set_client | utf8mb4 || character_set_connection | utf8mb4 || character_set_database | utf8mb4 || character_set_filesystem | binary || character_set_results | utf8mb4 || character_set_server | utf8mb4 || character_set_system | utf8mb3 || character_sets_dir | /usr/share/mysql-8.0/charsets/ |+--------------------------+--------------------------------+# 6.查看数据# 6.1.查看数据库show databases;# 结果，hmall是黑马商城数据库+--------------------+| Database |+--------------------+| hmall || information_schema || mysql || performance_schema || sys |+--------------------+5 rows in set (0.00 sec)# 6.2.切换到hmall数据库use hmall;# 6.3.查看表show tables;# 结果：+-----------------+| Tables_in_hmall |+-----------------+| address || cart || item || order || order_detail || order_logistics || pay_order || user |+-----------------+# 6.4.查看address表数据+----+---------+----------+--------+----------+-------------+---------------+-----------+------------+-------+| id | user_id | province | city | town | mobile | street | contact | is_default | notes |+----+---------+----------+--------+----------+-------------+---------------+-----------+------------+-------+| 59 | 1 | 北京 | 北京 | 朝阳区 | 13900112222 | 金燕龙办公楼 | 李佳诚 | 0 | NULL || 60 | 1 | 北京 | 北京 | 朝阳区 | 13700221122 | 修正大厦 | 李佳红 | 0 | NULL || 61 | 1 | 上海 | 上海 | 浦东新区 | 13301212233 | 航头镇航头路 | 李佳星 | 1 | NULL || 63 | 1 | 广东 | 佛山 | 永春 | 13301212233 | 永春武馆 | 李晓龙 | 0 | NULL |+----+---------+----------+--------+----------+-------------+---------------+-----------+------------+-------+4 rows in set (0.00 sec) 镜像 前面我们一直在使用别人准备好的镜像，那如果我要部署一个Java项目，把它打包为一个镜像该怎么做呢？ 镜像结构 要想自己构建镜像，必须先了解镜像的结构。 之前我们说过，镜像之所以能让我们快速跨操作系统部署应用而忽略其运行环境、配置，就是因为镜像中包含了程序运行需要的系统函数库、环境、配置、依赖。 因此，自定义镜像本质就是依次准备好程序运行的基础环境、依赖、应用本身、运行配置等文件，并且打包而成。 举个例子，我们要从0部署一个Java应用，大概流程是这样： 准备一个linux服务（CentOS或者Ubuntu均可） 安装并配置JDK 上传Jar包 运行jar包 那因此，我们打包镜像也是分成这么几步： 准备Linux运行环境（java项目并不需要完整的操作系统，仅仅是基础运行环境即可） 安装并配置JDK 拷贝jar包 配置启动脚本 上述步骤中的每一次操作其实都是在生产一些文件（系统运行环境、函数库、配置最终都是磁盘文件），所以镜像就是一堆文件的集合。 但需要注意的是，镜像文件不是随意堆放的，而是按照操作的步骤分层叠加而成，每一层形成的文件都会单独打包并标记一个唯一id，称为Layer（层）。这样，如果我们构建时用到的某些层其他人已经制作过，就可以直接拷贝使用这些层，而不用重复制作。 例如，第一步中需要的Linux运行环境，通用性就很强，所以Docker官方就制作了这样的只包含Linux运行环境的镜像。我们在制作java镜像时，就无需重复制作，直接使用Docker官方提供的CentOS或Ubuntu镜像作为基础镜像。然后再搭建其它层即可，这样逐层搭建，最终整个Java项目的镜像结构如图所示： Dockerfile 由于制作镜像的过程中，需要逐层处理和打包，比较复杂，所以Docker就提供了自动打包镜像的功能。我们只需要将打包的过程，每一层要做的事情用固定的语法写下来，交给Docker去执行即可。 而这种记录镜像结构的文件就称为Dockerfile，其对应的语法可以参考官方文档： https://docs.docker.com/engine/reference/builder/ 其中的语法比较多，比较常用的有： 指令 说明 示例 FROM 指定基础镜像 FROM centos:6 ENV 设置环境变量，可在后面指令使用 ENV key value COPY 拷贝本地文件到镜像的指定目录 COPY ./xx.jar /tmp/app.jar RUN 执行Linux的shell命令，一般是安装过程的命令 RUN yum install gcc EXPOSE 指定容器运行时监听的端口，是给镜像使用者看的 EXPOSE 8080 ENTRYPOINT 镜像中应用的启动命令，容器运行时调用 ENTRYPOINT java -jar xx.jar 例如，要基于Ubuntu镜像来构建一个Java应用，其Dockerfile内容如下： 123456789101112131415161718192021# 指定基础镜像FROM ubuntu:16.04# 配置环境变量，JDK的安装目录、容器内时区ENV JAVA_DIR=/usr/localENV TZ=Asia/Shanghai# 拷贝jdk和java项目的包COPY ./jdk8.tar.gz $JAVA_DIR/COPY ./docker-demo.jar /tmp/app.jar# 设定时区RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone# 安装JDKRUN cd $JAVA_DIR \\ &amp;&amp; tar -xf ./jdk8.tar.gz \\ &amp;&amp; mv ./jdk1.8.0_144 ./java8# 配置环境变量ENV JAVA_HOME=$JAVA_DIR/java8ENV PATH=$PATH:$JAVA_HOME/bin# 指定项目监听的端口EXPOSE 8080# 入口，java项目的启动命令ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/app.jar&quot;] 思考一下：以后我们会有很多很多java项目需要打包为镜像，他们都需要Linux系统环境、JDK环境这两层，只有上面的3层不同（因为jar包不同）。如果每次制作java镜像都重复制作前两层镜像，是不是很麻烦。 所以，就有人提供了基础的系统加JDK环境，我们在此基础上制作java镜像，就可以省去JDK的配置了： 123456789# 基础镜像FROM openjdk:11.0-jre-buster# 设定时区ENV TZ=Asia/ShanghaiRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone# 拷贝jar包COPY docker-demo.jar /app.jar# 入口ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/app.jar&quot;] 构建镜像 当Dockerfile文件写好以后，就可以利用命令来构建镜像了。 在课前资料中，我们准备好了一个demo项目及对应的Dockerfile： 首先，我们将课前资料提供的docker-demo.jar包以及Dockerfile拷贝到虚拟机的/root/demo目录： 然后，执行命令，构建镜像： 1234# 进入镜像目录cd /root/demo# 开始构建docker build -t docker-demo:1.0 . 命令说明： docker build : 就是构建一个docker镜像 -t docker-demo:1.0 ：-t参数是指定镜像的名称（repository和tag） . : 最后的点是指构建时Dockerfile所在路径，由于我们进入了demo目录，所以指定的是.代表当前目录，也可以直接指定Dockerfile目录： 12# 直接指定Dockerfile目录docker build -t docker-demo:1.0 /root/demo 结果： 查看镜像列表： 1234567# 查看镜像列表：docker images# 结果REPOSITORY TAG IMAGE ID CREATED SIZEdocker-demo 1.0 d6ab0b9e64b9 27 minutes ago 327MBnginx latest 605c77e624dd 16 months ago 141MBmysql latest 3218b38490ce 17 months ago 516MB 然后尝试运行该镜像： 12345678910111213# 1.创建并运行容器docker run -d --name dd -p 8080:8080 docker-demo:1.0# 2.查看容器dps# 结果CONTAINER ID IMAGE PORTS STATUS NAMES78a000447b49 docker-demo:1.0 0.0.0.0:8080-&gt;8080/tcp, :::8090-&gt;8090/tcp Up 2 seconds ddf63cfead8502 mysql 0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcp Up 2 hours mysql# 3.访问curl localhost:8080/hello/count# 结果：&lt;h5&gt;欢迎访问黑马商城, 这是您第1次访问&lt;h5&gt; 网络 上面我们创建了一个Java项目的容器，而Java项目往往需要访问其它各种中间件，例如MySQL、Redis等。现在，我们的容器之间能否互相访问呢？我们来测试一下。 首先，我们查看下MySQL容器的详细信息，重点关注其中的网络IP地址： 1234567891011121314151617# 1.用基本命令，寻找Networks.bridge.IPAddress属性docker inspect mysql# 也可以使用format过滤结果docker inspect --format=&#x27;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;println .IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27; mysql# 得到IP地址如下：172.17.0.2# 2.然后通过命令进入dd容器docker exec -it dd bash# 3.在容器内，通过ping命令测试网络ping 172.17.0.2# 结果PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.053 ms64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.059 ms64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.058 ms 发现可以互联，没有问题。 但是，容器的网络IP其实是一个虚拟的IP，其值并不固定与某一个容器绑定，如果我们在开发时写死某个IP，而在部署时很可能MySQL容器的IP会发生变化，连接会失败。 所以，我们必须借助于docker的网络功能来解决这个问题，官方文档： https://docs.docker.com/engine/reference/commandline/network/ 常见命令有： 命令 说明 文档地址 docker network create 创建一个网络 docker network create docker network ls 查看所有网络 docs.docker.com docker network rm 删除指定网络 docs.docker.com docker network prune 清除未使用的网络 docs.docker.com docker network connect 使指定容器连接加入某网络 docs.docker.com docker network disconnect 使指定容器连接离开某网络 docker network disconnect docker network inspect 查看网络详细信息 docker network inspect 案例演示： 自定义网络 1234567891011121314151617181920212223242526272829303132333435# 1.首先通过命令创建一个网络docker network create hmall# 2.然后查看网络docker network ls# 结果：NETWORK ID NAME DRIVER SCOPE639bc44d0a87 bridge bridge local403f16ec62a2 hmall bridge local0dc0f72a0fbb host host localcd8d3e8df47b none null local# 其中，除了hmall以外，其它都是默认的网络# 3.让dd和mysql都加入该网络，注意，在加入网络时可以通过--alias给容器起别名# 这样该网络内的其它容器可以用别名互相访问！# 3.1.mysql容器，指定别名为db，另外每一个容器都有一个别名是容器名docker network connect hmall mysql --alias db# 3.2.db容器，也就是我们的java项目docker network connect hmall dd# 4.进入dd容器，尝试利用别名访问db# 4.1.进入容器docker exec -it dd bash# 4.2.用db别名访问ping db# 结果PING db (172.18.0.2) 56(84) bytes of data.64 bytes from mysql.hmall (172.18.0.2): icmp_seq=1 ttl=64 time=0.070 ms64 bytes from mysql.hmall (172.18.0.2): icmp_seq=2 ttl=64 time=0.056 ms# 4.3.用容器名访问ping mysql# 结果：PING mysql (172.18.0.2) 56(84) bytes of data.64 bytes from mysql.hmall (172.18.0.2): icmp_seq=1 ttl=64 time=0.044 ms64 bytes from mysql.hmall (172.18.0.2): icmp_seq=2 ttl=64 time=0.054 ms OK，现在无需记住IP地址也可以实现容器互联了。 总结： 在自定义网络中，可以给容器起多个别名，默认的别名是容器名本身 在同一个自定义网络中的容器，可以通过别名互相访问 项目部署 在掌握 Docker 的基本用法后，可以试着部署项目了。 项目说明： hmall：商城的后端代码 hmall-portal：商城用户端的前端代码 hmall-admin：商城管理端的前端代码 部署的容器及端口说明： 项目 容器名 端口 备注 hmall hmall 8080 黑马商城后端API入口 hmall-portal nginx 18080 黑马商城用户端入口 hmall-admin 18081 黑马商城管理端入口 mysql mysql 3306 数据库 在正式部署前，我们先删除之前的nginx、dd两个容器： 1docker rm -f nginx dd mysql容器中已经准备好了商城的数据，所以就不再删除了。 部署Java项目 hmall项目是一个maven聚合项目，使用IDEA打开hmall项目，查看项目结构如图： 我们要部署的就是其中的hm-service，其中的配置文件采用了多环境的方式： 其中的application-dev.yaml是部署到开发环境的配置，application-local.yaml是本地运行时的配置。 查看application.yaml，你会发现其中的JDBC地址并未写死，而是读取变量： 这两个变量在application-dev.yaml和application-local.yaml中并不相同： 在dev开发环境（也就是Docker部署时）采用了mysql作为地址，刚好是我们的mysql容器名，只要两者在一个网络，就一定能互相访问。 我们将项目打包： 结果： 将hm-service目录下的Dockerfile和hm-service/target目录下的hm-service.jar一起上传到虚拟机的root目录： 部署项目： 1234567891011121314# 1.构建项目镜像，不指定tag，则默认为latestdocker build -t hmall .# 2.查看镜像docker images# 结果REPOSITORY TAG IMAGE ID CREATED SIZEhmall latest 0bb07b2c34b9 43 seconds ago 362MBdocker-demo 1.0 49743484da68 24 hours ago 327MBnginx latest 605c77e624dd 16 months ago 141MBmysql latest 3218b38490ce 17 months ago 516MB# 3.创建并运行容器，并通过--network将其加入hmall网络，这样才能通过容器名访问mysqldocker run -d --name hmall --network hmall -p 8080:8080 hmall 测试，通过浏览器访问：http://虚拟机地址:8080/search/list。 部署前端 hmall-portal和hmall-admin是前端代码，需要基于nginx部署。在课前资料中已经提供了nginx的部署目录： 其中： html是静态资源目录，我们需要把hmall-portal以及hmall-admin都复制进去 nginx.conf是nginx的配置文件，主要是完成对html下的两个静态资源目录做代理 我们现在要做的就是把整个nginx目录上传到虚拟机的/root目录下： 然后创建nginx容器并完成两个挂载： 把/root/nginx/nginx.conf挂载到/etc/nginx/nginx.conf 把/root/nginx/html挂载到/usr/share/nginx/html 由于需要让nginx同时代理hmall-portal和hmall-admin两套前端资源，因此我们需要暴露两个端口： 18080：对应hmall-portal 18081：对应hmall-admin 命令如下： 12345678docker run -d \\ --name nginx \\ -p 18080:18080 \\ -p 18081:18081 \\ -v /root/nginx/html:/usr/share/nginx/html \\ -v /root/nginx/nginx.conf:/etc/nginx/nginx.conf \\ --network hmall \\ nginx 测试，通过浏览器访问：http://虚拟机ip:8080 DockerCompose 前言 上面可以看到，我们部署一个简单的java项目，其中包含3个容器： MySQL Nginx Java项目 而稍微复杂的项目，其中还会有各种各样的其它中间件，需要部署的东西远不止3个。如果还像之前那样手动的逐一部署，就太麻烦了。 而Docker Compose就可以帮助我们实现多个相互关联的Docker容器的快速部署。它允许用户通过一个单独的 docker-compose.yml模板文件（YAML 格式）来定义一组相关联的应用容器。 基本语法 docker-compose.yml文件的基本语法可以参考官方文档： https://docs.docker.com/compose/compose-file/compose-file-v3/ docker-compose文件中可以定义多个相互关联的应用容器，每一个应用容器被称为一个服务（service）。由于service就是在定义某个应用的运行时参数，因此与docker run参数非常相似。 举例来说，用docker run部署MySQL的命令如下： 12345678910docker run -d \\ --name mysql \\ -p 3306:3306 \\ -e TZ=Asia/Shanghai \\ -e MYSQL_ROOT_PASSWORD=123 \\ -v ./mysql/data:/var/lib/mysql \\ -v ./mysql/conf:/etc/mysql/conf.d \\ -v ./mysql/init:/docker-entrypoint-initdb.d \\ --network hmall mysql 如果用docker-compose.yml文件来定义，就是这样： 12345678910111213141516171819version: &quot;3.8&quot;services: mysql: image: mysql container_name: mysql ports: - &quot;3306:3306&quot; environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123 volumes: - &quot;./mysql/conf:/etc/mysql/conf.d&quot; - &quot;./mysql/data:/var/lib/mysql&quot; networks: - newnetworks: new: name: hmall 对比如下： docker run 参数 docker compose 指令 说明 –name container_name 容器名称 -p ports 端口映射 -e environment 环境变量 -v volumes 数据卷配置 –network networks 网络 明白了其中的对应关系，编写docker-compose文件就不难了。 黑马商城部署文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344version: &quot;3.8&quot;services: mysql: image: mysql container_name: mysql ports: - &quot;3306:3306&quot; environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123 volumes: - &quot;./mysql/conf:/etc/mysql/conf.d&quot; - &quot;./mysql/data:/var/lib/mysql&quot; - &quot;./mysql/init:/docker-entrypoint-initdb.d&quot; networks: - hm-net hmall: build: context: . dockerfile: Dockerfile container_name: hmall ports: - &quot;8080:8080&quot; networks: - hm-net depends_on: - mysql nginx: image: nginx container_name: nginx ports: - &quot;18080:18080&quot; - &quot;18081:18081&quot; volumes: - &quot;./nginx/nginx.conf:/etc/nginx/nginx.conf&quot; - &quot;./nginx/html:/usr/share/nginx/html&quot; depends_on: - hmall networks: - hm-netnetworks: hm-net: name: hmall 基础命令 编写好docker-compose.yml文件，就可以部署项目了。常见的命令： https://docs.docker.com/compose/reference/ 基本语法如下： 1docker compose [OPTIONS] [COMMAND] 其中，OPTIONS和COMMAND都是可选参数，比较常见的有： 类型 参数或指令 说明 Options -f 指定compose文件的路径和名称 -p 指定project名称。project就是当前compose文件中设置的多个service的集合，是逻辑概念 Commands up 创建并启动所有service容器 down 停止并移除所有容器、网络 ps 列出所有启动的容器 logs 查看指定容器的日志 stop 停止容器 start 启动容器 restart 重启容器 top 查看运行的进程 exec 在指定的运行中容器中执行命令 演示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 1.进入root目录cd /root# 2.删除旧容器docker rm -f $(docker ps -qa)# 3.删除hmall镜像docker rmi hmall# 4.清空MySQL数据rm -rf mysql/data# 5.启动所有, -d 参数是后台启动docker compose up -d# 结果：[+] Building 15.5s (8/8) FINISHED =&gt; [internal] load build definition from Dockerfile 0.0s =&gt; =&gt; transferring dockerfile: 358B 0.0s =&gt; [internal] load .dockerignore 0.0s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [internal] load metadata for docker.io/library/openjdk:11.0-jre-buster 15.4s =&gt; [1/3] FROM docker.io/library/openjdk:11.0-jre-buster@sha256:3546a17e6fb4ff4fa681c3 0.0s =&gt; [internal] load build context 0.0s =&gt; =&gt; transferring context: 98B 0.0s =&gt; CACHED [2/3] RUN ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo 0.0s =&gt; CACHED [3/3] COPY hm-service.jar /app.jar 0.0s =&gt; exporting to image 0.0s =&gt; =&gt; exporting layers 0.0s =&gt; =&gt; writing image sha256:32eebee16acde22550232f2eb80c69d2ce813ed099640e4cfed2193f71 0.0s =&gt; =&gt; naming to docker.io/library/root-hmall 0.0s[+] Running 4/4 ✔ Network hmall Created 0.2s ✔ Container mysql Started 0.5s ✔ Container hmall Started 0.9s ✔ Container nginx Started 1.5s# 6.查看镜像docker compose images# 结果CONTAINER REPOSITORY TAG IMAGE ID SIZEhmall root-hmall latest 32eebee16acd 362MBmysql mysql latest 3218b38490ce 516MBnginx nginx latest 605c77e624dd 141MB# 7.查看容器docker compose ps# 结果NAME IMAGE COMMAND SERVICE CREATED STATUS PORTShmall root-hmall &quot;java -jar /app.jar&quot; hmall 54 seconds ago Up 52 seconds 0.0.0.0:8080-&gt;8080/tcp, :::8080-&gt;8080/tcpmysql mysql &quot;docker-entrypoint.s…&quot; mysql 54 seconds ago Up 53 seconds 0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcpnginx nginx &quot;/docker-entrypoint.…&quot; nginx 54 seconds ago Up 52 seconds 80/tcp, 0.0.0.0:18080-18081-&gt;18080-18081/tcp, :::18080-18081-&gt;18080-18081/tcp 打开浏览器，访问：http://虚拟机 ip:8080，完成！","tags":["Docker"],"categories":["学习笔记"]},{"title":"MySQL--运维","path":"/2024/01/31/MySQL--运维/","content":"日志 日志分类 在任何一种数据库中，都会有各种各样的日志，记录着数据库工作的过程，可以帮助数据库管理员追踪数据库曾经发生过的各种事件。 MySQL日志主要包括六种： 重做日志（redo log） 回滚日志（undo log） 归档日志（binlog）（二进制日志） 错误日志（errorlog） 慢查询日志（slow query log） 一般查询日志（general log） 中继日志（relay log） 错误日志 错误日志是 MySQL 中最重要的日志之一，记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，可以首先查看此日志。 该日志是默认开启的，默认位置是：/var/log/mysql/error.log 查看指令： 1SHOW VARIABLES LIKE &#x27;log_error%&#x27;; 查看日志内容： 1tail -f /var/log/mysql/error.log 二进制日志 基本介绍 二进制日志（BINLOG）也叫归档日志，是因为采用二进制进行存储，记录了所有的 DDL（数据定义语言）语句和 DML（数据操作语言）语句，但不包括数据查询语句，在事务提交前的最后阶段写入。 作用：灾难时的数据恢复和 MySQL 的主从复制 归档日志默认情况下是没有开启的，需要在 MySQL 配置文件中开启，并配置 MySQL 日志的格式： 1234567cd /etc/mysqlvim my.cnf# 配置开启binlog日志， 日志的文件前缀为 mysqlbin -----&gt; 生成的文件名如: mysqlbin.000001log_bin=mysqlbin# 配置二进制日志的格式binlog_format=STATEMENT 日志存放位置：配置时给定了文件名但是没有指定路径，日志默认写入MySQL 的数据目录。 日志格式： STATEMENT：该日志格式在日志文件中记录的都是 SQL 语句，每一条对数据进行修改的 SQL 都会记录在日志文件中，通过 mysqlbinlog 工具，可以查看到每条语句的文本。主从复制时，从库会将日志解析为原语句，并在从库重新执行一遍缺点：可能会导致主备不一致，因为记录的 SQL 在不同的环境中可能选择的索引不同，导致结果不同 ROW：该日志格式在日志文件中记录的是每一行的数据变更，而不是记录 SQL 语句。比如执行 SQL 语句 update tb_book set status='1'，如果是 STATEMENT，在日志中会记录一行 SQL 语句； 如果是 ROW，由于是对全表进行更新，就是每一行记录都会发生变更，ROW 格式的日志中会记录每一行的数据变更缺点：记录的数据比较多，占用很多的存储空间 MIXED：这是 MySQL 默认的日志格式，混合了STATEMENT 和 ROW 两种格式，MIXED 格式能尽量利用两种模式的优点，而避开它们的缺点 日志刷盘 事务执行过程中，先将日志写（write）到 binlog cache，事务提交时再把 binlog cache 写（fsync）到 binlog 文件中，一个事务的 binlog 是不能被拆开的，所以不论这个事务多大也要确保一次性写入。 事务提交时执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。 write 和 fsync 的时机由参数 sync_binlog 控制的： sync_binlog=0：表示每次提交事务都只 write，不 fsync sync_binlog=1：表示每次提交事务都会执行 fsync sync_binlog=N(N&gt;1)：表示每次提交事务都 write，但累积 N 个事务后才 fsync，但是如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志 日志读取 日志文件存储位置：/var/lib/mysql 由于日志以二进制方式存储，不能直接读取，需要用 mysqlbinlog 工具来查看，语法如下： 1mysqlbinlog log-file; 查看 STATEMENT 格式日志： 执行插入语句：INSERT INTO tb_book VALUES(NULL,‘Lucene’,‘2088-05-01’,‘0’); cd /var/lib/mysql： 12-rw-r----- 1 mysql mysql 177 5月 23 21:08 mysqlbin.000001-rw-r----- 1 mysql mysql 18 5月 23 21:04 mysqlbin.index mysqlbin.index：该文件是日志索引文件 ， 记录日志的文件名； mysqlbing.000001：日志文件 查看日志内容：日志结尾有 COMMIT 1mysqlbinlog mysqlbing.000001; 查看 ROW 格式日志： 修改配置： 12# 配置二进制日志的格式binlog_format=ROW 插入数据： 1INSERT INTO tb_book VALUES(NULL,&#x27;SpringCloud实战&#x27;,&#x27;2088-05-05&#x27;,&#x27;0&#x27;); 查看日志内容：日志格式 ROW，直接查看数据是乱码，可以在 mysqlbinlog 后面加上参数 -vv 1mysqlbinlog -vv mysqlbin.000002 日志删除 对于比较繁忙的系统，生成日志量大，这些日志如果长时间不清除，将会占用大量的磁盘空间，需要删除日志 Reset Master 指令删除全部 binlog 日志，删除之后，日志编号将从 xxxx.000001重新开始 1Reset Master\t-- MySQL指令 执行指令 PURGE MASTER LOGS TO 'mysqlbin.***，该命令将删除 *** 编号之前的所有日志 执行指令 PURGE MASTER LOGS BEFORE 'yyyy-mm-dd hh:mm:ss' ，该命令将删除日志为 yyyy-mm-dd hh:mm:ss 之前产生的日志 设置参数 --expire_logs_days=#，此参数的含义是设置日志的过期天数，过了指定的天数后日志将会被自动删除，这样做有利于减少管理日志的工作量，配置 my.cnf 文件： 123log_bin=mysqlbinbinlog_format=ROW--expire_logs_days=3 数据恢复 误删库或者表时，需要根据 binlog 进行数据恢复。 一般情况下数据库有定时的全量备份，假如每天 0 点定时备份，12 点误删了库，恢复流程： 取最近一次全量备份，用备份恢复出一个临时库 从日志文件中取出凌晨 0 点之后的日志 把除了误删除数据的语句外日志，全部应用到临时库 跳过误删除语句日志的方法： 如果原实例没有使用 GTID 模式，只能在应用到包含 12 点的 binlog 文件的时候，先用 –stop-position 参数执行到误操作之前的日志，然后再用 –start-position 从误操作之后的日志继续执行 如果实例使用了 GTID 模式，假设误操作命令的 GTID 是 gtid1，那么只需要提交一个空事务先将这个 GTID 加到临时实例的 GTID 集合，之后按顺序执行 binlog 的时就会自动跳过误操作的语句 查询日志 查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的 SQL 语句。 默认情况下，查询日志是未开启的，因为对于一些业务繁忙的系统，该查询日志文件占用的内存将会变得非常大，一般不建议开启。如果需要开启查询日志，配置 my.cnf： 1234# 该选项用来开启查询日志，可选值0或者1，0代表关闭，1代表开启 general_log=1# 设置日志的文件名，如果没有指定，默认的文件名为host_name.log，存放在/var/lib/mysqlgeneral_log_file=mysql_query.log 配置完毕之后，在数据库执行以下操作： 1234SELECT * FROM tb_book;SELECT * FROM tb_book WHERE id = 1;UPDATE tb_book SET name = &#x27;lucene入门指南&#x27; WHERE id = 5;SELECT * FROM tb_book WHERE id &lt; 8 执行完毕之后， 再次来查询日志文件： 慢查询日志 慢查询日志记录所有执行时间超过 long_query_time 并且扫描记录数不小于 min_examined_row_limit 的所有的 SQL 语句的日志 long_query_time 默认为 10 秒，最小为 0， 精度到微秒。 慢查询日志默认是关闭的，可以通过两个参数来控制慢查询日志，配置文件 /etc/mysql/my.cnf： 12345678# 该参数用来控制慢查询日志是否开启，可选值0或者1，0代表关闭，1代表开启 slow_query_log=1 # 该参数用来指定慢查询日志的文件名，存放在 /var/lib/mysqlslow_query_log_file=slow_query.log# 该选项用来配置查询的时间限制，超过这个时间将认为值慢查询，将需要进行日志记录，默认10slong_query_time=10 日志读取： 直接通过 cat 指令查询该日志文件： 1cat slow_query.log 如果慢查询日志内容很多，直接查看文件比较繁琐，可以借助 mysql 自带的 mysqldumpslow 工具对慢查询日志进行分类汇总： 1mysqldumpslow slow_query.log 主从 主从复制 主从结构 主从复制是指将主数据库的 DDL 和 DML 操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。 MySQL 的主从之间维持了一个长连接。主库内部有一个线程，专门用于服务从库的长连接，连接过程： 从库执行 change master 命令，设置主库的 IP、端口、用户名、密码以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量 从库执行 start slave 命令，这时从库会启动两个线程，就是图中的 io_thread 和 sql_thread，其中 io_thread 负责与主库建立连接 主库校验完用户名、密码后，开始按照从传过来的位置，从本地读取 binlog 发给从库，开始主从复制 MySQL 支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库，实现链状复制。 MySQL 复制的优点主要包含以下三个方面： 主库出现问题，可以快速切换到从库提供服务 可以在从库上执行查询操作，从主库中更新，实现读写分离 可以在从库中执行备份，以避免备份期间影响主库的服务（备份时会加全局读锁） 实现原理： 主从复制主要依赖的是 binlog，MySQL 默认是异步复制，需要三个线程： binlog thread：在主库事务提交时，把数据变更记录在日志文件 binlog 中，并通知 slave 有数据更新 I/O thread：负责从主服务器上拉取二进制日志，并将 binlog 日志内容依次写到 relay log 中转日志的最末端，并将新的 binlog 文件名和 offset 记录到 master-info 文件中，以便下一次读取日志时从指定 binlog 日志文件及位置开始读取新的 binlog 日志内容 SQL thread：监测本地 relay log 中新增了日志内容，读取中继日志并重做其中的 SQL 语句，从库在 relay-log.info 中记录当前应用中继日志的文件名和位点以便下一次执行 同步与异步： 异步复制有数据丢失风险，例如数据还未同步到从库，主库就给客户端响应，然后主库挂了，此时从库晋升为主库的话数据是缺失的 同步复制，主库需要将 binlog 复制到所有从库，等所有从库响应了之后主库才进行其他逻辑，这样的话性能很差，一般不会选择 MySQL 5.7 之后出现了半同步复制，有参数可以选择成功同步几个从库就返回响应 主主结构 主主结构就是两个数据库之间总是互为主从关系，这样在切换的时候就不用再修改主从关系。 循环复制：在库 A 上更新了一条语句，然后把生成的 binlog 发给库 B，库 B 执行完这条更新语句后也会生成 binlog，会再发给 A。 解决方法： 两个库的 server id 必须不同，如果相同则它们之间不能设定为主主关系 一个库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog 每个库在收到从主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志 主从延迟 延迟原因 正常情况主库执行更新生成的所有 binlog，都可以传到从库并被正确地执行，从库就能达到跟主库一致的状态，这就是最终一致性。 主从延迟是主从之间是存在一定时间的数据不一致，就是同一个事务在从库执行完成的时间和主库执行完成的时间的差值，即 T2-T1。 主库 A 执行完成一个事务，写入 binlog，该时刻记为 T1 日志传给从库 B，从库 B 执行完这个事务，该时刻记为 T2 通过在从库执行 show slave status 命令，返回结果会显示 seconds_behind_master 表示当前从库延迟了多少秒。 每一个事务的 binlog 都有一个时间字段，用于记录主库上写入的时间 从库取出当前正在执行的事务的时间字段，跟系统的时间进行相减，得到的就是 seconds_behind_master 主从延迟的原因： 从库的机器性能比主库的差，导致从库的复制能力弱 从库的查询压力大，建立一主多从的结构 大事务的执行，主库必须要等到事务完成之后才会写入 binlog，导致从节点出现应用 binlog 延迟 主库的 DDL，从库与主库的 DDL 同步是串行进行，DDL 在主库执行时间很长，那么从库也会消耗同样的时间 锁冲突问题也可能导致从节点的 SQL 线程执行慢 主从同步问题永远都是一致性和性能的权衡，需要根据实际的应用场景，可以采取下面的办法： 优化 SQL，避免慢 SQL，减少批量操作 降低多线程大事务并发的概率，优化业务逻辑 业务中大多数情况查询操作要比更新操作更多，搭建一主多从结构，让这些从库来分担读的压力 尽量采用短的链路，主库和从库服务器的距离尽量要短，提升端口带宽，减少 binlog 传输的网络延时 实时性要求高的业务读强制走主库，从库只做备份 并行复制 MySQL5.6 高并发情况下，主库的会产生大量的 binlog，在从库中有两个线程 IO Thread 和 SQL Thread 单线程执行，会导致主库延迟变大。为了改善复制延迟问题，MySQL 5.6 版本增加了并行复制功能，以采用多线程机制来促进执行。 coordinator 就是原来的 SQL Thread，并行复制中它不再直接更新数据，只负责读取中转日志和分发事务： 线程分配完成并不是立即执行，为了防止造成更新覆盖，更新同一 DB 的两个事务必须被分发到同一个工作线程 同一个事务不能被拆开，必须放到同一个工作线程 MySQL 5.6 版本的策略：每个线程对应一个 hash 表，用于保存当前这个线程的执行队列里的事务所涉及的表，hash 表的 key 是数据库名，value 是一个数字，表示队列中有多少个事务修改这个库，适用于主库上有多个 DB 的情况。 每个事务在分发的时候，跟线程的冲突（事务操作的是同一个库）关系包括以下三种情况： 如果跟所有线程都不冲突，coordinator 线程就会把这个事务分配给最空闲的线程 如果只跟一个线程冲突，coordinator 线程就会把这个事务分配给这个存在冲突关系的线程 如果跟多于一个线程冲突，coordinator 线程就进入等待状态，直到和这个事务存在冲突关系的线程只剩下 1 个 优缺点： 构造 hash 值的时候很快，只需要库名，而且一个实例上 DB 数也不会很多，不会出现需要构造很多项的情况 不要求 binlog 的格式，statement 格式的 binlog 也可以很容易拿到库名（日志章节详解了 binlog） 主库上的表都放在同一个 DB 里面，这个策略就没有效果了；或者不同 DB 的热点不同，比如一个是业务逻辑库，一个是系统配置库，那也起不到并行的效果，需要把相同热度的表均匀分到这些不同的 DB 中，才可以使用这个策略 MySQL5.7 MySQL 5.7 由参数 slave-parallel-type 来控制并行复制策略： 配置为 DATABASE，表示使用 MySQL 5.6 版本的按库（DB）并行策略 配置为 LOGICAL_CLOCK，表示的按提交状态并行执行 按提交状态并行复制策略的思想是： 所有处于 commit 状态的事务可以并行执行；同时处于 prepare 状态的事务，在从库执行时是可以并行的 处于 prepare 状态的事务，与处于 commit 状态的事务之间，在从库执行时也是可以并行的 MySQL 5.7.22 版本里，MySQL 增加了一个新的并行复制策略，基于 WRITESET 的并行复制，新增了一个参数 binlog-transaction-dependency-tracking，用来控制是否启用这个新策略： COMMIT_ORDER：表示根据同时进入 prepare 和 commit 来判断是否可以并行的策略 WRITESET：表示的是对于每个事务涉及更新的每一行，计算出这一行的 hash 值，组成该事务的 writeset 集合，如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行（按行并行）为了唯一标识，这个 hash 表的值是通过 库名 + 表名 + 索引名 + 值（表示的是某一行）计算出来的 WRITESET_SESSION：是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序 MySQL 5.7.22 按行并发的优势： writeset 是在主库生成后直接写入到 binlog 里面的，这样在备库执行的时候，不需要解析 binlog 内容，节省了计算量 不需要把整个事务的 binlog 都扫一遍才能决定分发到哪个线程，更省内存 从库的分发策略不依赖于 binlog 内容，所以 binlog 是 statement 格式也可以，更节约内存（因为 row 才记录更改的行） MySQL 5.7.22 的并行复制策略在通用性上是有保证的，但是对于表上没主键、唯一和外键约束的场景，WRITESET 策略也是没法并行的，也会暂时退化为单线程模型。 主从搭建 master 在master 的配置文件（/etc/mysql/my.cnf）中，配置如下内容： 1234567891011121314151617181920212223242526#mysql 服务ID,保证整个集群环境中唯一server-id=1#mysql binlog 日志的存储路径和文件名log-bin=/var/lib/mysql/mysqlbin#错误日志,默认已经开启#log-err#mysql的安装目录#basedir#mysql的临时目录#tmpdir#mysql的数据存放目录#datadir#是否只读,1 代表只读, 0 代表读写read-only=0#忽略的数据, 指不需要同步的数据库binlog-ignore-db=mysql#指定同步的数据库#binlog-do-db=db01 执行完毕之后，需要重启 MySQL 创建同步数据的账户，并且进行授权操作： 12GRANT REPLICATION SLAVE ON *.* TO &#x27;seazean&#x27;@&#x27;192.168.0.137&#x27; IDENTIFIED BY &#x27;123456&#x27;;FLUSH PRIVILEGES; 查看 master 状态：SHOW MASTER STATUS; File：从哪个日志文件开始推送日志文件 Position：从哪个位置开始推送日志 Binlog_Ignore_DB：指定不需要同步的数据库 slave 在 slave 端配置文件中，配置如下内容： 12345#mysql服务端ID,唯一server-id=2#指定binlog日志log-bin=/var/lib/mysql/mysqlbin 执行完毕之后，需要重启 MySQL 指定当前从库对应的主库的IP地址、用户名、密码，从哪个日志文件开始的那个位置开始同步推送日志 1CHANGE MASTER TO MASTER_HOST= &#x27;192.168.0.138&#x27;, MASTER_USER=&#x27;seazean&#x27;, MASTER_PASSWORD=&#x27;seazean&#x27;, MASTER_LOG_FILE=&#x27;mysqlbin.000001&#x27;, MASTER_LOG_POS=413; 开启同步操作： 12START SLAVE;SHOW SLAVE STATUS; 停止同步操作： 1STOP SLAVE; 验证 在主库中创建数据库，创建表并插入数据： 123456789101112CREATE DATABASE db01;USE db01;CREATE TABLE user( id INT(11) NOT NULL AUTO_INCREMENT, name VARCHAR(50) NOT NULL, sex VARCHAR(1), PRIMARY KEY (id))ENGINE=INNODB DEFAULT CHARSET=utf8;INSERT INTO user(id,NAME,sex) VALUES(NULL,&#x27;Tom&#x27;,&#x27;1&#x27;);INSERT INTO user(id,NAME,sex) VALUES(NULL,&#x27;Trigger&#x27;,&#x27;0&#x27;);INSERT INTO user(id,NAME,sex) VALUES(NULL,&#x27;Dawn&#x27;,&#x27;1&#x27;); 在从库中查询数据，进行验证：在从库中，可以查看到刚才创建的数据库： 在该数据库中，查询表中的数据： 主从切换 正常切换 正常切换步骤： 在开始切换之前先对主库进行锁表 flush tables with read lock，然后等待所有语句执行完成，切换完成后可以释放锁 检查 slave 同步状态，在 slave 执行 show processlist 停止 slave io 线程，执行命令 STOP SLAVE IO_THREAD 提升 slave 为 master 1234Stop slave;Reset master;Reset slave all;set global read_only=off;\t-- 设置为可更新状态 将原来 master 变为 slave（参考搭建流程中的 slave 方法） 可靠性优先策略： 判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步 把主库 A 改成只读状态，即把 readonly 设置为 true 判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止（该步骤比较耗时，所以步骤 1 中要尽量等待该值变小） 把备库 B 改成可读写状态，也就是把 readonly 设置为 false 把业务请求切到备库 B 可用性优先策略：先做最后两步，会造成主备数据不一致的问题 健康检测 主库发生故障后从库会上位，其他从库指向新的主库，所以需要一个健康检测的机制来判断主库是否宕机 select 1 判断，但是高并发下检测不出线程的锁等待的阻塞问题 查表判断，在系统库（mysql 库）里创建一个表，比如命名为 health_check，里面只放一行数据，然后定期执行。但是当 binlog 所在磁盘的空间占用率达到 100%，所有的更新和事务提交语句都被阻塞，查询语句可以继续运行 更新判断，在健康检测表中放一个 timestamp 字段，用来表示最后一次执行检测的时间 1UPDATE mysql.health_check SET t_modified=now(); 节点可用性的检测都应该包含主库和备库，为了让主备之间的更新不产生冲突，可以在 mysql.health_check 表上存入多行数据，并用主备的 server_id 做主键，保证主、备库各自的检测命令不会发生冲突。 基于位点 主库上位后，从库 B 执行 CHANGE MASTER TO 命令，指定 MASTER_LOG_FILE、MASTER_LOG_POS 表示从新主库 A 的哪个文件的哪个位点开始同步，这个位置就是同步位点，对应主库的文件名和日志偏移量。 寻找位点需要找一个稍微往前的，然后再通过判断跳过那些在从库 B 上已经执行过的事务，获取位点方法： 等待新主库 A 把中转日志（relay log）全部同步完成 在 A 上执行 show master status 命令，得到当前 A 上最新的 File 和 Position 取原主库故障的时刻 T，用 mysqlbinlog 工具解析新主库 A 的 File，得到 T 时刻的位点 通常情况下该值并不准确，在切换的过程中会发生错误，所以要先主动跳过这些错误： 切换过程中，可能会重复执行一个事务，所以需要主动跳过所有重复的事务 12SET GLOBAL sql_slave_skip_counter=1;START SLAVE; 设置 slave_skip_errors 参数，直接设置跳过指定的错误，保证主从切换的正常进行 1062 错误是插入数据时唯一键冲突 1032 错误是删除数据时找不到行 该方法针对的是主备切换时，由于找不到精确的同步位点，只能采用这种方法来创建从库和新主库的主备关系。等到主备间的同步关系建立完成并稳定执行一段时间后，还需要把这个参数设置为空，以免真的出现了主从数据不一致也跳过了 基于GTID GTID GTID 的全称是 Global Transaction Identifier，全局事务 ID，是一个事务在提交时生成的，是这个事务的唯一标识，组成： 1GTID=source_id:transaction_id source_id：是一个实例第一次启动时自动生成的，是一个全局唯一的值 transaction_id：初始值是 1，每次提交事务的时候分配给这个事务，并加 1，是连续的（区分事务 ID，事务 ID 是在执行时生成） 启动 MySQL 实例时，加上参数 gtid_mode=on 和 enforce_gtid_consistency=on 就可以启动 GTID 模式，每个事务都会和一个 GTID 一一对应，每个 MySQL 实例都维护了一个 GTID 集合，用来存储当前实例执行过的所有事务。 GTID 有两种生成方式，使用哪种方式取决于 session 变量 gtid_next： gtid_next=automatic：使用默认值，把 source_id:transaction_id （递增）分配给这个事务，然后加入本实例的 GTID 集合@@SESSION.GTID_NEXT = ‘source_id:transaction_id’; gtid_next=GTID：指定的 GTID 的值，如果该值已经存在于实例的 GTID 集合中，接下来执行的事务会直接被系统忽略；反之就将该值分配给接下来要执行的事务，系统不需要给这个事务生成新的 GTID，也不用加 1注意：一个 GTID 只能给一个事务使用，所以执行下一个事务，要把 gtid_next 设置成另外一个 GTID 或者 automatic 业务场景： 主库 X 和从库 Y 执行一条相同的指令后进行事务同步 1INSERT INTO t VALUES(1,1); 当 Y 同步 X 时，会出现主键冲突，导致实例 X 的同步线程停止，解决方法： 12345SET gtid_next=&#x27;(这里是主库 X 的 GTID 值)&#x27;;BEGIN;COMMIT;SET gtid_next=automatic;START SLAVE; 前三条语句通过提交一个空事务，把 X 的 GTID 加到实例 Y 的 GTID 集合中，实例 Y 就会直接跳过这个事务。 切换 在 GTID 模式下，CHANGE MASTER TO 不需要指定日志名和日志偏移量，指定 master_auto_position=1 代表使用 GTID 模式 新主库实例 A 的 GTID 集合记为 set_a，从库实例 B 的 GTID 集合记为 set_b，主备切换逻辑： 实例 B 指定主库 A，基于主备协议建立连接，实例 B 并把 set_b 发给主库 A 实例 A 算出 set_a 与 set_b 的差集，就是所有存在于 set_a 但不存在于 set_b 的 GTID 的集合，判断 A 本地是否包含了这个差集需要的所有 binlog 事务 如果不包含，表示 A 已经把实例 B 需要的 binlog 给删掉了，直接返回错误 如果确认全部包含，A 从自己的 binlog 文件里面，找出第一个不在 set_b 的事务，发给 B 实例 A 之后就从这个事务开始，往后读文件，按顺序取 binlog 发给 B 去执行 分库分表 基本介绍 随着应用系统的数据量也是成指数式增长，若采用单数据库进行数据存储，存在以下性能瓶颈： IO瓶颈：热点数据太多，数据库缓存不足，产生大量磁盘IO，效率较低。 请求数据太多，带宽不够，网络IO瓶颈 CPU瓶颈：排序、分组、连接查询、聚合统计等SQL会耗费大量的CPU资源，请求数太多，CPU出现瓶颈 为了解决上述问题，我们需要对数据库进行分库分表处理。 分库分表的中心思想都是将数据分散存储，使得单一数据库/表的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的。 拆分策略 分库分表的形式，主要是两种：垂直拆分和水平拆分。而拆分的粒度，一般又分为分库和分表，所以组成的拆分策略最终如下： 垂直拆分 垂直分库：以表为依据，根据业务将不同表拆分到不同库中 特点： 每个库的表结构都不一样 每个库的数据也不一样 所有库的并集是全量数据 垂直分表：以字段为依据，根据字段属性将不同字段拆分到不同表中 特点： 每个表的结构都不一样 每个表的数据也不一样，一般通过一列（主键/外键）关联 所有表的并集是全量数据 水平拆分 水平分库：以字段为依据，按照一定策略，将一个库的数据拆分到多个库中 特点： 每个库的表结构都一样 每个库的数据都不一样 所有库的并集是全量数据 水平分表：以字段为依据，按照一定策略，将一个表的数据拆分到多个表中 特点： 每个表的表结构都一样 每个表的数据都不一样 所有表的并集是全量数据 在业务系统中，为了缓解磁盘IO及CPU的性能瓶颈，到底是垂直拆分，还是水平拆分；具体是分库，还是分表，都需要根据具体的业务需求具体分析。 实现技术 shardingJDBC：基于AOP原理，在应用程序中对本地执行的SQL进行拦截，解析、改写、路由处理。需要自行编码配置实现，只支持java语言，性能较高。 MyCat：数据库分库分表中间件，不用调整代码即可实现分库分表，支持多种语言，性能不及前者。 MyCat 概述 Mycat是开源的、活跃的、基于Java语言编写的MySQL数据库中间件。可以像使用mysql一样来使用 mycat，对于开发人员来说根本感觉不到mycat的存在。 开发人员只需要连接MyCat即可，而具体底层用到几台数据库，每一台数据库服务器里面存储了什么数据，都无需关心。 具体的分库分表的策略，只需要在MyCat中配置即可。 下载地址：http://dl.mycat.org.cn/ 目录介绍： bin : 存放可执行文件，用于启动停止mycat conf：存放mycat的配置文件 lib：存放mycat的项目依赖包（jar） logs：存放mycat的日志文件 在MyCat的整体结构中，分为两个部分：上面的逻辑结构、下面的物理结构。 在MyCat的逻辑结构主要负责逻辑库、逻辑表、分片规则、分片节点等逻辑结构的处理，而具体的数据存储还是在物理结构，也就是数据库服务器中存储的。后文有所解释。 配置 schema.xml schema.xml 作为MyCat中最重要的配置文件之一 , 涵盖了MyCat的逻辑库、逻辑表、分片规 则、分片节点及数据源的配置。 主要包含以下三组标签： schema标签 datanode标签 datahost标签 schema标签 schema 定义逻辑库 schema 标签用于定义 MyCat实例中的逻辑库 , 一个MyCat实例中, 可以有多个逻辑库 , 可以通过 schema 标签来划分不同的逻辑库。MyCat 中的逻辑库的概念，等同于 MySQL 中的 database 概念, 需要操作某个逻辑库下的表时, 也需要切换逻辑库(use xxx)。 核心属性： name：指定自定义的逻辑库库名 checkSQLschema：在SQL语句操作时指定了数据库名称，执行时是否自动去除；true：自动去除，false：不自动去除 sqlMaxLimit：如果未指定limit进行查询，列表查询模式查询多少条记录 schema 中的table定义逻辑表 table 标签定义了MyCat中逻辑库schema下的逻辑表 , 所有需要拆分的表都需要在table标签中定 义 。 核心属性： name：定义逻辑表表名，在该逻辑库下唯一 dataNode：定义逻辑表所属的dataNode，该属性需要与dataNode标签中name对应；多个 dataNode逗号分隔 rule：分片规则的名字，分片规则名字是在rule.xml中定义的 primaryKey：逻辑表对应真实表的主键 type：逻辑表的类型，目前逻辑表只有全局表和普通表，如果未配置，就是普通表；全局表，配置为 global datanode 标签 核心属性： name：定义数据节点名称 dataHost：数据库实例主机名称，引用自 dataHost 标签中name属性 database：定义分片所属数据库 datahost标签 该标签在MyCat逻辑库中作为底层标签存在, 直接定义了具体的数据库实例、读写分离、心跳语句。 核心属性： name：唯一标识，供上层标签使用 maxCon/minCon：最大连接数/最小连接数 balance：负载均衡策略，取值 0,1,2,3 writeType：写操作分发方式（0：写操作转发到第一个writeHost，第一个挂了，切换到第二 个；1：写操作随机分发到配置的writeHost） dbDriver：数据库驱动，支持 native、jdbc rule.xml rule.xml 中定义所有拆分表的规则, 在使用过程中可以灵活的使用分片算法, 或者对同一个分片算法使用不同的参数, 它让分片过程可配置化。主要包含两类标签：tableRule、Function。 server.xml server.xml配置文件包含了MyCat的系统配置信息，主要有两个重要的标签：system、user。 system标签 主要配置MyCat中的系统配置信息，对应的系统配置项及其含义，如下： 属性 取值 含义 charset utf8 设置Mycat的字符集, 字符集需要与MySQL的字符集保持一致 nonePasswordLogin 0,1 0为需要密码登陆、1为不需要密码登陆 ,默认为0，设置为1则需要指定默认账户 useHandshakeV10 0,1 使用该选项主要的目的是为了能够兼容高版本的jdbc驱动, 是否采用HandshakeV10Packet来与client进行通信, 1:是, 0:否 useSqlStat 0,1 开启SQL实时统计, 1 为开启 , 0 为关闭 ; 开启之后, MyCat会自动统计SQL语句的执行情况 ; mysql -h 127.0.0.1 -P 9066 -u root -p 查看MyCat执行的SQL, 执行效率比较低的SQL , SQL的整体执行情况、读写比例等 ; show @@sql ; show @@sql.slow ; show @@sql.sum ; useGlobleTableCheck 0,1 是否开启全局表的一致性检测。1为开启 ，0为关闭 。 sqlExecuteTimeout 1000 SQL语句执行的超时时间 , 单位为 s ; sequnceHandlerType 0,1,2 用来指定Mycat全局序列类型，0 为本地文件，1 为数据库方式，2 为时间戳列方式，默认使用本地文件方式，文件方式主要用于测试 sequnceHandlerPattern 正则表达式 必须带有MYCATSEQ或者 mycatseq进入序列匹配流程 注意MYCATSEQ_有空格的情况 subqueryRelationshipCheck true,false 子查询中存在关联查询的情况下,检查关联字段中是否有分片字段 .默认 false useCompression 0,1 开启mysql压缩协议 , 0 : 关闭, 1 : 开启 fakeMySQLVersion 5.5,5.6 设置模拟的MySQL版本号 defaultSqlParser 由于MyCat的最初版本使用了FoundationDB的SQL解析器, 在MyCat1.3后增加了Druid解析器, 所以要设置defaultSqlParser属性来指定默认的解析器; 解析器有两个 : druidparser 和 fdbparser, 在MyCat1.4之后,默认是druidparser, fdbparser已经废除了 processors 1,2… 指定系统可用的线程数量, 默认值为CPU核心 x 每个核心运行线程数量; processors 会影响processorBufferPool, processorBufferLocalPercent, processorExecutor属性, 所有, 在性能调优时, 可以适当地修改processors值 processorBufferChunk 指定每次分配Socket Direct Buffer默认值为4096字节, 也会影响BufferPool长度, 如果一次性获取字节过多而导致buffer不够用, 则会出现警告, 可以调大该值 processorExecutor 指定NIOProcessor上共享 businessExecutor固定线程池的大小; MyCat把异步任务交给 businessExecutor线程池中, 在新版本的MyCat中这个连接池使用频次不高, 可以适当地把该值调小 packetHeaderSize 指定MySQL协议中的报文头长度, 默认4个字节 maxPacketSize 指定MySQL协议可以携带的数据最大大小, 默认值为16M idleTimeout 30 指定连接的空闲时间的超时长度;如果超时,将关闭资源并回收, 默认30分钟 txIsolation 1,2,3,4 初始化前端连接的事务隔离级别,默认为 REPEATED_READ , 对应数字为3 READ_UNCOMMITED=1; READ_COMMITTED=2; REPEATED_READ=3; SERIALIZABLE=4; sqlExecuteTimeout 300 执行SQL的超时时间, 如果SQL语句执行超时,将关闭连接; 默认300秒; serverPort 8066 定义MyCat的使用端口, 默认8066 managerPort 9066 定义MyCat的管理端口, 默认9066 user标签 配置MyCat中的用户、访问密码，以及用户针对于逻辑库、逻辑表的权限信息，具体的权限描述方式及配置说明如下： 在测试权限操作时，我们只需要将 privileges 标签的注释放开。 在 privileges 下的schema 标签中配置的dml属性配置的是逻辑库的权限。 在privileges的schema下的table标签的dml属性中配置逻辑表的权限。 全局表 全局表（Global Temporary Table）是一种临时表，它是在所有的会话之间都可以访问的临时表。全局表在创建时使用的是CREATE GLOBAL TEMPORARY TABLE语句，而不是普通的CREATE TABLE语句。 全局表的特点包括： 可以被所有的会话访问：全局表在创建后，所有的会话都可以访问和操作这个表，而不像普通的临时表一样只能在创建它的会话中访问。 数据在会话结束时自动删除：和普通的临时表一样，全局表的数据也会在会话结束时自动删除，不会对其他会话产生影响。 可以共享数据：全局表可以用于在不同的会话之间共享数据，这在某些特定的场景下可能会有用。 分片规则 范围分片 根据数据的范围将数据分散存储到不同的节点中。范围分片能够实现较好的局部性，但可能会导致数据分布不均匀。 配置属性含义： 取模分片 根据指定的字段值与节点数量进行求模运算，根据运算结果， 来决定该数据属于哪一个分片。该分片规则，主要是针对于数字类型的字段适用。 在前面水平拆分的演示中，我们选择的就是取模分片。 属性说明如下： 一致性 hash 分片 所谓一致性哈希，相同的哈希因子计算值总是被划分到相同的分区表中，不会因为分区节点的增加而改变原来数据的分区位置，有效的解决了分布式数据的拓容问题。 属性含义： 枚举分片 通过在配置文件中配置可能的枚举值, 指定数据分布到不同数据节点上, 本规则适用于按照省份、性别、状态拆分数据等业务。 分片规则属性含义： 应用指定算法分片 运行阶段由应用自主决定路由到那个分片 , 直接根据字符子串（必须是数字）计算分片号。 分片规则属性含义： 固定分片 hash 算法分片 该算法类似于十进制的求模运算，但是为二进制的操作，例如，取 id 的二进制低 10 位 与1111111111 进行位 &amp; 运算，位与运算最小值为 0000000000，最大值为1111111111，转换为十进制，也就是位于0-1023之间。 特点： 如果是求模，连续的值，分别分配到各个不同的分片；但是此算法会将连续的值可能分配到相同的分片，降低事务处理的难度。 可以均匀分配，也可以非均匀分配。 分片字段必须为数字类型。 分片规则属性含义： 约束 : 分片长度 : 默认最大2^10 , 为 1024 ; count, length的数组长度必须是一致的 ; 以上分为三个分区:0-255,256-511,512-1023 。 字符串 hash 解析分片 截取字符串中的指定位置的子字符串, 进行hash算法， 算出分片。 分片规则属性含义： 示例说明： 按天分片算法 按照日期及对应的时间周期来分片。 分片规则属性含义： 自然月分片 使用场景为按照月份来分片, 每个自然月为一个分片。 分片规则属性含义： 管理和监控 MyCat原理 在MyCat中，当执行一条SQL语句时，MyCat需要进行SQL解析、分片分析、路由分析、读写分离分析等操作，最终经过一系列的分析决定将当前的SQL语句到底路由到那几个(或哪一个)节点数据库，数据库将数据执行完毕后，如果有返回的结果，则将结果返回给MyCat，最终还需要在MyCat中进行结果合并、聚合处理、排序处理、分页处理等操作，最终再将结果返回给客户端。 而在MyCat的使用过程中，MyCat官方也提供了一个管理监控平台MyCat-Web（MyCat-eye）。Mycat-web 是 Mycat 可视化运维的管理和监控平台，弥补了 Mycat 在监控上的空白。帮 Mycat 分担统计任务和配置管理任务。Mycat-web 引入了 ZooKeeper 作为配置中心，可以管理多个节点。Mycat-web 主要管理和监控 Mycat 的流量、连接、活动线程和内存等，具备 IP 白名单、邮件告警等模块，还可以统计 SQL 并分析慢 SQL 和高频 SQL 等。为优化 SQL 提供依据。 MyCat管理 Mycat默认开通2个端口，可以在server.xml中进行修改。 8066 数据访问端口，即进行 DML 和 DDL 操作 9066 数据库管理端口，即 mycat 服务管理控制功能，用于管理mycat的整个集群状态 连接MyCat的管理控制台： 1mysql -h [主机ip] -p 9066 -uroot -p[密码] 操作指令： 除了命令行管理， Mycat-web(Mycat-eye) 也是对mycat-server提供监控服务，功能不局限于对mycat-server使用。他通过JDBC连接对Mycat、Mysql监控，监控远程服务器(目前仅限于linux系统)的cpu、内存、网络、磁盘。 Mycat-eye运行过程中需要依赖zookeeper，因此需要先安装zookeeper。对此了解即可，这里不过多赘述。 读写分离 读写延迟 读写分离：可以降低主库的访问压力，提高系统的并发能力。 主库不建查询的索引，从库建查询的索引。因为索引需要维护的，比如插入一条数据，不仅要在聚簇索引上面插入，对应的二级索引也得插入 将读操作分到从库了之后，可以在主库把查询要用的索引删了，减少写操作对主库的影响 读写分离产生了读写延迟，造成数据的不一致性。假如客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，可能读到的还是以前的数据，叫过期读。 解决方案： 强制将写之后立刻读的操作转移到主库，比如刚注册的用户，直接登录从库查询可能查询不到，先走主库登录 二次查询，如果从库查不到数据，则再去主库查一遍，由 API 封装，比较简单，但导致主库压力大 更新主库后，读从库之前先 sleep 一下，类似于执行一条 select sleep(1) 命令，大多数情况下主备延迟在 1 秒之内 确保机制 无延迟 确保主备无延迟的方法： 每次从库执行查询请求前，先判断 seconds_behind_master 是否已经等于 0，如果不等于那就等到参数变为 0 执行查询请求 对比位点，Master_Log_File 和 Read_Master_Log_Pos 表示的是读到的主库的最新位点，Relay_Master_Log_File 和 Exec_Master_Log_Pos 表示的是备库执行的最新位点，这两组值完全相同就说明接收到的日志已经同步完成 对比 GTID 集合，Retrieved_Gtid_Set 是备库收到的所有日志的 GTID 集合，Executed_Gtid_Set 是备库所有已经执行完成的 GTID 集合，如果这两个集合相同也表示备库接收到的日志都已经同步完成 半同步 半同步复制就是 semi-sync replication，适用于一主一备的场景，工作流程： 事务提交的时候，主库把 binlog 发给从库 从库收到 binlog 以后，发回给主库一个 ack，表示收到了 主库收到这个 ack 以后，才能给客户端返回事务完成的确认 在一主多从场景中，主库只要等到一个从库的 ack，就开始给客户端返回确认，这时在从库上执行查询请求，有两种情况： 如果查询是落在这个响应了 ack 的从库上，是能够确保读到最新数据 如果查询落到其他从库上，它们可能还没有收到最新的日志，就会产生过期读的问题 在业务更新的高峰期，主库的位点或者 GTID 集合更新很快，导致从库来不及处理，那么两个位点等值判断就会一直不成立，很可能出现从库上迟迟无法响应查询请求的情况。 等位点 在从库执行判断位点的命令，参数 file 和 pos 指的是主库上的文件名和位置，timeout 可选，设置为正整数 N 表示最多等待 N 秒。 1SELECT master_pos_wait(file, pos[, timeout]); 命令正常返回的结果是一个正整数 M，表示从命令开始执行，到应用完 file 和 pos 表示的 binlog 位置，执行了多少事务。 如果执行期间，备库同步线程发生异常，则返回 NULL 如果等待超过 N 秒，就返回 -1 如果刚开始执行的时候，就发现已经执行过这个位置了，则返回 0 工作流程：先执行 trx1，再执行一个查询请求的逻辑，要保证能够查到正确的数据。 trx1 事务更新完成后，马上执行 show master status 得到当前主库执行到的 File 和 Position 选定一个从库执行判断位点语句，如果返回值是 &gt;=0 的正整数，说明从库已经同步完事务，可以在这个从库执行查询语句 如果出现其他情况，需要到主库执行查询语句 注意：如果所有的从库都延迟超过 timeout 秒，查询压力就都跑到主库上，所以需要进行权衡。 等GTID 数据库开启了 GTID 模式，MySQL 提供了判断 GTID 的命令 1SELECT wait_for_executed_gtid_set(gtid_set [timeout]) 等待直到这个库执行的事务中包含传入的 gtid_set，返回 0 超时返回 1 工作流程：先执行 trx1，再执行一个查询请求的逻辑，要保证能够查到正确的数据 trx1 事务更新完成后，从返回包直接获取这个事务的 GTID，记为 gtid 选定一个从库执行查询语句，如果返回值是 0，则在这个从库执行查询语句，否则到主库执行查询语句 对比等待位点方法，减少了一次 show master status 的方法，将参数 session_track_gtids 设置为 OWN_GTID，然后通过 API 接口 mysql_session_track_get_first 从返回包解析出 GTID 的值即可 总结：所有的等待无延迟的方法，都需要根据具体的业务场景去判断实施 负载均衡 负载均衡是应用中使用非常普遍的一种优化方法，机制就是利用某种均衡算法，将固定的负载量分布到不同的服务器上，以此来降低单台服务器的负载，达到优化的效果 分流查询：通过 MySQL 的主从复制，实现读写分离，使增删改操作走主节点，查询操作走从节点，从而可以降低单台服务器的读写压力 分布式数据库架构：适合大数据量、负载高的情况，具有良好的拓展性和高可用性。通过在多台服务器之间分布数据，可以实现在多台服务器之间的负载均衡，提高访问效率 建表范式 第一范式 建立科学的，规范的数据表就需要满足一些规则来优化数据的设计和存储，这些规则就称为范式。 1NF：数据库表的每一列都是不可分割的原子数据项，不能是集合、数组等非原子数据项。即表中的某个列有多个值时，必须拆分为不同的列。简而言之，第一范式每一列不可再拆分，称为原子性。 基本表： 第一范式表： 第二范式 2NF：在满足第一范式的基础上，非主属性完全依赖于主码（主关键字、主键），消除非主属性对主码的部分函数依赖。简而言之，表中的每一个字段 （所有列）都完全依赖于主键，记录的唯一性。 作用：遵守第二范式减少数据冗余，通过主键区分相同数据。 函数依赖：A → B，如果通过 A 属性(属性组)的值，可以确定唯一 B 属性的值，则称 B 依赖于 A 学号 → 姓名；(学号，课程名称) → 分数 完全函数依赖：A → B，如果A是一个属性组，则 B 属性值的确定需要依赖于 A 属性组的所有属性值 (学号，课程名称) → 分数 部分函数依赖：A → B，如果 A 是一个属性组，则 B 属性值的确定只需要依赖于 A 属性组的某些属性值 (学号，课程名称) → 姓名 传递函数依赖：A → B，B → C，如果通过A属性(属性组)的值，可以确定唯一 B 属性的值，在通过 B 属性(属性组)的值，可以确定唯一 C 属性的值，则称 C 传递函数依赖于 A 学号 → 系名，系名 → 系主任 码：如果在一张表中，一个属性或属性组，被其他所有属性所完全依赖，则称这个属性(属性组)为该表的码 该表中的码：(学号，课程名称) 主属性：码属性组中的所有属性 非主属性：除码属性组以外的属性 第三范式 3NF： 在满足第二范式的基础上，表中的任何属性不依赖于其它非主属性，消除传递依赖。简而言之，非主键都直接依赖于主键，而不是通过其它的键来间接依赖于主键。 作用：可以通过主键 id 区分相同数据，修改数据的时候只需要修改一张表（方便修改），反之需要修改多表。 总结","tags":["MySQL"],"categories":["学习笔记"]},{"title":"MySQL--Innodb引擎","path":"/2024/01/29/MySQL--Innodb 引擎/","content":"逻辑结构 架构 概述 MySQL5.5 版本开始，默认使用InnoDB存储引擎，它擅长事务处理，具有崩溃恢复特性，在日常开发中使用非常广泛。下面是InnoDB架构图，左侧为内存结构，右侧为磁盘结构。 内存结构 Buffer Pool InnoDB存储引擎基于磁盘文件存储，访问物理硬盘和在内存中进行访问，速度相差很大，为了尽可能弥补这两者之间的I/O效率的差值，就需要把经常使用的数据加载到缓冲池中，避免每次访问都进行磁盘I/O。 在InnoDB的缓冲池中不仅缓存了索引页和数据页，还包含了undo页、插入缓存、自适应哈希索引以及 InnoDB 的锁信息等等。 缓冲池 Buffer Pool，是主内存中的一个区域，里面可以缓存磁盘上经常操作的真实数据，在执行增删改查操作时，先操作缓冲池中的数据（若缓冲池没有数据，则从磁盘加载并缓存），然后再以一定频率刷新到磁盘，从而减少磁盘IO，加快处理速度。 缓冲池以Page页为单位，底层采用链表数据结构管理Page。根据状态，将Page分为三种类型： free page：空闲page，未被使用。 clean page：被使用page，数据没有被修改过。 dirty page：脏页，被使用page，数据被修改过，也中数据与磁盘的数据产生了不一致。 在专用服务器上，通常将多达80％的物理内存分配给缓冲池 。 参数设置： show variables like 'innodb_buffer_pool_size'; Change Buffer Change Buffer，更改缓冲区（针对于非唯一二级索引页），在执行DML语句时，如果这些数据Page 没有在Buffer Pool中，不会直接操作磁盘，而会将数据变更存在更改缓冲区 Change Buffer 中，在未来数据被读取时，再将数据合并恢复到Buffer Pool中，再将合并后的数据刷新到磁盘中。 存在的意义：Change Buffer的主要意义在于提高数据库的性能和减少对磁盘的IO操作。通过将数据变更存储在Change Buffer中，可以避免直接操作磁盘，而是先将数据变更存储在内存中。这样可以减少对磁盘的读写操作，提高了数据库的响应速度和性能。另外，Change Buffer还可以减少数据页的分裂，提高了数据页的利用率，从而进一步提升了数据库的整体性能。 Adaptive Hash Index 自适应hash索引，用于优化对Buffer Pool数据的查询。MySQL的innoDB引擎中虽然没有直接支持 hash 索引，但是给我们提供了一个功能就是这个自适应hash索引。hash 索引在进行等值匹配时，一般性能是要高于B+树的，因为hash索引一般只需要一次IO即可，而B+树，可能需要几次匹配，所以hash索引的效率要高，但是hash索引又不适合做范围查询、模糊匹配等。 InnoDB 存储引擎会监控对表上各索引页的查询，如果观察到在特定的条件下hash索引可以提升速度，则建立hash索引，称之为自适应hash索引。**自适应哈希索引，无需人工干预，是系统根据情况自动完成。 ** 参数： adaptive_hash_index Log Buffer Log Buffer，日志缓冲区，用来保存要写入到磁盘中的 log 日志数据（redo log 、undo log），默认大小为 16MB，日志缓冲区的日志会定期刷新到磁盘中。如果需要更新、插入或删除许多行的事务，增加日志缓冲区的大小可以节省磁盘 I/O。 参数: innodb_log_buffer_size：缓冲区大小 innodb_flush_log_at_trx_commit：日志刷新到磁盘时机，取值主要包含以下三个： 1: 日志在每次事务提交时写入并刷新到磁盘，默认值 0: 每秒将日志写入并刷新到磁盘一次 2: 日志在每次事务提交后写入，并每秒刷新到磁盘一次 内存管理 Free 链表 当MySQL启动的时候，需要向操作系统申请Buffer Pool的内存空间，但是，当从磁盘上读取一个页然后存储到Buffer Pool的时候，该存放到Buffer Pool的哪个位置呢？而Buffer Pool中是存在有数据的空间和无数据的空间，那么问题又来了，怎么去区分哪些缓冲页是空闲的呢？针对这个问题，free链表就可以满足上述需求了。 MySQL 启动时完成对 Buffer Pool 的初始化，先向操作系统申请连续的内存空间，然后将内存划分为若干对控制块和缓冲页。为了区分空闲和已占用的数据页，将所有空闲缓冲页对应的控制块作为一个节点放入一个链表中，就是 Free 链表（空闲链表）。 基节点：是一块单独申请的内存空间（占 40 字节），并不在 Buffer Pool 的那一大片连续内存空间里 磁盘加载页的流程： 从 Free 链表中取出一个空闲的缓冲页 把缓冲页对应的控制块的信息填上（页所在的表空间、页号之类的信息） 把缓冲页对应的 Free 链表节点（控制块）从链表中移除，表示该缓冲页已经被使用 参考文章：https://blog.csdn.net/li1325169021/article/details/121124440 Flush 链表 Flush 链表是一个用来存储脏页的链表，对于已经修改过的缓冲脏页，第一次修改后加入到链表头部，以后每次修改都不会重新加入，只修改部分控制信息，出于性能考虑并不是直接更新到磁盘，而是在未来的某个时间进行刷脏。 flush链表的结构与free链表差不多，flush链表的结构如下图所示： 是否会存在一个控制块既是free链表的节点，也是flush链表的节点吗？ 不会的。因为如果一个缓冲页是空闲的，那它肯定不可能是脏页。反之亦然。 后台有专门的线程每隔一段时间把脏页刷新到磁盘，刷新方式有以下两种： 从 Flush 链表中刷新一部分页面到磁盘： 后台线程定时从 Flush 链表刷脏，根据系统的繁忙程度来决定刷新速率，这种方式称为 BUF_FLUSH_LIST 线程刷脏的比较慢，导致用户线程加载一个新的数据页时发现没有空闲缓冲页，此时会尝试从 LRU 链表尾部寻找缓冲页直接释放，如果该页面是已经修改过的脏页就同步刷新到磁盘，速度较慢，这种方式称为 BUF_FLUSH_SINGLE_PAGE 从 LRU 链表的冷数据中刷新一部分页面到磁盘，即：BUF_FLUSH_LRU 后台线程会定时从 LRU 链表的尾部开始扫描一些页面，扫描的页面数量可以通过系统变量 innodb_lru_scan_depth 指定，如果在 LRU 链表中发现脏页，则把它们刷新到磁盘，这种方式称为 BUF_FLUSH_LRU 控制块里会存储该缓冲页是否被修改的信息，所以可以很容易的获取到某个缓冲页是否是脏页 参考文章：https://blog.csdn.net/li1325169021/article/details/121125765 LRU 链表 LRU是Least Recently Used（最近最少使用）的缩写，是一种常用的缓存淘汰策略。在计算机系统中，缓存用于存储最常访问的数据，以提高访问速度。LRU算法根据数据的访问时间来决定淘汰哪些数据，具体来说，当缓存空间已满时，LRU算法会淘汰最近最少被访问的数据，以腾出空间来存储新的数据。这样可以保留最常用的数据，提高缓存的命中率，减少缓存未命中的情况。 LRU链表（LRU Linked List）是一种数据结构，用于实现LRU缓存淘汰策略。它是一个双向链表，其中每个节点都包含了缓存中的数据以及对应的键。链表的头部表示最近访问的数据，而尾部表示最久未被访问的数据。通过LRU链表，可以快速定位最近最少使用的数据，并进行淘汰，从而保持缓存的命中率较高。LRU链表的时间复杂度为O(1)，适用于需要频繁访问和更新的缓存场景，如数据库缓存、页面缓存等。 Buffer Pool 需要保证缓存的命中率，所以 MySQL 创建了一个 LRU 链表，当访问某个页时： 如果该页不在 Buffer Pool 中，把该页从磁盘加载进来后会将该缓冲页对应的控制块作为节点放入 LRU 链表的头部，保证热点数据在链表头 如果该页在 Buffer Pool 中，则直接把该页对应的控制块移动到 LRU 链表的头部，所以 LRU 链表尾部就是最近最少使用的缓冲页 MySQL 基于局部性原理提供了预读功能： 线性预读：系统变量 innodb_read_ahead_threshold，如果顺序访问某个区（extent：16 KB 的页，连续 64 个形成一个区，一个区默认 1MB 大小）的页面数超过了该系统变量值，就会触发一次异步读取下一个区中全部的页面到 Buffer Pool 中 随机预读：如果某个区 13 个连续的页面都被加载到 Buffer Pool，无论这些页面是否是顺序读取，都会触发一次异步读取本区所有的其他页面到 Buffer Pool 中 预读会造成加载太多用不到的数据页，造成那些使用频率很高的数据页被挤到 LRU 链表尾部，所以 InnoDB 将 LRU 链表分成两段，冷热数据隔离： 一部分存储使用频率很高的数据页，这部分链表也叫热数据，young 区，靠近链表头部的区域 一部分存储使用频率不高的冷数据，old 区，靠近链表尾部，默认占 37%，可以通过系统变量 innodb_old_blocks_pct 指定 当磁盘上的某数据页被初次加载到 Buffer Pool 中会被放入 old 区，淘汰时优先淘汰 old 区 当对 old 区的数据进行访问时，会在控制块记录下访问时间，等待后续的访问时间与第一次访问的时间是否在某个时间间隔内，通过系统变量 innodb_old_blocks_time 指定时间间隔，默认 1000ms，成立就移动到 young 区的链表头部 innodb_old_blocks_time 为 0 时，每次访问一个页面都会放入 young 区的头部 磁盘结构 System Tablespace 系统表空间是更改缓冲区的存储区域。如果表是在系统表空间而不是每个表文件或通用表空间中创建的，它也可能包含表和索引数据。(在MySQL5.x版本中还包含InnoDB数据字典、undolog等) 参数：innodb_data_file_path 系统表空间，默认的文件名叫 ibdata1。 File-Per-Table Tablespaces 如果开启了innodb_file_per_table开关 ，则每个表的文件表空间包含单个InnoDB表的数据和索引 ，并存储在文件系统上的单个数据文件中。也就是说，我们每创建一个表，都会产生一个表空间文件。 开关参数：innodb_file_per_table ，该参数默认开启。 General Tablespaces 通用表空间，需要通过 CREATE TABLESPACE 语法创建通用表空间，在创建表时，可以指定该表空间。 创建表空间 1CREATE TABLESPACE ts_name ADD DATAFILE &#x27;file_name&#x27; ENGINE = engine_name; 创建表时指定表空间 1CREATE TABLE xxx ... TABLESPACE ts_name; Undo Tablespaces 撤销表空间，MySQL实例在初始化时会自动创建两个默认的undo表空间（初始大小16M），用于存储 undo log 日志。 Temporary Tablespaces 临时表空间， InnoDB 使用会话临时表空间和全局临时表空间。存储用户创建的临时表等数据。 Doublewrite Buffer Files 双写缓冲区，innoDB引擎将数据页从Buffer Pool刷新到磁盘前，先将数据页写入双写缓冲区文件中，便于系统异常时恢复数据。文件后缀名为 .dblwr。 Redo Log 重做日志，是用来实现事务的持久性。该日志文件由两部分组成：重做日志缓冲（redo log buffer）以及重做日志文件（redo log）,前者是在内存中，后者在磁盘中。当事务提交之后会把所有修改信息都会存到该日志中, 用于在刷新脏页到磁盘时,发生错误时, 进行数据恢复使用。以循环方式写入重做日志文件，涉及两个文件：ib_logfile0 和 ib_logfile1。 后台线程 InnoDB 使用了一些后台线程来执行不同的任务，以提高数据库的性能和可靠性。下面是几个常见的 InnoDB 后台线程: Master Thread（主线程） 主线程是 InnoDB 的最重要的后台线程之一。它负责处理各种关键任务，例如事务的提交和回滚，缓冲池的管理，以及刷新脏页（即写回内存中被修改但尚未写回磁盘的数据页）等。 IO Threads（IO 线程） IO 线程是 InnoDB 的另一个重要的后台线程组。它由多个线程组成，其中包括读取线程和写入线程。读取线程负责从磁盘读取数据页到内存中的缓冲池，写入线程负责将脏页写回到磁盘。通过使用多个 IO 线程，InnoDB 可以并行地进行数据的读写，提高了系统的并发性能。 可以通过show engine innodb status \\G;，查看到InnoDB的状态信息，其中就包含IO Thread信息。 Purge Thread（清理线程） 清理线程负责回收已经完成的事务产生的 undo 日志空间。当事务提交或回滚后，InnoDB 会将相应的 undo 日志标记为不再需要，并由清理线程负责删除这些不再需要的 undo 日志，释放相关的空间。 Page Cleaner Thread（页清理线程） 页清理线程负责在需要时刷新脏页到磁盘，以确保系统具有足够的空闲页。当内存中的页不足时，页清理线程将会将一些脏页刷新到磁盘或者被写入其他地方，以释放内存，并提供空闲页供新数据加载到缓冲池。 事务原理 基本概述 事务是一组操作的集合，它是一个不可分割的工作单位，事务会把所有的操作作为一个整体一起向系 统提交或撤销操作请求，即这些操作要么同时成功，要么同时失败。 特性（acid） 原子性（Atomicity）：事务是不可分割的最小操作单元，要么全部成功，要么全部失败 一致性（Consistency）：事务完成时，必须使所有的数据都保持一致状态 隔离性（Isolation）：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立环境下运行 持久性（Durability）：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的 而对于这四大特性，实际上分为两个部分。 其中的原子性、一致性、持久化，实际上是由InnoDB中的两份日志来保证的，一份是redo log日志，一份是undo log日志。 而持久性是通过数据库的锁，加上MVCC来保证的。 Redo log 重做日志，记录的是事务提交时数据页的物理修改，是用来实现事务的持久性。 该日志文件由两部分组成：重做日志缓冲（redo log buffer）以及重做日志文件（redo log file）,前者是在内存中，后者在磁盘中。当事务提交之后会把所有修改信息都存到该日志文件中, 用于在刷新脏页到磁盘,发生错误时, 进行数据恢复使用。 如果没有redolog，可能会存在什么问题的？ 我们知道，在InnoDB引擎中的内存结构中，主要的内存区域就是缓冲池，在缓冲池中缓存了很多的数据页。 当我们在一个事务中，执行多个增删改的操作时，InnoDB引擎会先操作缓冲池中的数据，如果缓冲区没有对应的数据，会通过后台线程将磁盘中的数据加载出来，存放在缓冲区中，然后将缓冲池中的数据修改，修改后的数据页我们称为脏页。 而脏页则会在一定的时机，通过后台线程刷新到磁盘中，从而保证缓冲区与磁盘的数据一致。 而缓冲区的脏页数据并不是实时刷新的，而是一段时间之后将缓冲区的数据刷新到磁盘中，假如刷新到磁盘的过程出错了，而提示给用户事务提交成功，而数据却没有持久化下来，这就出现问题了，没有保证事务的持久性。 有了redo log之后，当对缓冲区的数据进行增删改之后，会首先将操作的数据页的变化，记录在redo log buffer中。在事务提交时，会将redo log buffer中的数据刷新到redo log磁盘文件中。过一段时间之后，如果刷新缓冲区的脏页到磁盘时，发生错误，此时就可以借助于redo log进行数据恢复，这样就保证了事务的持久性。 而如果脏页成功刷新到磁盘或者涉及到的数据已经落盘，此时 redolog就没有作用了，就可以删除了，所以存在的两个redolog文件是循环写的。 那为什么每一次提交事务，要刷新redo log 到磁盘中呢，而不是直接将buffer pool中的脏页刷新到磁盘呢 ? 因为在业务操作中，我们操作数据一般都是随机读写磁盘的，而不是顺序读写磁盘。 而redo log在往磁盘文件中写入数据，由于是日志文件，所以都是顺序写的。顺序写的效率，要远大于随机写。 这种先写日志的方式，称之为 WAL（Write-Ahead Logging）。 Undo log 回滚日志，用于记录数据被修改前的信息 , 作用包含两个 : 提供回滚 (保证事务的原子性) MVCC (多版本并发控制) undo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当 update 一条记录时，它记录一条对应相反的 update 记录。当执行rollback时，就可以从undo log中的逻辑记录读取到相应的内容并进行回滚。 Undo log 销毁：undo log在事务执行时产生，事务提交时，并不会立即删除undo log，因为这些日志可能还用于MVCC。 Undo log 存储：undo log采用段的方式进行管理和记录，存放在前面介绍的 rollback segment 回滚段中，内部包含1024个undo log segment。 MVCC 基本概述 MVCC 全称 Multi-Version Concurrency Control，即多版本并发控制，用来解决读写冲突的无锁并发控制，可以在发生读写请求冲突时不用加锁解决，这个读是指的快照读（也叫一致性读或一致性无锁读），而不是当前读： 快照读：实现基于 MVCC，因为是多版本并发，所以快照读读到的数据不一定是当前最新的数据，有可能是历史版本的数据 Read Committed：每次select，都生成一个快照读 Repeatable Read：开启事务后第一个select语句才是快照读的地方 Serializable：快照读会退化为当前读 当前读：又叫加锁读，读取数据库记录是当前最新的版本（产生幻读、不可重复读），可以对读取的数据进行加锁，防止其他事务修改数据，是悲观锁的一种操作，读写操作加共享锁或者排他锁和串行化事务的隔离级别都是当前读 数据库并发场景： 读-读：不存在任何问题，也不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读 写-写：有线程安全问题，可能会存在脏写（丢失更新）问题 MVCC 的优点： 在并发读写数据库时，做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了并发读写的性能 可以解决脏读，不可重复读等事务隔离问题（加锁也能解决），但不能解决更新丢失问题（写锁会解决） 提高读写和写写的并发性能： MVCC + 悲观锁：MVCC 解决读写冲突，悲观锁解决写写冲突 MVCC + 乐观锁：MVCC 解决读写冲突，乐观锁解决写写冲突 隐藏字段 InnoDB 存储引擎，数据库中的聚簇索引每行数据，除了自定义的字段，还有数据库隐式定义的字段： DB_TRX_ID：最近修改事务 ID，记录创建该数据或最后一次修改该数据的事务 ID DB_ROLL_PTR：回滚指针，指向记录对应的 undo log 日志，undo log 中又指向上一个旧版本的 undo log DB_ROW_ID：隐含的自增 ID（隐藏主键），如果数据表没有主键，InnoDB 会自动以 DB_ROW_ID 作为聚簇索引 版本链 undo log 是逻辑日志，记录的是每个事务对数据执行的操作，而不是记录的全部数据，要根据 undo log 逆推出以往事务的数据。 undo log 的作用： 保证事务进行 rollback 时的原子性和一致性，当事务进行回滚的时候可以用 undo log 的数据进行恢复 用于 MVCC 快照读，通过读取 undo log 的历史版本数据可以实现不同事务版本号都拥有自己独立的快照数据 undo log 主要分为两种： insert undo log：事务在 insert 新记录时产生的 undo log，只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log：事务在进行 update 或 delete 时产生的 undo log，在事务回滚时需要，在快照读时也需要。不能随意删除，只有在当前读或事务回滚不涉及该日志时，对应的日志才会被 purge 线程统一清除 每次对数据库记录进行改动，都会产生的新版本的 undo log，随着更新次数的增多，所有的版本都会被 roll_pointer 属性连接成一个链表，把这个链表称之为版本链，版本链的头节点就是当前的最新的 undo log，链尾就是最早的旧 undo log。 说明：因为 DELETE 删除记录，都是移动到垃圾链表中，不是真正的删除，所以才可以通过版本链访问原始数据。 注意：undo 是逻辑日志，这里只是直观的展示出来。 工作流程： 有个事务插入 persion 表一条新记录，name 为 Jerry，age 为 24 事务 1 修改该行数据时，数据库会先对该行加排他锁，然后先记录 undo log，然后修改该行 name 为 Tom，并且修改隐藏字段的事务 ID 为当前事务 1 的 ID（默认为 1 之后递增），回滚指针指向拷贝到 undo log 的副本记录，事务提交后，释放锁 以此类推 Read View Read View 是事务进行读数据操作时产生的读视图，该事务执行快照读的那一刻会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的 ID，用来做可见性判断，根据视图判断当前事务能够看到哪个版本的数据。 注意：这里的快照并不是把所有的数据拷贝一份副本，而是由 undo log 记录的逻辑日志，根据库中的数据进行计算出历史数据。 工作流程：将版本链的头节点的事务 ID（最新数据事务 ID，大概率不是当前线程）DB_TRX_ID 取出来，与系统当前活跃事务的 ID 对比进行可见性分析，不可见就通过 DB_ROLL_PTR 回滚指针去取出 undo log 中的下一个 DB_TRX_ID 比较，直到找到最近的满足可见性的 DB_TRX_ID，该事务 ID 所在的旧记录就是当前事务能看见的最新的记录。 Read View 几个属性： m_ids：生成 Read View 时当前系统中活跃的事务 id 列表（未提交的事务集合，当前事务也在其中） min_trx_id：生成 Read View 时当前系统中活跃的最小的事务 id，也就是 m_ids 中的最小值（已提交的事务集合） max_trx_id：生成 Read View 时当前系统应该分配给下一个事务的 id 值，m_ids 中的最大值加 1（未开始事务） creator_trx_id：生成该 Read View 的事务的事务 id，就是判断该 id 的事务能读到什么数据 creator 创建一个 Read View，进行可见性算法分析：（解决了读未提交） db_trx_id == creator_trx_id：表示这个数据就是当前事务自己生成的，自己生成的数据自己肯定能看见，所以此数据对 creator 是可见的 db_trx_id &lt; min_trx_id：该版本对应的事务 ID 小于 Read view 中的最小活跃事务 ID，则这个事务在当前事务之前就已经被提交了，对 creator 可见（因为比已提交的最大事务 ID 小的并不一定已经提交，所以应该判断是否在活跃事务列表） db_trx_id &gt;= max_trx_id：该版本对应的事务 ID 大于 Read view 中当前系统的最大事务 ID，则说明该数据是在当前 Read view 创建之后才产生的，对 creator 不可见 min_trx_id&lt;= db_trx_id &lt; max_trx_id：判断 db_trx_id 是否在活跃事务列表 m_ids 中 在列表中，说明该版本对应的事务正在运行，数据不能显示（不能读到未提交的数据） 不在列表中，说明该版本对应的事务已经被提交，数据可以显示（可以读到已经提交的数据） 实现流程 RC隔离级别下，在事务中每一次执行快照读时生成ReadView。我们就来分析事务5中，两次快照读读取数据，是如何获取数据的? 如图，在事务5中，查询了两次id为30的记录，由于隔离级别为Read Committed，所以每一次进行快照读都会生成一个ReadView，那么两次生成的ReadView如下。 那么这两次快照读在获取数据时，就需要根据所生成的ReadView以及ReadView的版本链访问规则，到undolog版本链中匹配数据，最终决定此次快照读返回的数据。 先来看第一次快照读具体的读取过程： 在进行匹配时，会从undo log的版本链，从上到下进行挨个匹配： 先匹配第一条条记录，这条记录对应的 trx_id 为4，也就是将4带入右侧的匹配规则中。 ①不满足 ②不满足 ③不满足 ④也不满足，都不满足，则继续匹配undo log版本链的下一条。 再匹配第二条，这条记录对应的trx_id为3，也就是将3带入右侧的匹配规则中。①不满足 ②不满足 ③不满足 ④也不满足 ，都不满足，则继续匹配undo log版本链的下一条。 再匹配第三条，这条记录对应的trx_id为2，也就是将2带入右侧的匹配规则中。①不满足 ②满足，终止匹配，此次快照读，返回的数据就是版本链中记录的这条数据。 再来看第二次快照读具体的读取过程: 先匹配第一条记录，这条记录对应的trx_id为4，也就是将4带入右侧的匹配规则中。 ①不满足 ②不满足 ③不满足 ④也不满足 ，都不满足，则继续匹配undo log版本链的下一条。 再匹配第二条，这条记录对应的trx_id为3，也就是将3带入右侧的匹配规则中。①不满足 ②满足。终止匹配，此次快照读，返回的数据就是版本链中记录的这条数据。 RR隔离级别下，仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。 而RR 是可重复读，在一个事务中，执行两次相同的select语句，查询到的结果是一样的。 那MySQL是如何做到可重复读的呢? 我们简单分析一下就知道了。 在RR隔离级别下，只是在事务中第一次快照读时生成ReadView，后续都是复用该 ReadView，那么既然ReadView都一样， ReadView的版本链匹配规则也一样， 那么最终快照读返回的结果也是一样的。 所以呢，MVCC的实现原理就是通过 InnoDB表的隐藏字段、UndoLog 版本链、ReadView来实现的。而MVCC + 锁，则实现了事务的隔离性。 而一致性则是由redolog 与 undolog保证。","tags":["MySQL"],"categories":["学习笔记"]},{"title":"MySQL--SQL优化","path":"/2024/01/27/MySQL--SQL 优化/","content":"SQL 性能分析 SQL 执行频率 MySQL 客户端连接成功后，查询服务器状态信息： 123SHOW [SESSION|GLOBAL] STATUS LIKE &#x27;&#x27;;-- SESSION: 显示当前会话连接的统计结果，默认参数-- GLOBAL: 显示自数据库上次启动至今的统计结果 查看 SQL 执行频率： 1SHOW STATUS LIKE &#x27;Com_____&#x27;; Com_xxx 表示每种语句执行的次数 查询 SQL 语句影响的行数： 1SHOW STATUS LIKE &#x27;Innodb_rows_%&#x27;; Com_xxxx：这些参数对于所有存储引擎的表操作都会进行累计 Innodb_xxxx：这几个参数只是针对 InnoDB 存储引擎的，累加的算法也略有不同 参数 含义 Com_select 执行 SELECT 操作的次数，一次查询只累加 1 Com_insert 执行 INSERT 操作的次数，对于批量插入的 INSERT 操作，只累加一次 Com_update 执行 UPDATE 操作的次数 Com_delete 执行 DELETE 操作的次数 Innodb_rows_read 执行 SELECT 查询返回的行数 Innodb_rows_inserted 执行 INSERT 操作插入的行数 Innodb_rows_updated 执行 UPDATE 操作更新的行数 Innodb_rows_deleted 执行 DELETE 操作删除的行数 Connections 试图连接 MySQL 服务器的次数 Uptime 服务器工作时间 Slow_queries 慢查询的次数 慢查询日志 SQL 执行慢有两种情况： 偶尔慢：DB 在刷新脏页（事务原理部分） redo log 写满了 内存不够用，要从 LRU 链表中淘汰 MySQL 认为系统空闲的时候 MySQL 关闭时 一直慢的原因：索引没有设计好、SQL 语句没写好、MySQL 选错了索引 通过以下两种方式定位执行效率较低的 SQL 语句 慢日志查询： 慢查询日志在查询结束以后才记录，执行效率出现问题时查询日志并不能定位问题配置文件修改：修改.cnf 文件 vim /etc/mysql/my.cnf，重启 MySQL 服务器 1234slow_query_log=ONslow_query_log_file=/usr/local/mysql/var/localhost-slow.loglong_query_time=1\t#记录超过long_query_time秒的SQL语句的日志log-queries-not-using-indexes = 1 使用命令配置： 12mysql&gt; SET slow_query_log=ON;mysql&gt; SET GLOBAL slow_query_log=ON; 查看是否配置成功： 1SHOW VARIABLES LIKE &#x27;%query%&#x27; SHOW PROCESSLIST：实时查看当前 MySQL 在进行的连接线程，包括线程的状态、是否锁表、SQL 的执行情况，同时对一些锁表操作进行优化 EXPLAIN 执行计划 通过 EXPLAIN 命令获取执行 SQL 语句的信息，包括在 SELECT 语句执行过程中如何连接和连接的顺序，执行计划在优化器优化完成后、执行器之前生成，然后执行器会调用存储引擎检索数据。 查询 SQL 语句的执行计划： 1EXPLAIN SELECT * FROM table_1 WHERE id = 1; 字段 含义 id SELECT 的序列号 select_type 表示 SELECT 的类型 table 访问数据库中表名称，有时可能是简称或者临时表名称（&lt;table_name&gt;） type 表示表的连接类型 possible_keys 表示查询时，可能使用的索引 key 表示实际使用的索引 key_len 索引字段的长度 ref 表示与索引列进行等值匹配的对象，常数、某个列、函数等，type 必须在（range, const] 之间，左闭右开 rows 扫描出的行数，表示 MySQL 根据表统计信息及索引选用情况，估算的找到所需的记录扫描的行数 filtered 条件过滤的行百分比，单表查询没意义，用于连接查询中对驱动表的扇出进行过滤，查询优化器预测所有扇出值满足剩余查询条件的百分比，相乘以后表示多表查询中还要对被驱动执行查询的次数 extra 执行情况的说明和描述 MySQL 执行计划的局限： 只是计划，不是执行 SQL 语句，可以随着底层优化器输入的更改而更改 EXPLAIN 不会告诉显示关于触发器、存储过程的信息对查询的影响情况， 不考虑各种 Cache EXPLAIN 不能显示 MySQL 在执行查询时的动态，因为执行计划在执行查询之前生成 EXPALIN 只能解释 SELECT 操作，其他操作要重写为 SELECT 后查看执行计划 EXPLAIN PLAN 显示的是在解释语句时数据库将如何运行 SQL 语句，由于执行环境和 EXPLAIN PLAN 环境的不同，此计划可能与 SQL 语句实际的执行计划不同，部分统计信息是估算的，并非精确值 SHOW WARINGS：在使用 EXPALIN 命令后执行该语句，可以查询与执行计划相关的拓展信息，展示出 Level、Code、Message 三个字段，当 Code 为 1003 时，Message 字段展示的信息类似于将查询语句重写后的信息，但是不是等价，不能执行复制过来运行。 环境准备： id id 代表 SQL 执行的顺序的标识，每个 SELECT 关键字对应一个唯一 id，所以在同一个 SELECT 关键字中的表的 id 都是相同的。SELECT 后的 FROM 可以跟随多个表，每个表都会对应一条记录，这些记录的 id 都是相同的， id 相同时，执行顺序由上至下。连接查询的执行计划，记录的 id 值都是相同的，出现在前面的表为驱动表，后面为被驱动表 1EXPLAIN SELECT * FROM t_role r, t_user u, user_role ur WHERE r.id = ur.role_id AND u.id = ur.user_id ; id 不同时，id 值越大优先级越高，越先被执行 1EXPLAIN SELECT * FROM t_role WHERE id = (SELECT role_id FROM user_role WHERE user_id = (SELECT id FROM t_user WHERE username = &#x27;stu1&#x27;)) id 有相同也有不同时，id 相同的可以认为是一组，从上往下顺序执行；在所有的组中，id 的值越大的组，优先级越高，越先执行 1EXPLAIN SELECT * FROM t_role r , (SELECT * FROM user_role ur WHERE ur.`user_id` = &#x27;2&#x27;) a WHERE r.id = a.role_id ; id 为 NULL 时代表的是临时表 select 表示查询中每个 select 子句的类型（简单 OR 复杂）。 select_type 含义 SIMPLE 简单的 SELECT 查询，查询中不包含子查询或者 UNION PRIMARY 查询中若包含任何复杂的子查询，最外层（也就是最左侧）查询标记为该标识 UNION 对于 UNION 或者 UNION ALL 的复杂查询，除了最左侧的查询，其余的小查询都是 UNION UNION RESULT UNION 需要使用临时表进行去重，临时表的是 UNION RESULT DEPENDENT UNION 对于 UNION 或者 UNION ALL 的复杂查询，如果各个小查询都依赖外层查询，是相关子查询，除了最左侧的小查询为 DEPENDENT SUBQUERY，其余都是 DEPENDENT UNION SUBQUERY 子查询不是相关子查询，该子查询第一个 SELECT 代表的查询就是这种类型，会进行物化（该子查询只需要执行一次） DEPENDENT SUBQUERY 子查询是相关子查询，该子查询第一个 SELECT 代表的查询就是这种类型，不会物化（该子查询需要执行多次） DERIVED 在 FROM 列表中包含的子查询，被标记为 DERIVED（衍生），也就是生成物化派生表的这个子查询 MATERIALIZED 将子查询物化后与与外层进行连接查询，生成物化表的子查询 子查询为 DERIVED： 1SELECT * FROM (SELECT key1 FROM t1) AS derived_1 WHERE key1 &gt; 10 子查询为 MATERIALIZED： 1SELECT * FROM t1 WHERE key1 IN (SELECT key1 FROM t2) type 对表的访问方式，表示 MySQL 在表中找到所需行的方式，又称访问类型。 type 含义 ALL 全表扫描，如果是 InnoDB 引擎是扫描聚簇索引 index 可以使用覆盖索引，但需要扫描全部索引 range 索引范围扫描，常见于 between、&lt;、&gt; 等的查询 index_subquery 子查询可以普通索引，则子查询的 type 为 index_subquery unique_subquery 子查询可以使用主键或唯一二级索引，则子查询的 type 为 index_subquery index_merge 索引合并 ref_or_null 非唯一性索引（普通二级索引）并且可以存储 NULL，进行等值匹配 ref 非唯一性索引与常量等值匹配 eq_ref 唯一性索引（主键或不存储 NULL 的唯一二级索引）进行等值匹配，如果二级索引是联合索引，那么所有联合的列都要进行等值匹配 const 通过主键或者唯一二级索引与常量进行等值匹配 system system 是 const 类型的特例，当查询的表只有一条记录的情况下，使用 system NULL MySQL 在优化过程中分解语句，执行时甚至不用访问表或索引 从上到下，性能从差到好，一般来说需要保证查询至少达到 range 级别， 最好达到 ref。 key possible_keys： 指出 MySQL 能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用 如果该列是 NULL，则没有相关的索引 key： 显示 MySQL 在查询中实际使用的索引，若没有使用索引，显示为 NULL 查询中若使用了覆盖索引，则该索引可能出现在 key 列表，不出现在 possible_keys key_len： 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度 key_len 显示的值为索引字段的最大可能长度，并非实际使用长度，即 key_len 是根据表定义计算而得，不是通过表内检索出的 在不损失精确性的前提下，长度越短越好 Extra 其他的额外的执行计划信息，在该列展示： No tables used：查询语句中使用 FROM dual 或者没有 FROM 语句 Impossible WHERE：查询语句中的 WHERE 子句条件永远为 FALSE，会导致没有符合条件的行 Using index：该值表示相应的 SELECT 操作中使用了覆盖索引（Covering Index） Using index condition：第一种情况是搜索条件中虽然出现了索引列，但是部分条件无法形成扫描区间（索引失效），会根据可用索引的条件先搜索一遍再匹配无法使用索引的条件，回表查询数据；第二种是使用了索引条件下推优化 Using where：搜索的数据需要在 Server 层判断，无法使用索引下推 Using join buffer：连接查询被驱动表无法利用索引，需要连接缓冲区来存储中间结果 Using filesort：无法利用索引完成排序（优化方向），需要对数据使用外部排序算法，将取得的数据在内存或磁盘中进行排序 Using temporary：表示 MySQL 需要使用临时表来存储结果集，常见于排序、去重（UNION）、分组等场景 Select tables optimized away：说明仅通过使用索引，优化器可能仅从聚合函数结果中返回一行 No tables used：Query 语句中使用 from dual 或不含任何 from 子句 参考文章：https://www.cnblogs.com/ggjucheng/archive/2012/11/11/2765237.html PROFILES SHOW PROFILES 能够在做 SQL 优化时分析当前会话中语句执行的资源消耗情况。 通过 have_profiling 参数，能够看到当前 MySQL 是否支持 profile： 默认 profiling 是关闭的，可以通过 set 语句在 Session 级别开启 profiling： 1SET profiling=1; #开启profiling 开关； 执行 SHOW PROFILES 指令， 来查看 SQL 语句执行的耗时: 1SHOW PROFILES; 查看到该 SQL 执行过程中每个线程的状态和消耗的时间： 1SHOW PROFILE FOR QUERY query_id; 在获取到最消耗时间的线程状态后，MySQL 支持选择 all、cpu、block io 、context switch、page faults 等类型查看 MySQL 在使用什么资源上耗费了过高的时间。例如，选择查看 CPU 的耗费时间： Status：SQL 语句执行的状态 Durationsql：执行过程中每一个步骤的耗时 CPU_user：当前用户占有的 CPU CPU_system：系统占有的 CPU TRACE MySQL 提供了对 SQL 的跟踪， 通过 trace 文件可以查看优化器生成执行计划的过程。 打开 trace 功能，设置格式为 JSON，并设置 trace 的最大使用内存，避免解析过程中因默认内存过小而不能够完整展示 12SET optimizer_trace=&quot;enabled=on&quot;,end_markers_in_json=ON;\t-- 会话内有效SET optimizer_trace_max_mem_size=1000000; 执行 SQL 语句： 1SELECT * FROM tb_item WHERE id &lt; 4; 检查 information_schema.optimizer_trace： 1SELECT * FROM information_schema.optimizer_trace \\G; -- \\G代表竖列展示 执行信息主要有三个阶段：prepare 阶段、optimize 阶段（成本分析）、execute 阶段（执行）。 优化方式 插入数据优化 如果使用 insert 插入数据，建议批量插入，以减少数据库连接次数，优化性能。 当插入的数据量过于庞大，可以使用 load 命令导入数据的，适当的设置可以提高导入的效率。 语法 1LOAD DATA LOCAL INFILE = &#x27;/home/seazean/sql1.log&#x27; INTO TABLE `tb_user_1` FIELD TERMINATED BY &#x27;,&#x27; LINES TERMINATED BY &#x27; &#x27;; -- 文件格式如上图 对于 InnoDB 类型的表，有以下几种方式可以提高导入的效率： 主键顺序插入：因为 InnoDB 类型的表是按照主键的顺序保存的，所以将导入的数据按照主键的顺序排列，可以有效的提高导入数据的效率，如果 InnoDB 表没有主键，那么系统会自动默认创建一个内部列作为主键 主键是否连续对性能影响不大，只要是递增的就可以，比如雪花算法产生的 ID 不是连续的，但是是递增的，因为递增可以让主键索引尽量地保持顺序插入，避免了页分裂，因此索引更紧凑。 关闭唯一性校验：在导入数据前执行 SET UNIQUE_CHECKS=0，关闭唯一性校验；导入结束后执行 SET UNIQUE_CHECKS=1，恢复唯一性校验，可以提高导入的效率。 手动提交事务：如果应用使用自动提交的方式，建议在导入前执行SET AUTOCOMMIT=0，关闭自动提交；导入结束后再打开自动提交，可以提高导入的效率。 事务需要控制大小，事务太大可能会影响执行的效率。MySQL 有 innodb_log_buffer_size 配置项，超过这个值的日志会写入磁盘数据，效率会下降，所以在事务大小达到配置项数据级前进行事务提交可以提高效率 主键优化 在InnoDB存储引擎中，表数据都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表 (index organized table IOT)。 行数据，都是存储在聚集索引的叶子节点上的。在InnoDB引擎中，数据行是记录在逻辑结构 page 页中的，而每一个页的大小是固定的，默认16K。那也就意味着，一个页中所存储的行也是有限的，如果插入的数据行row在该页存储不小，将会存储到下一个页中，页与页之间会通过指针连接。 页分裂 页可以为空，也可以填充一半，也可以填充100%。每个页包含了2-N行数据(如果一行数据过大，会行溢出)，根据主键排列。 主键顺序插入效果 从磁盘中申请页， 主键顺序插入 第一个页没有满，继续往第一页插入 当第一个也写满之后，再申请并写入第二个页，页与页之间会通过指针连接 当第二页写满了，再往第三页写入，以此类推 主键乱序插入效果 加入1#,2#页都已经写满了，存放了如图所示的数据 此时再插入id为50的记录，会再次开启一个页，写入新的页中吗？答案是不会。因为，索引结构的叶子节点是有顺序的。按照顺序，应该存储在47之后。 但是47所在的1#页，已经写满了，存储不了50对应的数据了。 那么此时会开辟一个新的页 3#。 但是并不会直接将50存入3#页，而是会将1#页后一半的数据，移动到3#页，然后在3#页，插入50。 移动数据，并插入id为50的数据之后，那么此时，这三个页之间的数据顺序是有问题的。 1#的下一个 页，应该是3#， 3#的下一个页是2#。 所以，此时，需要重新设置链表指针。 上述的这种现象，称之为 “页分裂”，是比较耗费性能的操作。 页合并 假设目前表中已有数据的索引结构(叶子节点)如下： 当我们对已有数据进行删除时，具体的效果如下: 当删除一行记录时，实际上记录并没有被物理删除，只是记录被标记（flaged）为删除并且它的空间， 变得允许被其他记录声明使用。 当页中删除的记录达到 MERGE_THRESHOLD（默认为页的50%），InnoDB会开始寻找最靠近的页（前或后）看看是否可以将两个页合并以优化空间使用。 删除数据，并将页合并之后，再次插入新的数据21，则直接插入3#页。 这个里面所发生的合并页的这个现象，就称之为 “页合并”。 MERGE_THRESHOLD：合并页的阈值，可以自己设置，在创建表或者创建索引时指定。 自增机制 自增主键可以让主键索引尽量地保持在数据页中递增顺序插入，不自增需要寻找其他页插入，导致随机 IO 和页分裂的情况。 表的结构定义存放在后缀名为 .frm 的文件中，但是并不会保存自增值，不同的引擎对于自增值的保存策略不同： MyISAM 引擎的自增值保存在数据文件中 InnoDB 引擎的自增值保存在了内存里，每次打开表都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为当前的自增值；8.0 版本后，才有了自增值持久化的能力，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值 在插入一行数据的时候，自增值的行为如下： 如果插入数据时 id 字段指定为 0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT值填到自增字段 如果插入数据时 id 字段指定了具体的值，比如某次要插入的值是 X，当前的自增值是 Y 如果 X&lt;Y，那么这个表的自增值不变 如果 X≥Y，就需要把当前自增值修改为新的自增值 参数说明：auto_increment_offset 和 auto_increment_increment 分别表示自增的初始值和步长，默认值都是 1。 语句执行失败也不回退自增 id，所以保证了自增 id 是递增的。，但不保证是连续的（不能回退，所以有些回滚事务的自增 id 就不会重新使用，导致出现不连续）。 自增 ID MySQL 不同的自增 id 在达到上限后的表现不同： 表的自增 id 如果是 int 类型，达到上限 2^32-1 后，再申请时值就不会改变，进而导致继续插入数据时报主键冲突的错误 row_id 长度为 6 个字节，达到上限后则会归 0 再重新递增，如果出现相同的 row_id，后写的数据会覆盖之前的数据，造成旧数据丢失，影响的是数据可靠性，所以应该在 InnoDB 表中主动创建自增主键报主键冲突，插入失败影响的是可用性，而一般情况下，可靠性优先于可用性 Xid 长度 8 字节，由 Server 层维护，只需要不在同一个 binlog 文件中出现重复值即可，虽然理论上会出现重复值，但是概率极小 InnoDB 的 max_trx_id 递增值每次 MySQL 重启都会被保存起来，重启也不会重置为 0，所以会导致一直增加到达上限，然后从 0 开始，这时原事务 0 修改的数据对当前事务就是可见的，产生脏读的现象只读事务不分配 trx_id，所以 trx_id 的增加速度变慢了 thread_id 长度 4 个字节，到达上限后就会重置为 0，MySQL 设计了一个唯一数组的逻辑，给新线程分配 thread_id 时做判断，保证不会出现两个相同的 thread_id： 123do &#123;\tnew_id = thread_id_counter++;&#125; while (!thread_ids.insert_unique(new_id).second); 主键设计原则 满足业务需求的情况下，尽量降低主键的长度。 插入数据时，尽量选择顺序插入，选择使用 AUTO_INCREMENT 自增主键。 尽量不要使用UUID做主键或者是其他自然主键，如身份证号。 业务操作时，避免对主键的修改。 ORDER BY 优化 数据准备： 123456789CREATE TABLE `emp` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `name` VARCHAR(100) NOT NULL, `age` INT(3) NOT NULL, `salary` INT(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;INSERT INTO `emp` (`id`, `name`, `age`, `salary`) VALUES(&#x27;1&#x27;,&#x27;Tom&#x27;,&#x27;25&#x27;,&#x27;2300&#x27;);CREATE INDEX idx_emp_age_salary ON emp(age, salary); 第一种是通过对返回数据进行排序，所有不通过索引直接返回结果的排序都叫 FileSort 排序，会在内存中重新排序 1EXPLAIN SELECT * FROM emp ORDER BY age DESC;\t-- 年龄降序 第二种通过有序索引顺序扫描直接返回有序数据，这种情况为 Using index，不需要额外排序，操作效率高 1EXPLAIN SELECT id, age, salary FROM emp ORDER BY age DESC; 多字段排序： 123EXPLAIN SELECT id,age,salary FROM emp ORDER BY age DESC, salary DESC;EXPLAIN SELECT id,age,salary FROM emp ORDER BY salary DESC, age DESC;EXPLAIN SELECT id,age,salary FROM emp ORDER BY age DESC, salary ASC; Using filesort : 通过表的索引或全表扫描，读取满足条件的数据行，然后在排序缓冲区sort buffer中完成排序操作，所有不是通过索引直接返回排序结果的排序都叫 FileSort 排序。 Using index : 通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。 对于以上的两种排序方式，Using index的性能高，而Using filesort的性能低。 尽量减少额外的排序，通过索引直接返回有序数据。需要满足 Order by 使用相同的索引、Order By 的顺序和索引顺序相同、Order by 的字段都是升序或都是降序，否则需要额外的操作，就会出现 FileSort。 ORDER BY RAND() 命令用来进行随机排序，会使用了临时内存表，临时内存表排序的时使用 rowid 排序方法 优化方式：创建合适的索引能够减少 Filesort 的出现，但是某些情况下条件限制不能让 Filesort 消失，就要加快 Filesort 的排序操作。 内存临时表，MySQL 有两种 Filesort 排序算法： rowid 排序：首先根据条件取出排序字段和信息，然后在排序区 sort buffer（Server 层）中排序，如果 sort buffer 不够，则在临时表 temporary table 中存储排序结果。完成排序后再根据行指针回表读取记录，该操作可能会导致大量随机 I/O 操作说明：对于临时内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，不会导致多访问磁盘，优先选择该方式 全字段排序：一次性取出满足条件的所有数据，需要回表，然后在排序区 sort buffer 中排序后直接输出结果集。排序时内存开销较大，但是排序效率比两次扫描算法高 具体的选择方式： MySQL 通过比较系统变量 max_length_for_sort_data 的大小和 Query 语句取出的字段的大小，来判定使用哪种排序算法。如果前者大，则说明 sort buffer 空间足够，使用第二种优化之后的算法，否则使用第一种。 可以适当提高 sort_buffer_size 和 max_length_for_sort_data 系统变量，来增大排序区的大小，提高排序的效率 1234SET @@max_length_for_sort_data = 10000; -- 设置全局变量SET max_length_for_sort_data = 10240; -- 设置会话变量SHOW VARIABLES LIKE &#x27;max_length_for_sort_data&#x27;;\t-- 默认1024SHOW VARIABLES LIKE &#x27;sort_buffer_size&#x27;; -- 默认262114 磁盘临时表：排序使用优先队列（堆）的方式。 order by优化原则 根据排序字段建立合适的索引，多字段排序时，也遵循最左前缀法则。 尽量使用覆盖索引。 多字段排序, 一个升序一个降序，此时需要注意联合索引在创建时的规则（ASC/DESC）。 如果不可避免的出现filesort，大数据量排序时，可以适当增大排序缓冲区大小 sort_buffer_size (默认256k)。 GROUP BY 优化 GROUP BY 也会进行排序操作，与 ORDER BY 相比，GROUP BY 主要只是多了排序之后的分组操作，所以在 GROUP BY 的实现过程中，与 ORDER BY 一样也可以利用到索引。 分组查询： 12DROP INDEX idx_emp_age_salary ON emp;EXPLAIN SELECT age,COUNT(*) FROM emp GROUP BY age; Using temporary：表示 MySQL 需要使用临时表（不是 sort buffer）来存储结果集，常见于排序和分组查询。 查询包含 GROUP BY 但是用户想要避免排序结果的消耗， 则可以执行 ORDER BY NULL 禁止排序： 1EXPLAIN SELECT age,COUNT(*) FROM emp GROUP BY age ORDER BY NULL; 创建索引：索引本身有序，不需要临时表，也不需要再额外排序 1CREATE INDEX idx_emp_age_salary ON emp(age, salary); 数据量很大时，使用 SQL_BIG_RESULT 提示优化器直接使用直接用磁盘临时表 在分组操作中，需要通过以下两点进行优化，以提升性能： 在分组操作时，可以通过索引来提高效率。 分组操作时，索引的使用也是满足最左前缀法则的。 LIMIT 分页优化 一般分页查询时，通过创建覆盖索引能够比较好地提高性能。 一个常见的问题是 LIMIT 200000,10，此时需要 MySQL 扫描前 200010 记录，仅仅返回 200000 - 200010 之间的记录，其他记录丢弃，查询排序的代价非常大。 执行limit分页查询耗时对比：越往后，分页查询效率越低，这就是分页查询的问题所在。 因为，当在进行分页查询时，如果执行 limit 2000000,10 ，此时需要MySQL排序前2000010 记录，仅仅返回 2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。 优化思路（覆盖索引+子查询） 一般分页查询时，通过创建覆盖索引 能够比较好地提高性能，可以通过覆盖索引加子查询形式进行优化 COUNT 计数优化 在不同的 MySQL 引擎中，count(*) 有不同的实现方式： MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高，但不支持事务 InnoDB 表执行count(*) 会遍历全表，虽然结果准确，但会导致性能问题 show table status 命令通过采样估算可以快速获取，但是不准确 解决方案： 计数保存在 Redis 中，但是更新 MySQL 和 Redis 的操作不是原子的，会存在数据一致性的问题 计数直接放到数据库里单独的一张计数表中，利用事务解决计数精确问题： 会话 B 的读操作在 T3 执行的，这时更新事务还没有提交，所以计数值加 1 这个操作对会话 B 还不可见，因此会话 B 查询的计数值和最近 100 条记录，返回的结果逻辑上就是一致的并发系统性能的角度考虑，应该先插入操作记录再更新计数表，因为更新计数表涉及到行锁的竞争，先插入再更新能最大程度地减少事务之间的锁等待，提升并发度。 count 函数的按照效率排序：count(字段) &lt; count(主键id) &lt; count(1) ≈ count()，所以建议尽量使用 count()。 count(字段)： 如果这个字段是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个字段定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加 count(主键 id)：InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来返回给 Server 层，Server 判断 id 不为空就按行累加 count(1)：InnoDB 引擎遍历整张表但不取值，Server 层对于返回的每一行，放一个数字 1 进去，判断不为空就按行累加 count(*)：不取值，按行累加 UPDATE 更新优化 当我们在执行更新的SQL语句时，会锁定该行的数据，然后事务提交之后，行锁释放。如果 where 条件的列并未添加索引，此时行锁升级为了表锁。 导致该 update 语句的性能大大降低。 联合查询 OR 对于包含 OR 的查询子句，如果要利用索引，则 OR 之间的每个条件列都必须用到索引，而且不能使用到条件之间的复合索引，如果没有索引，则应该考虑增加索引。 执行查询语句： 1EXPLAIN SELECT * FROM emp WHERE id = 1 OR age = 30;\t-- 两个索引，并且不是复合索引! 1Extra: Using sort_union(idx_emp_age_salary,PRIMARY); Using where 使用 UNION 替换 OR，求并集： 注意：该优化只针对多个索引列有效，如果有列没有被索引，查询效率可能会因为没有选择 OR 而降低。 1EXPLAIN SELECT * FROM emp WHERE id = 1 UNION SELECT * FROM emp WHERE age = 30; UNION 要优于 OR 的原因： UNION 语句的 type 值为 ref，OR 语句的 type 值为 range UNION 语句的 ref 值为 const，OR 语句的 ref 值为 null，const 表示是常量值引用，非常快 嵌套(子)查询 MySQL 4.1 版本之后，开始支持 SQL 的子查询。 可以使用 SELECT 语句来创建一个单列的查询结果，然后把结果作为过滤条件用在另一个查询中 使用子查询可以一次性的完成逻辑上需要多个步骤才能完成的 SQL 操作，同时也可以避免事务或者表锁死 在有些情况下，子查询是可以被更高效的连接（JOIN）替代 例如查找有角色的所有的用户信息： 执行计划： 1EXPLAIN SELECT * FROM t_user WHERE id IN (SELECT user_id FROM user_role); 优化后： 1EXPLAIN SELECT * FROM t_user u , user_role ur WHERE u.id = ur.user_id; 连接查询之所以效率更高 ，是因为不需要在内存中创建临时表来完成逻辑上需要两个步骤的查询工作。","tags":["MySQL"],"categories":["学习笔记"]},{"title":"MySQL--锁机制","path":"/2024/01/26/MySQL--锁机制/","content":"基本介绍 锁定机制简单来说，就是数据库为了保证数据的一致性，而使各种共享资源在被并发访问变得有序所设计的一种规则。 利用 MVCC 性质进行读取的操作叫一致性读，读取数据前加锁的操作叫锁定读。 MySQL 数据库由于其自身架构的特点，存在多种数据存储引擎，每种存储引擎的锁定机制都是为各自所面对的特定场景而优化设计，所以各存储引擎的锁定机制也有较大区别。 锁的分类： 按操作属性分类： 共享锁：也叫读锁。对同一份数据，多个事务读操作可以同时加锁而不互相影响 ，但不能修改数据 排他锁：也叫写锁，独占锁。当前的操作没有完成前，会阻断其他操作的读取和写入 按粒度分类： 全局锁：锁的粒度最大，并发度最低 表级锁：会锁定整个表，开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低，偏向 MyISAM 行级锁：会锁定当前操作行，开销大，加锁慢；会出现死锁；锁定力度小，发生锁冲突概率低，并发度高，偏向 InnoDB 页级锁：锁的力度、发生冲突的概率和加锁开销介于表锁和行锁之间，会出现死锁，并发性能一般 按使用方式分类： 悲观锁：每次查询数据时都认为别人会修改，很悲观，所以查询时加锁 乐观锁：每次查询数据时都认为别人不会修改，很乐观，但是更新时会判断一下在此期间别人有没有去更新这个数据 不同存储引擎支持的锁 存储引擎 表级锁 行级锁 页级锁 MyISAM 支持 不支持 不支持 InnoDB 支持 支持 不支持 MEMORY 支持 不支持 不支持 BDB 支持 不支持 支持 从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如 Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理系统。 内存结构 对一条记录加锁的本质就是在内存中创建一个锁结构与之关联，结构包括 事务信息：锁对应的事务信息，一个锁属于一个事务 索引信息：对于行级锁，需要记录加锁的记录属于哪个索引 表锁和行锁信息：表锁记录着锁定的表，行锁记录了 Space ID 所在表空间、Page Number 所在的页号、n_bits 使用了多少比特 type_mode：一个 32 比特的数，被分成 lock_mode、lock_type、rec_lock_type 三个部分 lock_mode：锁模式，记录是共享锁、排他锁、意向锁之类 lock_type：代表表级锁还是行级锁 rec_lock_type：代表行锁的具体类型和 is_waiting 属性，is_waiting = true 时表示当前事务尚未获取到锁，处于等待状态。事务获取锁后的锁结构是 is_waiting 为 false，释放锁时会检查是否与当前记录关联的锁结构，如果有就唤醒对应事务的线程 一个事务可能操作多条记录，为了节省内存，满足下面条件的锁使用同一个锁结构： 在同一个事务中的加锁操作 被加锁的记录在同一个页面中 加锁的类型是一样的 加锁的状态是一样的 全局锁 介绍 FLUSH TABLES WITH READ LOCK 简称（FTWRL），全局读锁，在 server 层实现，让整个库处于只读状态，DDL DML 都被阻塞，工作流程： 上全局读锁（lock_global_read_lock） 清理表缓存（close_cached_tables） 上全局 COMMIT 锁（make_global_read_lock_block_commit） 全局锁就是对数据库的整个实例加锁， 加锁之后整个实例就处于只读状态，后续的 DML 语句，DDL 语句，以及更新操作的事务提交语句都会被阻塞，全局锁的典型使用场景就是进行全库的逻辑备份，对所有的表进行锁定，从而获取一致性视图，保证数据的完整性。 下面我们通过一个简单的例子来介绍一下不加全局锁时，可能出现的问题：假设数据库中存在着三张表 ， tb_stock 库存表，tb_order 订单表，tb_orderlog 订单日志表。 在进行数据备份时 ： 先备份了 tb _ stock 表 接下来， 在业务系统中，执行了下单操作，扣减库存，生成订单，(更新 tb_stock 表， 插入 tb _ order 表) 再执行备份 tb_order 表 的逻辑 业务中执行插入订单日志的操作 最后 ， 备份了 tb_orderlog 表 上述的执行过程中， 备份出来的数据是存在问题的 ， 因为备份出来的数据 ， tb_stock 表 和 tb_order 表 存在着数据不一致的问题， 有最新的订单信息 ， 但是总的库存数没有发生变化。 那如何规避这种数据不一致的现象呢 ， 此时就需要借助 MySQL 的全局锁来解决： 在进行数据库的逻辑备份之前， 先对数据库加上全局锁，一旦加上全局锁之后，其他的 DDL，DML 全部都处于阻塞状态。 但是可以执行 DQL 语句，也就是只读状态，而数据备份就是查询操作， 那么在数据备份的过程中 ， 数据库中的数据是不会发生变化的，这样就保证了数据的完整性和一致性。 语法 加全局锁 1flush tables with read lock; 该命令主要用于备份工具做一致性备份，由于 FTWRL 需要持有两把全局的 MDL 锁，并且还要关闭所有表对象，因此杀伤性很大。 数据备份 1mysqldump -u root –p 密码 数据库名 &gt; 文件名.sql 释放锁 1unlock tables; 特点 数据库中如果加全局锁 ， 是一个粒度比较重的操作， 容易存在以下问题： 如果在主库上进行备份，那么在备份期间都不能执行更新操作，业务基本就处于停摆 如果在从库上进行备份，那么在备份期间，从库不能执行主库同步过来的二进制文件 ，就会导致主从延迟 在 InnoDB 引擎中， 可以通过备份时增加一个参数来完成不加锁的数据一致性备份。 1mysqldump --single-transaction -u root –p 密码 数据库名 &gt; 文件名.sql 表级锁 表级锁 ， 顾名思义，每次操作能够锁住整张表， 锁定粒度大，发生锁冲突概率较高，并发度最低 ， 通常应用在 InnoDB ， MyISAM， BDB 等引擎当中， 此处我们只对 InnoDB 中的表级锁进行详解。 表锁 表共享读锁(read lock ) 对指定表加了读锁之后，当事务 T1 进行读操作时，不会影响事务 T2 的读，但是都会阻塞事务间的写操作。 表独占写锁(write lock) 针对指定表增加了独占写锁之后， 事务 T1 可以针对表进行读和写， 而事务 T2 的读和写就会被阻塞。 加锁 1lock tables 表名... read/write 释放锁 1unlock tables / 客户端断开连接 元数据锁（Meta Data Lock ） meta data lock ， 元数据锁，简写 MDL。 MDL加锁过程是系统自动控制，无需显式使用，在访问一张表的时候会自动加上。MDL锁主要作用是维护表元数据的数据一致性，在表上有活动事务的时候，不可以对元数据进行写入操作。为了避免DML 与 DDL冲突，保证读写的正确性。当对一个表做增删改查的时候，加 MDL 读锁；当要对表做结构变更操作 DDL 的时候，加 MDL 写锁，两种锁不相互兼容，所以可以保证 DDL、DML、DQL 操作的安全。 这里的元数据，可以简单理解为就是一张表的表结构。也就是说，某一张表涉及到未提交的事务时，是不能够修改这张表的表结构的。 在MySQL5.5中引入了MDL，当对一张表进行增删改查的时候，加MDL读锁(共享)；当对表结构进行变更操作的时候，加MDL写锁(排他)。 MDL 锁的特性： MDL 锁不需要显式使用，在访问一个表的时候会被自动加上，在事务开始时申请，整个事务提交后释放（执行完单条语句不释放） MDL 锁是在 Server 层中实现，不是 InnoDB 存储引擎层能直接实现的锁 MDL 锁还能实现其他粒度级别的锁，比如全局锁、库级别的锁、表空间级别的锁 那如果数据库有一个长事务（所谓的长事务，就是开启了事务，但是一直还没提交），那在对表结构做变更操作的时候，可能会发生意想不到的事情，比如下面这个顺序的场景： 首先，线程 A 先启用了事务（但是一直不提交），然后执行一条 select 语句，此时就先对该表加上 MDL 读锁； 然后，线程 B 也执行了同样的 select 语句，此时并不会阻塞，因为「读读」并不冲突； 接着，线程 C 修改了表字段，此时由于线程 A 的事务并没有提交，也就是 MDL 读锁还在占用着，这时线程 C 就无法申请到 MDL 写锁，就会被阻塞， 那么在线程 C 阻塞后，后续有对该表的 select 语句，就都会被阻塞，如果此时有大量该表的 select 语句的请求到来，就会有大量的线程被阻塞住，这时数据库的线程很快就会爆满了。 这是因为申请 MDL 锁的操作会形成一个队列，队列中中锁，一旦出现 MDL 写锁等待，会阻塞后续该表的所有 CRUD 操作。 演示： 当执行SELECT、INSERT、UPDATE、DELETE等语句时，添加的是元数据共享锁（SHARED_READ / SHARED_WRITE），之间是兼容的。 当执行SELECT语句时，添加的是元数据共享锁（SHARED_READ），会阻塞元数据排他锁 （EXCLUSIVE），之间是互斥的。 可以通过下面的SQL，来查看数据库中的元数据锁的情况： 1select object_type，object_schema，object_name，lock_type，lock_duration from performance_schema.metadata_locks ; MDL 是在事务提交后才会释放，这意味着事务执行期间，MDL 是一直持有的。 意向锁（Intention Lock） 为了避免DML在执行时，加的行锁与表锁的冲突，在InnoDB中引入了意向锁，使得表锁不用检查每行数据是否加锁，使用意向锁来减少表锁的检查。 假如没有意向锁，客户端一对表加了行锁后，客户端二如何给表加表锁呢，来通过示意图简单分析一下： 首先客户端一，开启一个事务，然后执行DML操作，在执行DML语句时，会对涉及到的行加行锁。 当客户端二想要对这张表进行加表锁时，会检查当前表是否有对应的行锁，如果没有，则添加表锁，此时就会从第一行数据检查到最后一行 ， 效率较低。 有了意向锁之后， 在执行DML操作时，会对涉及的行加上行锁，同时也会给该表加上意向锁。 而其他客户端对该表进行加表锁时，就可以根据是否存在意向锁来判断是否可以成功加锁。 意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度上进行加锁，意向锁分为两种： 意向共享锁( IS ): 由语句 select ... lock in share mode 添加 。 与表锁共享锁 ( read )兼容，与表锁排他锁(write)互斥。 意向排他锁( IX ): 由 insert、update、delete、select...for update 添加 。与表锁共享锁( read )及排他锁( write )都互斥，意向锁之间不会互斥。 兼容性如下所示： 一旦事务提交了，意向共享锁、意向排他锁，都会自动释放。 意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables ... read）和独占表锁（lock tables ... write）发生冲突。 在执行插入、更新、删除操作前，需要先对表加上「意向独占锁」，然后对该记录加独占锁 ， 这样就可以快速判断表中是否有记录加锁。 可以通过以下SQL，查看意向锁及行锁的加锁情况： 1select object_schema，object_name，index_name，lock_type，lock_mode，lock_data from performance_schema.data_locks; 演示： 意向共享锁与表读锁是兼容的 意向排他锁与表读锁、写锁都是互斥的 行级锁 行级锁，每次操作锁住对应的行数据。锁定粒度最小，发生锁冲突的概率最低，并发度最高。应用在 InnoDB 存储引擎中。 行锁（Record Lock） 锁定单个行记录的锁，防止其他事务对此行进行update和delete，也称为记录锁。在 RC、RR 隔离级别下都支持。 InnoDB实现了以下两种类型的行锁： 共享锁 (S)：又称为读锁，简称 S 锁，多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改 排他锁 (X)：又称为写锁，简称 X 锁，不能与其他锁并存，获取排他锁的事务是可以对数据读取和修改 RR 隔离级别下，对于 UPDATE、DELETE 和 INSERT 语句，InnoDB 会自动给涉及数据集加排他锁（行锁），在 commit 时自动释放；对于普通 SELECT 语句，不会加任何锁（只是针对 InnoDB 层来说的，因为在 Server 层会加 MDL 读锁），通过 MVCC 防止并发冲突。 在事务中加的锁，并不是不需要了就释放，而是在事务中止或提交时自动释放，这个就是两阶段锁协议。所以一般将更新共享资源（并发高）的 SQL 放到事务的最后执行，可以让其他线程尽量的减少等待时间。 两种行锁的兼容情况如下: 显式给数据集加共享锁或排他锁：加锁读就是当前读，读取的是最新数据 12SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE\t-- 共享锁SELECT * FROM table_name WHERE ... FOR UPDATE -- 排他锁 默认情况下，InnoDB在 REPEATABLE READ 事务隔离级别运行，InnoDB使用 next-key 锁进行搜索和索引扫描，以防止幻读。 针对唯一索引进行检索时，对已存在的记录进行等值匹配时，将会自动优化为行锁。 InnoDB的行锁是针对于索引加的锁，不通过索引条件检索数据，那么 InnoDB 将对表中的所有记录加锁，此时就会升级为表锁。 可以通过以下 SQL，查看意向锁以及行锁的加锁情况： 1select object_schema，object_name，index_name，lock_type，lock_mode，lock_data from performance_schema.data_locks; 常见的SQL语句，在执行时，所加的行锁如下： 注意：锁默认会锁聚簇索引（锁就是加在索引上），但是当使用覆盖索引时，加共享锁只锁二级索引，不锁聚簇索引。 示例演示 普通的select语句，执行时，不会加锁。 select...lock in share mode，加共享锁，共享锁与共享锁之间兼容 共享锁与排他锁之间互斥 客户端一获取的是 id 为 1 这行的共享锁，客户端二是可以获取 id 为 3 这行的排它锁的，因为不是同一行数据。 而如果客户端二想获取 id 为 1 这行的排他锁，会处于阻塞状态，以为共享锁与排他锁之间互斥。 排它锁与排他锁之间互斥 当客户端一，执行update语句，会为id为1的记录加排他锁； 客户端二，如果也执行update语句更新id为1的数据，也要为id为1的数据加排他锁，但是客户端二会处于阻塞状态，因为排他锁之间是互斥的。 直到客户端一，把事务提交了，才会把这一行的行锁释放，此时客户端二，解除阻塞。 间隙锁（Gap Lock）&amp; 临键锁（Next-Key Lock） InnoDB 会对间隙（GAP）进行加锁（不含该记录），就是间隙锁 （RR 隔离级别下才有该锁），确保索引记录间隙不变，防止其他事务在这个间隙进行insert，产生幻读。间隙锁之间不存在冲突关系，多个事务可以同时对一个间隙加锁，但是间隙锁会阻止往这个间隙中插入一个记录的操作。 临键锁（Next-Key Lock），行锁和间隙锁组合，同时锁住数据，并锁住数据前面的间隙Gap。在RR隔离级别下支持。 InnoDB 加锁的基本单位是 next-key lock，该锁是行锁和 gap lock 的组合（X or S 锁），但是加锁过程是分为间隙锁和行锁两段执行。 可以保护当前记录和前面的间隙，遵循左开右闭原则，单纯的间隙锁是左开右开 假设有 10、11、13，那么可能的间隙锁包括：(负无穷,10]、(10,11]、(11,13]、(13,正无穷) 几种索引的加锁情况： 唯一索引加锁在值存在时是行锁，next-key lock 会退化为行锁，值不存在会变成间隙锁 普通索引加锁会继续向右遍历到不满足条件的值为止，next-key lock 退化为间隙锁 范围查询无论是否是唯一索引，都需要访问到不满足条件的第一个值为止 对于联合索引且是唯一索引，如果 where 条件只包括联合索引的一部分，那么会加间隙锁 间隙锁优点： RR 级别下间隙锁可以解决事务的一部分的幻读问题，通过对间隙加锁，可以防止读取过程中数据条目发生变化。一部分的意思是不会对全部间隙加锁，只能加锁一部分的间隙。 间隙锁危害： 当锁定一个范围的键值后，即使某些不存在的键值也会被无辜的锁定，造成在锁定的时候无法插入锁定键值范围内的任何数据，在某些场景下这可能会对性能造成很大的危害，影响并发度 事务 A B 同时锁住一个间隙后，A 往当前间隙插入数据时会被 B 的间隙锁阻塞，B 也执行插入间隙数据的操作时就会产生死锁 注：间隙锁唯一目的是防止其他事务插入间隙。间隙锁可以共存，一个事务采用的间隙锁不会阻止另一个事务在同一间隙上采用间隙锁。 示例演示 索引上的等值查询(唯一索引)，给不存在的记录加锁时， 优化为间隙锁 我们知道InnoDB的B+树索引，叶子节点是有序的双向链表。 假如，我们要根据这个二级索引查询值为18的数据，并加上共享锁，我们是只锁定18这一行就可以了吗？ 并不是，因为是非唯一索引，这个结构中可能有多个18的存在，所以，在加锁时会继续往后找，找到一个不满足条件的值（当前案例中也就是29）。此时会对18加临键锁，并对29之前的间隙加锁。 索引上的范围查询(唯一索引)–会访问到不满足条件的第一个值为止 查询的条件为id&gt;=19，并添加共享锁。 此时我们可以根据数据库表中现有的数据，将数据分为三个部分：[19] 、(19，25] 、(25，+∞]。 所以数据库数据在加锁是，就是将19加了行锁，25的临键锁（包含25及25之前的间隙），正无穷的临键锁(正无穷及之前的间隙)。 其他锁 自增锁 系统会自动给 AUTO_INCREMENT 修饰的列进行递增赋值，实现方式： AUTO_INC 锁：表级锁，执行插入语句时会自动添加，在该语句执行完成后释放，并不是事务结束 轻量级锁：为插入语句生成 AUTO_INCREMENT 修饰的列时获取该锁，生成以后释放掉，不需要等到插入语句执行完后释放 系统变量 innodb_autoinc_lock_mode控制采取哪种方式： 0：全部采用 AUTO_INC 锁 1：全部采用轻量级锁 2：混合使用，在插入记录的数量确定时采用轻量级锁，不确定时采用 AUTO_INC 锁 隐式锁 一般情况下 INSERT 语句是不需要在内存中生成锁结构的，会进行隐式的加锁，保护的是插入后的安全。 注意：如果插入的间隙被其他事务加了间隙锁，此次插入会被阻塞，并在该间隙插入一个插入意向锁 聚簇索引：索引记录有 trx_id 隐藏列，表示最后改动该记录的事务 id，插入数据后事务 id 就是当前事务。其他事务想获取该记录的锁时会判断当前记录的事务 id 是否是活跃的，如果不是就可以正常加锁；如果是就创建一个 X 的锁结构，该锁的 is_waiting 是 false，为自己的事务创建一个锁结构，is_waiting 是 true（类似 Java 中的锁升级） 二级索引：获取数据页 Page Header 中的 PAGE_MAX_TRX_ID 属性，代表修改当前页面的最大的事务 ID，如果小于当前活跃的最小事务 id，就证明插入该数据的事务已经提交，否则就需要获取到主键值进行回表操作 隐式锁起到了延迟生成锁的效果，如果其他事务与隐式锁没有冲突，就可以避免锁结构的生成，节省了内存资源。 INSERT 在两种情况下会生成锁结构： 重复键：在插入主键或唯一二级索引时遇到重复的键值会报错，在报错前需要对对应的聚簇索引进行加锁 隔离级别 &lt;= Read Uncommitted，加 S 型 Record Lock 隔离级别 &gt;= Repeatable Read，加 S 型 next_key 锁 外键检查：如果待插入的记录在父表中可以找到，会对父表的记录加 S 型 Record Lock。如果待插入的记录在父表中找不到 隔离级别 &lt;= Read Committed，不加锁 隔离级别 &gt;= Repeatable Read，加间隙锁 锁优化 优化锁 InnoDB 存储引擎实现了行级锁定，虽然在锁定机制的实现方面带来了性能损耗可能比表锁会更高，但是在整体并发处理能力方面要远远优于 MyISAM 的表锁，当系统并发量较高的时候，InnoDB 的整体性能远远好于 MyISAM。 但是使用不当可能会让 InnoDB 的整体性能表现不仅不能比 MyISAM 高，甚至可能会更差。 优化建议： 尽可能让所有数据检索都能通过索引来完成，避免无索引行锁升级为表锁 合理设计索引，尽量缩小锁的范围 尽可能减少索引条件及索引范围，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可使用低级别事务隔离（需要业务层面满足需求） 锁升级 索引失效造成行锁升级为表锁，不通过索引检索数据，全局扫描的过程中 InnoDB 会将对表中的所有记录加锁，实际效果和表锁一样，实际开发过程应避免出现索引失效的状况。 查看当前表的索引： 1SHOW INDEX FROM test_innodb_lock; 关闭自动提交功能： 1SET AUTOCOMMIT=0;\t-- C1、C2 执行更新语句： 12UPDATE test_innodb_lock SET sex=&#x27;2&#x27; WHERE name=10;\t-- C1UPDATE test_innodb_lock SET sex=&#x27;2&#x27; WHERE id=3; -- C2 示例： 索引失效：执行更新时 name 字段为 varchar 类型，造成索引失效，最终行锁变为表锁。 死锁 不同事务由于互相持有对方需要的锁而导致事务都无法继续执行的情况称为死锁。 死锁情况：线程 A 修改了 id = 1 的数据，请求修改 id = 2 的数据，线程 B 修改了 id = 2 的数据，请求修改 id = 1 的数据，产生死锁。 解决策略： 直接进入等待直到超时，超时时间可以通过参数 innodb_lock_wait_timeout 来设置，默认 50 秒，但是时间的设置不好控制，超时可能不是因为死锁，而是因为事务处理比较慢，所以一般不采取该方式 主动死锁检测，发现死锁后主动回滚死锁链条中较小的一个事务，让其他事务得以继续执行，将参数 innodb_deadlock_detect 设置为 on，表示开启该功能（事务较小的意思就是事务执行过程中插入、删除、更新的记录条数）死锁检测并不是每个语句都要检测，只有在加锁访问的行上已经有锁时，当前事务被阻塞了才会检测，也是从当前事务开始进行检测 通过执行SHOW ENGINE INNODB STATUS可以查看最近发生的一次死循环，全局系统变量 innodb_print_all_deadlocks 设置为 on，就可以将每个死锁信息都记录在 MySQL 错误日志中。 死锁一般是行级锁，当表锁发生死锁时，会在事务中访问其他表时直接报错，破坏了持有并等待的死锁条件。 锁状态 查看锁信息 1SHOW STATUS LIKE &#x27;innodb_row_lock%&#x27;; 参数说明： Innodb_row_lock_current_waits：当前正在等待锁定的数量 Innodb_row_lock_time：从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg：每次等待所花平均时长 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花的时间 Innodb_row_lock_waits：系统启动后到现在总共等待的次数 当等待的次数很高，而且每次等待的时长也不短的时候，就需要分析系统中为什么会有如此多的等待，然后根据分析结果制定优化计划。 查看锁状态： 12SELECT * FROM information_schema.innodb_locks;\t#锁的概况SHOW ENGINE INNODB STATUS\\G; #InnoDB整体状态，其中包括锁的情况 lock_id 是锁 id；lock_trx_id 为事务 id；lock_mode 为 X 代表排它锁（写锁）；lock_type 为 RECORD 代表锁为行锁（记录锁）。","tags":["MySQL"],"categories":["学习笔记"]},{"title":"MySQL--高级结构","path":"/2024/01/24/MySQL--高级结构/","content":"视图 基本介绍 视图概念：视图是一种虚拟存在的数据表，这个虚拟的表并不在数据库中实际存在。 本质：将一条 SELECT 查询语句的结果封装到了一个虚拟表中，所以在创建视图的时候，工作重心要放在这条 SELECT 查询语句上。 作用：将一些比较复杂的查询语句的结果，封装到一个虚拟表中，再有相同查询需求时，直接查询该虚拟表。 优点： 简单：使用视图的用户不需要关心表的结构、关联条件和筛选条件，因为虚拟表中已经是过滤好的结果集 安全：使用视图的用户只能访问查询的结果集，对表的权限管理并不能限制到某个行某个列 数据独立，一旦视图的结构确定，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响 视图创建 创建视图 1234CREATE [OR REPLACE] VIEW 视图名称 [(列名列表)] AS 查询语句[WITH [CASCADED | LOCAL] CHECK OPTION]; WITH [CASCADED | LOCAL] CHECK OPTION 决定了是否允许更新数据使记录不再满足视图的条件： LOCAL：只要满足本视图的条件就可以更新 CASCADED：必须满足所有针对该视图的所有视图的条件才可以更新，默认值 例如 123456789101112131415161718192021222324252627-- 数据准备 cityid\tNAME\tcid1 深圳 12 上海 13 纽约 24 莫斯科 3-- 数据准备 countryid\tNAME1 中国2 美国3 俄罗斯-- 创建city_country视图，保存城市和国家的信息(使用指定列名)CREATE VIEW city_country (city_id,city_name,country_name)AS SELECT c1.id, c1.name, c2.name FROM city c1, country c2 WHERE c1.cid=c2.id; 视图查询 查询所有数据表，视图也会查询出来 12SHOW TABLES;SHOW TABLE STATUS [\\G]; 查询视图 1SELECT * FROM 视图名称; 查询某个视图创建 1SHOW CREATE VIEW 视图名称; 视图修改 视图表数据修改，会自动修改源表中的数据，因为更新的是视图中的基表中的数据。 修改视图表中的数据 1UPDATE 视图名称 SET 列名 = 值 WHERE 条件; 修改视图的结构 12345678910111213141516171819ALTER [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;]VIEW 视图名称 [(列名列表)] AS 查询语句[WITH [CASCADED | LOCAL] CHECK OPTION]-- 将视图中的country_name修改为nameALTER VIEW city_country (city_id,city_name,name) AS SELECT c1.id, c1.name, c2.name FROM city c1, country c2 WHERE c1.cid=c2.id; 要使视图可更新，视图中的行与基础表中的行之间必须存在一对一的关系。如果视图包含以下任何一项，则该视图不可更新： 聚合函数或窗口函数（SUM()、 MIN()、 MAX()、 COUNT()等） DISTINCT GROUP BY HAVING UNION 或者 UNION ALL 视图删除 删除视图 1DROP VIEW 视图名称; 如果存在则删除 1DROP VIEW IF EXISTS 视图名称; 存储过程 基本介绍 存储过程和函数：存储过程和函数是事先经过编译并存储在数据库中的一段 SQL 语句的集合。 存储过程和函数的好处： 提高代码的复用性 减少数据在数据库和应用服务器之间的传输，提高传输效率 减少代码层面的业务处理 一次编译永久有效 存储过程和存储函数的区别： 存储函数必须有返回值 存储过程可以没有返回值 基本操作 DELIMITER： DELIMITER 关键字用来声明 sql 语句的分隔符，告诉 MySQL 该段命令已经结束 MySQL 语句默认的分隔符是分号，但是有时需要一条功能 sql 语句中包含分号，但是并不作为结束标识，这时使用 DELIMITER 来指定分隔符：DELIMITER 分隔符 存储过程的创建调用查看和删除： 创建存储过程 1234567891011-- 修改分隔符为$DELIMITER $-- 标准语法CREATE PROCEDURE 存储过程名称(参数...)BEGIN\tsql语句;END$-- 修改分隔符为分号DELIMITER ; 调用存储过程 1CALL 存储过程名称(实际参数); 查看存储过程 1SELECT * FROM mysql.proc WHERE db=&#x27;数据库名称&#x27;; 删除存储过程 1DROP PROCEDURE [IF EXISTS] 存储过程名称; 练习： 数据准备 12345id\tNAME age gender\tscore1 张三 23 男 952 李四 24 男 983 王五 25 女 1004 赵六 26 女 90 创建 stu_group() 存储过程，封装分组查询总成绩，并按照总成绩升序排序的功能 12345678910111213DELIMITER $CREATE PROCEDURE stu_group()BEGIN\tSELECT gender,SUM(score) getSum FROM student GROUP BY gender ORDER BY getSum ASC; END$DELIMITER ;-- 调用存储过程CALL stu_group();-- 删除存储过程DROP PROCEDURE IF EXISTS stu_group; 存储语法 变量使用 存储过程是可以进行编程的，意味着可以使用变量、表达式、条件控制语句等，来完成比较复杂的功能。 定义变量：DECLARE 定义的是局部变量，只能用在 BEGIN END 范围之内 1DECLARE 变量名 数据类型 [DEFAULT 默认值]; 变量的赋值 12SET 变量名 = 变量值;SELECT 列名 INTO 变量名 FROM 表名 [WHERE 条件]; 数据准备：表 student 12345id\tNAME\tage gender\tscore1\t张三 23 男 952\t李四 24 男 983\t王五 25 女 1004\t赵六 26 女 90 定义两个 int 变量，用于存储男女同学的总分数 12345678910111213141516DELIMITER $CREATE PROCEDURE pro_test3()BEGIN\t-- 定义两个变量\tDECLARE men,women INT;\t-- 查询男同学的总分数，为men赋值\tSELECT SUM(score) INTO men FROM student WHERE gender=&#x27;男&#x27;;\t-- 查询女同学的总分数，为women赋值\tSELECT SUM(score) INTO women FROM student WHERE gender=&#x27;女&#x27;;\t-- 使用变量\tSELECT men,women;END$DELIMITER ;-- 调用存储过程CALL pro_test3(); IF语句 if 语句标准语法 12345IF 判断条件1 THEN 执行的sql语句1; [ELSEIF 判断条件2 THEN 执行的sql语句2;] ... [ELSE 执行的sql语句n;]END IF; 数据准备：表 student 12345id\tNAME\tage gender\tscore1\t张三 23 男 952\t李四 24 男 983\t王五 25 女 1004\t赵六 26 女 90 根据总成绩判断：全班 380 分及以上学习优秀、320 ~ 380 学习良好、320 以下学习一般 12345678910111213141516171819DELIMITER $CREATE PROCEDURE pro_test4()BEGIN\tDECLARE total INT; -- 定义总分数变量\tDECLARE description VARCHAR(10); -- 定义分数描述变量\tSELECT SUM(score) INTO total FROM student; -- 为总分数变量赋值\t-- 判断总分数\tIF total &gt;= 380 THEN SET description = &#x27;学习优秀&#x27;;\tELSEIF total &gt;=320 AND total &lt; 380 THEN SET description = &#x27;学习良好&#x27;;\tELSE SET description = &#x27;学习一般&#x27;;\tEND IF;END$DELIMITER ;-- 调用pro_test4存储过程CALL pro_test4(); 参数传递 参数传递的语法IN：代表输入参数，需要由调用者传递实际数据，默认的 OUT：代表输出参数，该参数可以作为返回值 INOUT：代表既可以作为输入参数，也可以作为输出参数 123456789DELIMITER $-- 标准语法CREATE PROCEDURE 存储过程名称([IN|OUT|INOUT] 参数名 数据类型)BEGIN\t执行的sql语句;END$DELIMITER ; 输入总成绩变量，代表学生总成绩，输出分数描述变量，代表学生总成绩的描述 1234567891011121314151617181920DELIMITER $CREATE PROCEDURE pro_test6(IN total INT, OUT description VARCHAR(10))BEGIN\t-- 判断总分数\tIF total &gt;= 380 THEN SET description = &#x27;学习优秀&#x27;;\tELSEIF total &gt;= 320 AND total &lt; 380 THEN SET description = &#x27;学习不错&#x27;;\tELSE SET description = &#x27;学习一般&#x27;;\tEND IF;END$DELIMITER ;-- 调用pro_test6存储过程CALL pro_test6(310,@description);CALL pro_test6((SELECT SUM(score) FROM student), @description);-- 查询总成绩描述SELECT @description; 查看参数方法 @变量名 : 用户会话变量，代表整个会话过程他都是有作用的，类似于全局变量 @@变量名 : 系统变量 CASE 标准语法 1 123456CASE 表达式 WHEN 值1 THEN 执行sql语句1; [WHEN 值2 THEN 执行sql语句2;] ... [ELSE 执行sql语句n;]END CASE; 标准语法 2 123456sCASE WHEN 判断条件1 THEN 执行sql语句1; [WHEN 判断条件2 THEN 执行sql语句2;] ... [ELSE 执行sql语句n;]END CASE; 演示 12345678910111213141516171819202122DELIMITER $CREATE PROCEDURE pro_test7(IN total INT)BEGIN\t-- 定义变量\tDECLARE description VARCHAR(10);\t-- 使用case判断\tCASE\tWHEN total &gt;= 380 THEN SET description = &#x27;学习优秀&#x27;;\tWHEN total &gt;= 320 AND total &lt; 380 THEN SET description = &#x27;学习不错&#x27;;\tELSE SET description = &#x27;学习一般&#x27;;\tEND CASE; -- 查询分数描述信息\tSELECT description;END$DELIMITER ;-- 调用pro_test7存储过程CALL pro_test7(390);CALL pro_test7((SELECT SUM(score) FROM student)); WHILE while 循环语法 1234WHILE 条件判断语句 DO\t循环体语句;\t条件控制语句;END WHILE; 计算 1~100 之间的偶数和 123456789101112131415161718192021DELIMITER $CREATE PROCEDURE pro_test6()BEGIN\t-- 定义求和变量\tDECLARE result INT DEFAULT 0;\t-- 定义初始化变量\tDECLARE num INT DEFAULT 1;\t-- while循环\tWHILE num &lt;= 100 DO IF num % 2 = 0 THEN SET result = result + num; END IF; SET num = num + 1;\tEND WHILE;\t-- 查询求和结果\tSELECT result;END$DELIMITER ;-- 调用pro_test6存储过程CALL pro_test6(); REPEAT repeat 循环标准语法 123456初始化语句;REPEAT\t循环体语句;\t条件控制语句;\tUNTIL 条件判断语句END REPEAT; 计算 1~10 之间的和 1234567891011121314151617181920212223DELIMITER $CREATE PROCEDURE pro_test9()BEGIN\t-- 定义求和变量\tDECLARE result INT DEFAULT 0;\t-- 定义初始化变量\tDECLARE num INT DEFAULT 1;\t-- repeat循环\tREPEAT -- 累加 SET result = result + num; -- 让num+1 SET num = num + 1; -- 停止循环 UNTIL num &gt; 10\tEND REPEAT;\t-- 查询求和结果\tSELECT result;END$DELIMITER ;-- 调用pro_test9存储过程CALL pro_test9(); LOOP LOOP 实现简单的循环，退出循环的条件需要使用其他的语句定义，通常可以使用 LEAVE 语句实现，如果不加退出循环的语句，那么就变成了死循环。 loop 循环标准语法 123456[循环名称:] LOOP\t条件判断语句 [LEAVE 循环名称;]\t循环体语句;\t条件控制语句;END LOOP 循环名称; 计算 1~10 之间的和 123456789101112131415161718192021222324DELIMITER $CREATE PROCEDURE pro_test10()BEGIN\t-- 定义求和变量\tDECLARE result INT DEFAULT 0;\t-- 定义初始化变量\tDECLARE num INT DEFAULT 1;\t-- loop循环\tl:LOOP -- 条件成立，停止循环 IF num &gt; 10 THEN LEAVE l; END IF; -- 累加 SET result = result + num; -- 让num+1 SET num = num + 1;\tEND LOOP l;\t-- 查询求和结果\tSELECT result;END$DELIMITER ;-- 调用pro_test10存储过程CALL pro_test10(); CURSOR 游标 游标是用来存储查询结果集的数据类型，在存储过程和函数中可以使用光标对结果集进行循环的处理。 游标可以遍历返回的多行结果，每次拿到一整行数据 简单来说游标就类似于集合的迭代器遍历 MySQL 中的游标只能用在存储过程和函数中 游标的语法 创建游标 1DECLARE 游标名称 CURSOR FOR 查询sql语句; 打开游标 1OPEN 游标名称; 使用游标获取数据 1FETCH 游标名称 INTO 变量名1,变量名2,...; 关闭游标 1CLOSE 游标名称; Mysql 通过一个 Error handler 声明来判断指针是否到尾部，并且必须和创建游标的 SQL 语句声明在一起： 1DECLARE EXIT HANDLER FOR NOT FOUND (do some action，一般是设置标志变量) 游标的基本使用 数据准备：表 student 12345id\tNAME age gender\tscore1 张三 23 男 952 李四 24 男 983 王五 25 女 1004 赵六 26 女 90 创建 stu_score 表 1234CREATE TABLE stu_score( id INT PRIMARY KEY AUTO_INCREMENT, score INT); 将student表中所有的成绩保存到stu_score表中 12345678910111213141516171819202122232425262728293031323334DELIMITER $CREATE PROCEDURE pro_test12()BEGIN\t-- 定义成绩变量\tDECLARE s_score INT;\t-- 定义标记变量\tDECLARE flag INT DEFAULT 0; -- 创建游标，查询所有学生成绩数据\tDECLARE stu_result CURSOR FOR SELECT score FROM student;\t-- 游标结束后，将标记变量改为1 这两个必须声明在一起\tDECLARE EXIT HANDLER FOR NOT FOUND SET flag = 1; -- 开启游标\tOPEN stu_result;\t-- 循环使用游标\tREPEAT -- 使用游标，遍历结果,拿到数据 FETCH stu_result INTO s_score; -- 将数据保存到stu_score表中 INSERT INTO stu_score VALUES (NULL,s_score);\tUNTIL flag=1\tEND REPEAT;\t-- 关闭游标\tCLOSE stu_result;END$DELIMITER ;-- 调用pro_test12存储过程CALL pro_test12();-- 查询stu_score表SELECT * FROM stu_score; 存储函数 存储函数和存储过程是非常相似的，存储函数可以做的事情，存储过程也可以做到。 存储函数有返回值，存储过程没有返回值（参数的 out 其实也相当于是返回数据了）。 创建存储函数 12345678910DELIMITER $-- 标准语法CREATE FUNCTION 函数名称(参数 数据类型)RETURNS 返回值类型BEGIN\t执行的sql语句;\tRETURN 结果;END$DELIMITER ; 调用存储函数，因为有返回值，所以使用 1SELECT 调用SELECT 函数名称(实际参数); 删除存储函数 1DROP FUNCTION 函数名称; 定义存储函数，获取学生表中成绩大于95分的学生数量 1234567891011121314DELIMITER $CREATE FUNCTION fun_test()RETURN INTBEGIN\t-- 定义统计变量\tDECLARE result INT;\t-- 查询成绩大于95分的学生数量，给统计变量赋值\tSELECT COUNT(score) INTO result FROM student WHERE score &gt; 95;\t-- 返回统计结果\tSELECT result;ENDDELIMITER ;-- 调用fun_test存储函数SELECT fun_test(); 触发器 基本介绍 触发器是与表有关的数据库对象，在 insert/update/delete 之前或之后触发并执行触发器中定义的 SQL 语句。 触发器的这种特性可以协助应用在数据库端确保数据的完整性 、日志记录 、数据校验等操作 使用别名 NEW 和 OLD 来引用触发器中发生变化的记录内容，这与其他的数据库是相似的 现在触发器还只支持行级触发，不支持语句级触发 触发器类型 OLD的含义 NEW的含义 INSERT 型触发器 无 (因为插入前状态无数据) NEW 表示将要或者已经新增的数据 UPDATE 型触发器 OLD 表示修改之前的数据 NEW 表示将要或已经修改后的数据 DELETE 型触发器 OLD 表示将要或者已经删除的数据 无 (因为删除后状态无数据) 基本操作 创建触发器 1234567891011DELIMITER $CREATE TRIGGER 触发器名称 BEFORE|AFTER INSERT|UPDATE|DELETE ON 表名 [FOR EACH ROW] -- 行级触发器BEGIN\t触发器要执行的功能;END$DELIMITER ; 查看触发器的状态、语法等信息 1SHOW TRIGGERS; 删除触发器，如果没有指定 schema_name，默认为当前数据库 1DROP TRIGGER [schema_name.]trigger_name; 触发演示 通过触发器记录账户表的数据变更日志。包含：增加、修改、删除。 数据准备 1234-- 创建db9数据库CREATE DATABASE db9;-- 使用db9数据库USE db9; 12345678-- 创建账户表accountCREATE TABLE account( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 账户id NAME VARCHAR(20), -- 姓名 money DOUBLE -- 余额);-- 添加数据INSERT INTO account VALUES (NULL,&#x27;张三&#x27;,1000),(NULL,&#x27;李四&#x27;,2000); 12345678-- 创建日志表account_logCREATE TABLE account_log( id INT PRIMARY KEY AUTO_INCREMENT, -- 日志id operation VARCHAR(20), -- 操作类型 (insert update delete) operation_time DATETIME, -- 操作时间 operation_id INT, -- 操作表的id operation_params VARCHAR(200) -- 操作参数); 创建 INSERT 型触发器 1234567891011DELIMITER $CREATE TRIGGER account_insertAFTER INSERTON accountFOR EACH ROWBEGIN\tINSERT INTO account_log VALUES (NULL,&#x27;INSERT&#x27;,NOW(),new.id,CONCAT(&#x27;插入后&#123;id=&#x27;,new.id,&#x27;,name=&#x27;,new.name,&#x27;,money=&#x27;,new.money,&#x27;&#125;&#x27;));END$DELIMITER ; 123456789-- 向account表添加记录INSERT INTO account VALUES (NULL,&#x27;王五&#x27;,3000);-- 查询日志表SELECT * FROM account_log;/*id\toperation\toperation_time operation_id\toperation_params1\tINSERT 2021-01-26 19:51:11 3 插入后&#123;id=3,name=王五money=2000&#125;*/ 创建 UPDATE 型触发器 1234567891011DELIMITER $CREATE TRIGGER account_updateAFTER UPDATEON accountFOR EACH ROWBEGIN\tINSERT INTO account_log VALUES (NULL,&#x27;UPDATE&#x27;,NOW(),new.id,CONCAT(&#x27;修改前&#123;id=&#x27;,old.id,&#x27;,name=&#x27;,old.name,&#x27;,money=&#x27;,old.money,&#x27;&#125;&#x27;,&#x27;修改后&#123;id=&#x27;,new.id,&#x27;,name=&#x27;,new.name,&#x27;,money=&#x27;,new.money,&#x27;&#125;&#x27;));END$DELIMITER ; 12345678910-- 修改account表UPDATE account SET money=3500 WHERE id=3;-- 查询日志表SELECT * FROM account_log;/*id\toperation\toperation_time operation_id operation_params2\tUPDATE 2021-01-26 19:58:54 2 更新前&#123;id=2,name=李四money=1000&#125; 更新后&#123;id=2,name=李四money=200&#125;*/ 创建 DELETE 型触发器 1234567891011DELIMITER $CREATE TRIGGER account_deleteAFTER DELETEON accountFOR EACH ROWBEGIN\tINSERT INTO account_log VALUES (NULL,&#x27;DELETE&#x27;,NOW(),old.id,CONCAT(&#x27;删除前&#123;id=&#x27;,old.id,&#x27;,name=&#x27;,old.name,&#x27;,money=&#x27;,old.money,&#x27;&#125;&#x27;));END$DELIMITER ; 123456789-- 删除account表数据DELETE FROM account WHERE id=3;-- 查询日志表SELECT * FROM account_log;/*id\toperation\toperation_time operation_id\toperation_params3\tDELETE 2021-01-26 20:02:48 3 删除前&#123;id=3,name=王五money=2000&#125;*/","tags":["MySQL"],"categories":["学习笔记"]},{"title":"MySQL--索引","path":"/2024/01/22/MySQL--索引/","content":"索引概述 MySQL 官方对索引的定义为：索引（index）是帮助 MySQL 高效获取数据的一种数据结构，本质是排好序的快速查找数据结构。在表数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式指向数据， 这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。 索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。 索引使用：一张数据表，用于保存数据；一个索引配置文件，用于保存索引；每个索引都指向了某一个数据 。 左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快 Col2 的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据的物理地址的指针，这样就可以运用二叉查找快速获取到相应数据。 索引的优点： 类似于书籍的目录索引，提高数据检索的效率，降低数据库的 IO 成本 通过索引列对数据进行排序，降低数据排序的成本，降低 CPU 的消耗 索引的缺点： 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上 虽然索引大大提高了查询效率，同时却也降低更新表的速度。对表进行 INSERT、UPDATE、DELETE 操作，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，还会调整因为更新所带来的键值变化后的索引信息，但是更新数据也需要先从数据库中获取，索引加快了获取速度，所以可以相互抵消一下。 索引会影响到 WHERE 的查询条件和排序 ORDER BY 两大功能 索引结构 数据页 文件系统的最小单元是块（block），一个块的大小是 4K，系统从磁盘读取数据到内存时是以磁盘块为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来，而不是需要什么取什么。 InnoDB 存储引擎中有页（Page）的概念，页是 MySQL 磁盘管理的最小单位。 InnoDB 存储引擎中默认每个页的大小为 16KB，索引中一个节点就是一个数据页，所以会一次性读取 16KB 的数据到内存 InnoDB 引擎将若干个地址连接磁盘块，以此来达到页的大小 16KB 在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘 I/O 次数，提高查询效率 超过 16KB 的一条记录，主键索引页只会存储部分数据和指向溢出页的指针，剩余数据都会分散存储在溢出页中。 数据页物理结构，从上到下： File Header：上一页和下一页的指针、该页的类型（索引页、数据页、日志页等）、校验和、LSN（最近一次修改当前页面时的系统 lsn 值，事务持久性部分详解）等信息 Page Header：记录状态信息 Infimum + Supremum：当前页的最小记录和最大记录（头尾指针），Infimum 所在分组只有一条记录，Supremum 所在分组可以有 1 ~ 8 条记录，剩余的分组可以有 4 ~ 8 条记录 User Records：存储数据的记录 Free Space：尚未使用的存储空间 Page Directory：分组的目录，可以通过目录快速定位（二分法）数据的分组 File Trailer：检验和字段，在刷脏过程中，页首和页尾的校验和一致才能说明页面刷新成功，二者不同说明刷新期间发生了错误；LSN 字段，也是用来校验页面的完整性 数据页中包含数据行，数据的存储是基于数据行的，数据行有 next_record 属性指向下一个行数据，所以是可以遍历的，但是一组数据至多 8 个行，通过 Page Directory 先定位到组，然后遍历获取所需的数据行即可。 数据行中有三个隐藏字段：trx_id、roll_pointer、row_id（在事务章节会详细介绍它们的作用）。 B-Tree BTree 的索引类型是基于 B+Tree 树型数据结构的，B+Tree 又是 BTree 数据结构的变种，用在数据库和操作系统中的文件系统，特点是能够保持数据稳定有序。 BTree 又叫多路平衡搜索树，一颗 m 叉的 BTree 特性如下： 树中每个节点最多包含 m 个孩子 除根节点与叶子节点外，每个节点至少有 [ceil(m/2)] 个孩子 若根节点不是叶子节点，则至少有两个孩子 所有的叶子节点都在同一层 每个非叶子节点由 n 个 key 与 n+1 个指针组成，其中** [ceil(m/2)-1] &lt;= n &lt;= m-1** 5 叉树，key 的数量 [ceil(m/2)-1] &lt;= n &lt;= m-1 为 2 &lt;= n &lt;=4 ，当 n&gt;4 时中间节点分裂到父节点，两边节点分裂 插入 C N G A H E K Q M F W L T Z D P R X Y S 数据的工作流程： 插入前 4 个字母 C N G A 插入 H，n&gt;4，中间元素 G 字母向上分裂到新的节点 插入 E、K、Q 不需要分裂 插入 M，中间元素 M 字母向上分裂到父节点 G 插入 F，W，L，T 不需要分裂 插入 Z，中间元素 T 向上分裂到父节点中 插入 D，中间元素 D 向上分裂到父节点中，然后插入 P，R，X，Y 不需要分裂 最后插入 S，NPQR 节点 n&gt;5，中间节点 Q 向上分裂，但分裂后父节点 DGMT 的 n&gt;5，中间节点 M 向上分裂 BTree 树就已经构建完成了，BTree 树和二叉树相比， 查询数据的效率更高， 因为对于相同的数据量来说，BTree 的层级结构比二叉树少，所以搜索速度快。 BTree 结构的数据可以让系统高效的找到数据所在的磁盘块，定义一条记录为一个二元组 [key, data] ，key 为记录的键值，对应表中的主键值，data 为一行记录中除主键外的数据。对于不同的记录，key 值互不相同，BTree 中的每个节点根据实际情况可以包含大量的关键字信息和分支 。 缺点：当进行范围查找时会出现回旋查找。 B+Tree BTree 数据结构中每个节点中不仅包含数据的 key 值，还有 data 值。磁盘中每一页的存储空间是有限的，如果 data 数据较大时将会导致每个节点（即一个页）能存储的 key 的数量很小，当存储的数据量很大时同样会导致 B-Tree 的深度较大，增大查询时的磁盘 I/O 次数，进而影响查询效率，所以引入 B+Tree。 B+Tree 为 BTree 的变种，B+Tree 与 BTree 的区别为： n 叉 B+Tree 最多含有 n 个 key（哈希值），而 BTree 最多含有 n-1 个 key 所有非叶子节点只存储键值 key 信息，只进行数据索引，使每个非叶子节点所能保存的关键字大大增加 所有数据都存储在叶子节点，所以每次数据查询的次数都一样 叶子节点按照 key 大小顺序排列，左边结尾数据都会保存右边节点开始数据的指针，形成一个链表 所有节点中的 key 在叶子节点中也存在（比如 5)，key 允许重复，B 树不同节点不存在重复的 key 存储数据大小估算思路： B* 树：是 B+ 树的变体，在 B+ 树的非根和非叶子结点再增加指向兄弟的指针。 可以通过一个数据结构可视化的网站来简单演示一下：https://www.cs.usfca.edu/~galles/visualization MySQL 对 B+Tree 的优化 MySQL 索引数据结构对经典的 B+Tree 进行了优化，在原 B+Tree 的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的 B+Tree，提高区间访问的性能，防止回旋查找。 区间访问的意思是访问索引为 5 - 15 的数据，可以直接根据相邻节点的指针遍历。 B+ 树的叶子节点是数据页（page），一个页里面可以存多个数据行。 通常在 B+Tree 上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。可以对 B+Tree 进行两种查找运算： 有范围：对于主键的范围查找和分页查找 有顺序：从根节点开始，进行随机查找，顺序查找 InnoDB 中每个数据页的大小默认是 16KB， 索引行：一般表的主键类型为 INT（4 字节）或 BIGINT（8 字节），指针大小在 InnoDB 中设置为 6 字节，也就是说一个页大概存储 16KB/(8B+6B)=1K 个键值（估值）。则一个深度为 3 的 B+Tree 索引可以维护 10^3 * 10^3 * 10^3 = 10亿 条记录 数据行：一行数据的大小可能是 1k，一个数据页可以存储 16 行 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree 的高度一般都在 2-4 层。MySQL 的 InnoDB 存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要 1~3 次磁盘 I/O 操作。 B+Tree 优点：提高查询速度，减少磁盘的 IO 次数，树形结构较小。 索引维护（不完全理解*） B+ 树为了保持索引的有序性，在插入新值的时候需要做相应的维护。 每个索引中每个块存储在磁盘页中，可能会出现以下两种情况： 如果所在的数据页已经满了，这时候需要申请一个新的数据页，然后挪动部分数据过去，这个过程称为页分裂，原本放在一个页的数据现在分到两个页中，降低了空间利用率 当相邻两个页由于删除了数据，利用率很低之后，会将数据页做页合并，合并的过程可以认为是分裂过程的逆过程 这两个情况都是由 B+ 树的结构决定的（页分裂和页合并在 SQL 优化部分） 一般选用数据小的字段做索引，字段长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 自增主键的插入数据模式，可以让主键索引尽量地保持递增顺序插入，不涉及到挪动其他记录，避免了页分裂，页分裂的目的就是保证后一个数据页中的所有行主键值比前一个数据页中主键值大。 常考题 问：为什么InnoDB存储引擎选择使用B+tree索引结构? 答： 相对于二叉树，层级更少，搜索效率高； 对于B-tree，无论是叶子节点还是非叶子节点，都会保存数据，这样导致一页中存储 的键值减少，指针跟着减少，要同样保存大量数据，只能增加树的高度，导致性能降低； 相对Hash索引，B+tree支持范围匹配及排序操作。 索引分类 索引一般的分类如下： 功能分类 主键索引：一种特殊的唯一索引，不允许有空值，一般在建表时同时创建主键索引 单列索引：一个索引只包含单个列，一个表可以有多个单列索引（普通索引） 联合索引：顾名思义，就是将单列索引进行组合 唯一索引：索引列的值必须唯一，允许有空值，如果是联合索引，则列值组合必须唯一 NULL 值可以出现多次，因为两个 NULL 比较的结果既不相等，也不不等，结果仍然是未知 可以声明不允许存储 NULL 值的非空唯一索引 外键索引：只有 InnoDB 引擎支持外键索引，用来保证数据的一致性、完整性和实现级联操作 结构分类 BTree 索引：MySQL 使用最频繁的一个索引数据结构，是 InnoDB 和 MyISAM 存储引擎默认的索引类型，底层基于 B+Tree Hash 索引：MySQL中 Memory 存储引擎默认支持的索引类型 R-tree 索引（空间索引）：空间索引是 MyISAM 引擎的一个特殊索引类型，主要用于地理空间数据类型 Full-text 索引（全文索引）：快速匹配全部文档的方式。MyISAM 支持， InnoDB 不支持 FULLTEXT 类型的索引，但是 InnoDB 可以使用 sphinx 插件支持全文索引，MEMORY 引擎不支持 索引 InnoDB MyISAM Memory BTREE 支持 支持 支持 HASH 不支持 不支持 支持 R-tree 不支持 支持 不支持 Full-text 5.6 版本之后支持 支持 不支持 联合索引图示：根据身高年龄建立的组合索引（height、age） 索引语法 基本使用 索引在创建表的时候可以同时创建， 也可以随时增加新的索引。 创建索引：如果一个表中有一列是主键，那么会默认为其创建主键索引（主键列不需要单独创建索引） 12CREATE [UNIQUE|FULLTEXT] INDEX 索引名称 [USING 索引类型] ON 表名(列名...);-- 索引类型默认是 B+TREE 查看索引 1SHOW INDEX FROM 表名; 添加索引 1234567891011121314151617-- 单列索引ALTER TABLE 表名 ADD INDEX 索引名称(列名);-- 组合索引ALTER TABLE 表名 ADD INDEX 索引名称(列名1,列名2,...);-- 主键索引ALTER TABLE 表名 ADD PRIMARY KEY(主键列名); -- 外键索引(添加外键约束，就是外键索引)ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主键列名);-- 唯一索引ALTER TABLE 表名 ADD UNIQUE 索引名称(列名);-- 全文索引(mysql只支持文本类型)ALTER TABLE 表名 ADD FULLTEXT 索引名称(列名); 删除索引 1DROP INDEX 索引名称 ON 表名; 案例练习数据准备：student 12345id\tNAME age\tscore1\t张三 23 992\t李四 24 953\t王五 25 984\t赵六 26 97 索引操作： 12345-- 为student表中姓名列创建一个普通索引CREATE INDEX idx_name ON student(NAME);-- 为student表中年龄列创建一个唯一索引CREATE UNIQUE INDEX idx_age ON student(age); 查看索引（不理解） 12SHOW STATUS LIKE &#x27;Handler_read%&#x27;;\tSHOW GLOBAL STATUS LIKE &#x27;Handler_read%&#x27;; Handler_read_first：索引中第一条被读的次数，如果较高，表示服务器正执行大量全索引扫描（这个值越低越好） Handler_read_key：如果索引正在工作，这个值代表一个行被索引值读的次数，值越低表示索引不经常使用（这个值越高越好） Handler_read_next：按照键顺序读下一行的请求数，如果范围约束或执行索引扫描来查询索引列，值增加 Handler_read_prev：按照键顺序读前一行的请求数，该读方法主要用于优化 ORDER BY … DESC Handler_read_rnd：根据固定位置读一行的请求数，如果执行大量查询并对结果进行排序则该值较高，可能是使用了大量需要 MySQL 扫描整个表的查询或连接，这个值较高意味着运行效率低，应该建立索引来解决 Handler_read_rnd_next：在数据文件中读下一行的请求数，如果正进行大量的表扫描，该值较高，说明表索引不正确或写入的查询没有利用索引 聚簇 &amp; 非聚簇 不同引擎下的聚簇对比 聚簇索引是一种数据存储方式，并不是一种单独的索引类型 聚簇索引的叶子节点存放的是主键值和数据行，支持覆盖索引 非聚簇索引的叶子节点存放的是主键值或指向数据行的指针（由存储引擎决定） 在 Innodb 下主键索引是聚簇索引，在 MyISAM 下主键索引是非聚簇索引。 Innodb 概述 而在在InnoDB存储引擎中，根据索引的存储形式，又可以分为以下两种： 聚集索引选取规则: 如果存在主键，主键索引就是聚集索引 如果不存在主键，将使用第一个唯一（UNIQUE）索引作为聚集索引 如果表没有主键，或没有合适的唯一索引，则InnoDB会自动生成一个rowid作为隐藏的聚集索引 聚集索引的叶子节点下挂的是这一行的数据，二级索引的叶子节点下挂的是该字段值对应的主键值。 聚簇索引 在 Innodb 存储引擎，B+ 树索引可以分为聚簇索引（也称聚集索引、clustered index）和辅助索引（也称非聚簇索引或二级索引、secondary index、non-clustered index）。 InnoDB 中，聚簇索引是按照每张表的主键构造一颗 B+ 树，叶子节点中存放的就是整张表的数据，将聚簇索引的叶子节点称为数据页。 这个特性决定了数据也是索引的一部分，所以一张表只能有一个聚簇索引 辅助索引的存在不影响聚簇索引中数据的组织，所以一张表可以有多个辅助索引 聚簇索引的优点： 数据访问更快，聚簇索引将索引和数据保存在同一个 B+ 树中，因此从聚簇索引中获取数据比非聚簇索引更快 聚簇索引对于主键的排序查找和范围查找速度非常快 聚簇索引的缺点： 插入速度严重依赖于插入顺序，按照主键的顺序（递增）插入是最快的方式，否则将会出现页分裂，严重影响性能，所以对于 InnoDB 表，一般都会定义一个自增的 ID 列为主键 更新主键的代价很高，将会导致被更新的行移动，所以对于 InnoDB 表，一般定义主键为不可更新 二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据 非聚簇（二级）索引 在聚簇索引之上创建的索引称之为辅助索引，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引等。 辅助索引叶子节点存储的是主键值，而不是数据的物理地址，所以访问数据需要二次查找，推荐使用覆盖索引，可以减少回表查询。 检索过程：辅助索引找到主键值，再通过聚簇索引（二分）找到数据页，最后通过数据页中的 Page Directory（二分）找到对应的数据分组，遍历组内所所有的数据找到数据行。 补充：无索引走全表查询，查到数据页后和上述步骤一致。 索引实现 InnoDB 使用 B+Tree 作为索引结构，并且 InnoDB 一定有索引。 主键索引： 在 InnoDB 中，表数据文件本身就是按 B+Tree 组织的一个索引结构，这个索引的 key 是数据表的主键，叶子节点 data 域保存了完整的数据记录 InnoDB 的表数据文件通过主键聚集数据，如果没有定义主键，会选择非空唯一索引代替，如果也没有这样的列，MySQL 会自动为 InnoDB 表生成一个隐含字段 row_id 作为主键，这个字段长度为 6 个字节，类型为长整形 辅助索引： InnoDB 的所有辅助索引（二级索引）都引用主键作为 data 域 InnoDB 表是基于聚簇索引建立的，因此 InnoDB 的索引能提供一种非常快速的主键查找性能。不过辅助索引也会包含主键列，所以不建议使用过长的字段作为主键，过长的主索引会令辅助索引变得过大 MyISAM 非聚簇 MyISAM 的主键索引使用的是非聚簇索引，索引文件和数据文件是分离的，索引文件仅保存数据的地址。 主键索引 B+ 树的节点存储了主键，辅助键索引 B+ 树存储了辅助键，表数据存储在独立的地方，这两颗 B+ 树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别 由于索引树是独立的，通过辅助索引检索无需回表查询访问主键的索引树 索引实现 MyISAM 的索引方式也叫做非聚集的，之所以这么称呼是为了与 InnoDB 的聚集索引区分。 主键索引：MyISAM 引擎使用 B+Tree 作为索引结构，叶节点的 data 域存放的是数据记录的地址。 辅助索引：MyISAM 中主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求 key 是唯一的，而辅助索引的 key 可以重复。 索引使用 索引的失效情况 全值匹配：对索引中所有列都指定具体值，这种情况索引生效，执行效率高。 1EXPLAIN SELECT * FROM tb_seller WHERE name=&#x27;小米科技&#x27; AND status=&#x27;1&#x27; AND address=&#x27;西安市&#x27;; 失效情况及分析 违背最左前缀法则 如果索引了多列（联合索引），要遵守最左前缀法则。最左前缀法则指的是查询从索引的最左列开始， 并且不跳过索引中的列。如果跳跃某一列，索引将会部分失效(后面的字段索引失效)。 比如：在一个表中，有一个联合索引，这个联合索引涉及到三个字段，顺序分别为：profession， age，status。 对于最左前缀法则指的是，查询时，最左边的列，也就是 profession 必须存在，否则索引全部失效。 而且中间不能跳过某一列，否则该列后面的字段索引将失效。 注：顺序可以乱，但必须存在。 范围查询 联合索引中，出现范围查询(&gt;,&lt;)，范围查询右侧的列索引失效。但使用&gt;=或&lt;=索引可以生效。所以，在业务允许的情况下，尽可能的使用类似于 &gt;= 或 &lt;= 这类的范围查询，而避免使用 &gt; 或 &lt; 。 函数或运算操作 在索引列上函数或者运算（+ - 数值）操作， 索引将失效：会破坏索引值的有序性 字符串不加单引号 字符串不加单引号，造成索引失效：隐式类型转换，当字符串和数字比较时会把字符串转化为数字没有对字符串加单引号，查询优化器会调用 CAST 函数将 status 转换为 int 进行比较，造成索引失效 如果 status 是 int 类型，SQL 为 SELECT * FROM tb_seller WHERE status = '1' 并不会造成索引失效，因为会将 '1' 转换为 1，并不会对索引列产生操作 字符集不同 多表连接查询时，如果两张表的字符集不同，会造成索引失效，因为会进行类型转换。 解决方法：CONVERT 函数是加在输入参数上、修改表的字符集 用 OR 分割条件 用 OR 分割条件，索引失效，导致全表查询：OR 前的条件中的列有索引而后面的列中没有索引或 OR 前后两个列是同一个复合索引，都造成索引失效 AND 分割的条件不影响： 以 % 开头的 LIKE 模糊查询 以 % 开头的 LIKE 模糊查询，索引失效：如果是尾部模糊匹配，索引不会失效；如果是头部模糊匹配，索引失效。 解决方案：通过覆盖索引来解决 原因：在覆盖索引的这棵 B+ 数上只需要进行 like 的匹配，或者是基于覆盖索引查询再进行 WHERE 的判断就可以获得结果 数据分布影响 如果MySQL评估使用索引比全表更慢，则不使用索引。比如查找的数据量超过表的一半，MySQL 认为索引对查询的帮助不大，此时索引失效。 索引失效原理 索引失效一般是针对联合索引，联合索引一般由几个字段组成，排序方式是先按照第一个字段进行排序，然后排序第二个，依此类推，图示（a, b）索引，a 相等的情况下 b 是有序的。 最左前缀法则：当不匹配前面的字段的时候，后面的字段都是无序的。这种无序不仅体现在叶子节点，也会导致查询时扫描的非叶子节点也是无序的，因为索引树相当于忽略的第一个字段，就无法使用二分查找 范围查询右边的列，不能使用索引，比如语句： WHERE a &gt; 1 AND b = 1 ，在 a 大于 1 的时候，b 是无序的，a &gt; 1 是扫描时有序的，但是找到以后进行寻找 b 时，索引树就不是有序的了 以 % 开头的 LIKE 模糊查询，索引失效，比如语句：WHERE a LIKE ‘%d’，前面的不确定，导致不符合最左匹配，直接去索引中搜索以 d 结尾的节点，所以没有顺序 参考文章：https://mp.weixin.qq.com/s/B_M09dzLe9w7cT46rdGIeQ 索引优化 SQL 提示 SQL 提示，是优化数据库的一个重要手段，就是在 SQL 语句中加入一些提示来达到优化操作的目的。 USE INDEX：在查询语句中表名的后面添加 USE INDEX 来提供 MySQL 去参考的索引列表，可以让 MySQL 不再考虑其他可用的索引 IGNORE INDEX：让 MySQL 忽略一个或者多个索引，则可以使用 IGNORE INDEX 作为提示 FORCE INDEX：强制 MySQL 使用一个特定的索引 覆盖索引 覆盖索引：包含所有满足查询需要的数据的索引（SELECT 后面的字段刚好是索引字段），可以利用该索引返回 SELECT 列表的字段，而不必根据索引去聚簇索引上读取数据文件。 回表查询： 要查找的字段不在非主键索引树上时，需要通过叶子节点的主键值去主键索引上获取对应的行数据。 使用覆盖索引，防止回表查询： 表 user 主键为 id，普通索引为 age，查询语句： 1SELECT * FROM user WHERE age = 30; 查询过程：先通过普通索引 age=30 定位到主键值 id=1，再通过聚集索引 id=1 定位到行记录数据，需要两次扫描 B+ 树。 使用覆盖索引： 123DROP INDEX idx_age ON user;CREATE INDEX idx_age_name ON user(age,name);SELECT id,age FROM user WHERE age = 30; 在一棵索引树上就能获取查询所需的数据，无需回表速度更快。 使用覆盖索引，要注意 SELECT 列表中只取出需要的列，不可用 SELECT *，所有字段一起做索引会导致索引文件过大，查询性能下降。 索引下推 索引条件下推优化（Index Condition Pushdown，ICP）是 MySQL5.6 添加，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 索引下推充分利用了索引中的数据，在查询出整行数据之前过滤掉无效的数据，再去主键索引树上查找。 不使用索引下推优化时存储引擎通过索引检索到数据，然后回表查询记录返回给 Server 层，服务器判断数据是否符合条件 使用索引下推优化时，如果存在某些被索引的列的判断条件时，由存储引擎在索引遍历的过程中判断数据是否符合传递的条件，将符合条件的数据进行回表，检索出来返回给服务器，由此减少 IO 次数 适用条件： 需要存储引擎将索引中的数据与条件进行判断（所以条件列必须都在同一个索引中），所以优化是基于存储引擎的，只有特定引擎可以使用，适用于 InnoDB 和 MyISAM 存储引擎没有调用跨存储引擎的能力，跨存储引擎的功能有存储过程、触发器、视图，所以调用这些功能的不可以进行索引下推优化 对于 InnoDB 引擎只适用于二级索引，InnoDB 的聚簇索引会将整行数据读到缓冲区，不再需要去回表查询了 工作过程：用户表 user，(name, age) 是联合索引 1SELECT * FROM user WHERE name LIKE &#x27;张%&#x27; AND　age = 10;\t-- 头部模糊匹配会造成索引失效 优化前：在非主键索引树上找到满足第一个条件的行，然后通过叶子节点记录的主键值再回到主键索引树上查找到对应的行数据，再对比 AND 后的条件是否符合，符合返回数据，需要 4 次回表 优化后：检查索引中存储的列信息是否符合索引条件，然后交由存储引擎用剩余的判断条件判断此行数据是否符合要求，不满足条件的不去读取表中的数据，满足下推条件的就根据主键值进行回表查询，2 次回表 当使用 EXPLAIN 进行分析时，如果使用了索引条件下推，Extra 会显示 Using index condition。 参考文章：https://blog.csdn.net/sinat_29774479/article/details/103470244 参考文章：https://time.geekbang.org/column/article/69636 前缀索引 当要索引的列字符很多时，索引会变大变慢，可以只索引列开始的部分字符串，节约索引空间，提高索引效率。 注意：使用前缀索引就系统就忽略覆盖索引对查询性能的优化了。 优化原则：降低重复的索引值 比如地区表： 123456area gdp codechinaShanghai\t100 aaachinaDalian 200 bbbusaNewYork 300 cccchinaFuxin 400 dddchinaBeijing\t500 eee 发现 area 字段很多都是以 china 开头的，那么如果以前 1-5 位字符做前缀索引就会出现大量索引值重复的情况，索引值重复性越低，查询效率也就越高，所以需要建立前 6 位字符的索引： 1CREATE INDEX idx_area ON table_name(area(7)); 场景：存储身份证 直接创建完整索引，这样可能比较占用空间 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题（前 6 位相同的很多） 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描 索引合并（不理解） 使用多个索引来完成一次查询的执行方法叫做索引合并 index merge。 Intersection 索引合并 1SELECT * FROM table_test WHERE key1 = &#x27;a&#x27; AND key3 = &#x27;b&#x27;; # key1 和 key3 列都是单列索引、二级索引 从不同索引中扫描到的记录的 id 值取交集（相同 id），然后执行回表操作，要求从每个二级索引获取到的记录都是按照主键值排序 Union 索引合并 1SELECT * FROM table_test WHERE key1 = &#x27;a&#x27; OR key3 = &#x27;b&#x27;; 从不同索引中扫描到的记录的 id 值取并集，然后执行回表操作，要求从每个二级索引获取到的记录都是按照主键值排序 Sort-Union 索引合并 1SELECT * FROM table_test WHERE key1 &lt; &#x27;a&#x27; OR key3 &gt; &#x27;b&#x27;; 先将从不同索引中扫描到的记录的主键值进行排序，再按照 Union 索引合并的方式进行查询 索引合并算法的效率并不好，通过将其中的一个索引改成联合索引会优化效率。 索引设计原则 索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率。 创建索引时的原则： 对查询频次较高，且数据量比较大的表建立索引 使用唯一索引，区分度越高，使用索引的效率越高 尽量使用联合索引，避免单列索引；索引字段的选择，针对于常作为查询条件（where）、排序（order by）、分组（group by）操作的字段建立索引，使用覆盖索引 使用短索引，索引创建之后也是使用硬盘来存储的，因此提升索引访问的 I/O 效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升 MySQL 访问索引的 I/O 效率 索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价越高。对于插入、更新、删除等 DML 操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低 DML 操作的效率，增加相应操作的时间消耗；另外索引过多的话，MySQL 也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但提高了选择的代价 MySQL 建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配N 个列组合而成的组合索引，相当于创建了 N 个索引，如果查询时 where 句中使用了组成该索引的前几个字段，那么这条查询 SQL 可以利用组合索引来提升查询效率 如果索引列不能存储NULL值，请在创建表时使用NOT NULL约束它。当优化器知道每列是否包含 NULL值时，它可以更好地确定哪个索引最有效地用于查询 12345678910-- 对name、address、phone列建一个联合索引ALTER TABLE user ADD INDEX index_three(name,address,phone);-- 查询语句执行时会依照最左前缀匹配原则，检索时分别会使用索引进行数据匹配。(name,address,phone)(name,address)(name,phone)\t-- 只有name字段走了索引(name)-- 索引的字段可以是任意顺序的，优化器会帮助我们调整顺序，下面的SQL语句可以命中索引SELECT * FROM user WHERE address = &#x27;北京&#x27; AND phone = &#x27;12345&#x27; AND name = &#x27;张三&#x27;; 12-- 如果联合索引中最左边的列不包含在条件查询中，SQL语句就不会命中索引，比如：SELECT * FROM user WHERE address = &#x27;北京&#x27; AND phone = &#x27;12345&#x27;; 哪些情况不要建立索引： 记录太少的表 经常增删改的表 频繁更新的字段不适合创建索引 where 条件里用不到的字段不创建索引","tags":["MySQL"],"categories":["学习笔记"]},{"title":"Mybatis","path":"/2023/05/25/Mybatis/","content":"基本介绍 ORM（Object Relational Mapping）： 对象关系映射，指的是持久化数据和实体对象的映射模式，解决面向对象与关系型数据库存在的互不匹配的现象 MyBatis： MyBatis 是一个优秀的基于 Java 的持久层框架，它内部封装了 JDBC，使开发者只需关注 SQL 语句本身，而不需要花费精力去处理加载驱动、创建连接、创建 Statement 等过程。 MyBatis 通过 XML 或注解的方式将要执行的各种 Statement 配置起来，并通过 Java 对象和 Statement 中 SQL 的动态参数进行映射生成最终执行的 SQL 语句。 MyBatis 框架执行 SQL 并将结果映射为 Java 对象并返回。采用 ORM 思想解决了实体和数据库映射的问题，对 JDBC 进行了封装，屏蔽了 JDBC 底层 API 的调用细节，使我们不用操作 JDBC API，就可以完成对数据库的持久化操作。 MyBatis 官网地址：http://www.mybatis.org/mybatis-3/ 参考视频：https://space.bilibili.com/37974444/ 基本操作 相关API Resources：加载资源的工具类 InputStream getResourceAsStream(String fileName)：通过类加载器返回指定资源的字节流 参数 fileName 是放在 src 的核心配置文件名：MyBatisConfig.xml SqlSessionFactoryBuilder：构建器，用来获取 SqlSessionFactory 工厂对象 SqlSessionFactory build(InputStream is)：通过指定资源的字节输入流获取 SqlSession 工厂对象 SqlSessionFactory：获取 SqlSession 构建者对象的工厂接口 SqlSession openSession()：获取 SqlSession 构建者对象，并开启手动提交事务 SqlSession openSession(boolean)：获取 SqlSession 构建者对象，参数为 true 开启自动提交事务 SqlSession：构建者对象接口，用于执行 SQL、管理事务、接口代理 SqlSession 代表和数据库的一次会话，用完必须关闭 SqlSession 和 Connection 一样都是非线程安全，每次使用都应该去获取新的对象 注：update 数据需要提交事务，或开启默认提交 SqlSession 常用 API： 方法 说明 List selectList(String statement,Object parameter) 执行查询语句，返回List集合 T selectOne(String statement,Object parameter) 执行查询语句，返回一个结果对象 int insert(String statement,Object parameter) 执行新增语句，返回影响行数 int update(String statement,Object parameter) 执行删除语句，返回影响行数 int delete(String statement,Object parameter) 执行修改语句，返回影响行数 void commit() 提交事务 void rollback() 回滚事务 T getMapper(Class cls) 获取指定接口的代理实现类对象 void close() 释放资源 映射配置 映射配置文件包含了数据和对象之间的映射关系以及要执行的 SQL 语句，放在 src 目录下。 命名：StudentMapper.xml 映射配置文件的文件头： 1234&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; 根标签： ：核心根标签 namespace：属性，名称空间 功能标签： &lt; select &gt;：查询功能标签 ：新增功能标签 ：修改功能标签 ：删除功能标签 id：属性，唯一标识，配合名称空间使用 resultType：指定结果映射对象类型，和对应的方法的返回值类型（全限定名）保持一致，但是如果返回值是 List 则和其泛型保持一致 parameterType：指定参数映射对象类型，必须和对应的方法的参数类型（全限定名）保持一致 statementType：可选 STATEMENT，PREPARED 或 CALLABLE，默认值：PREPARED STATEMENT：直接操作 SQL，使用 Statement 不进行预编译，获取数据：$ PREPARED：预处理参数，使用 PreparedStatement 进行预编译，获取数据：# CALLABLE：执行存储过程，CallableStatement 参数获取方式： SQL 获取参数：#{属性名} 12345&lt;mapper namespace=&quot;StudentMapper&quot;&gt; &lt;select id=&quot;selectById&quot; resultType=&quot;student&quot; parameterType=&quot;int&quot;&gt; SELECT * FROM student WHERE id = #&#123;id&#125; &lt;/select&gt; &lt;mapper/&gt; 推荐官方文档：https://mybatis.org/mybatis-3/zh/sqlmap-xml.html 核心配置 核心配置文件包含了 MyBatis 最核心的设置和属性信息，如数据库的连接、事务、连接池信息等。 命名：MyBatisConfig.xml 核心配置文件的文件头： 12&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt; 根标签： configuration：核心根标签 引入连接配置文件： properties： 引入数据库连接配置文件标签 resource：属性，指定配置文件名 1&lt;properties resource=&quot;jdbc.properties&quot;/&gt; 调整设置 &lt;settings：可以改变 Mybatis 运行时行为，比如开启缓存、懒加载等 起别名： typeAliases：为全类名起别名的父标签 typeAlias：为全类名起别名的子标签 type：指定全类名 alias：指定别名 package：为指定包下所有类起别名的子标签，别名就是类名，首字母小写 自带别名： 别名 数据类型 string java.lang.String long java.lang.Lang int java.lang.Integer double java.lang.Double boolean java.lang.Boolean … … 123456&lt;!--起别名--&gt;&lt;typeAliases&gt; &lt;typeAlias type=&quot;bean.Student&quot; alias=&quot;student&quot;/&gt; &lt;package name=&quot;com.seazean.bean&quot;/&gt; &lt;!--二选一--&gt;&lt;/typeAliases&gt; 配置环境，可以配置多个标签 environments ：配置数据库环境标签，default 属性指定哪个 environment environment：配置数据库环境子标签，id 属性是唯一标识，与 default 对应 transactionManager：事务管理标签，type 属性默认 JDBC 事务 dataSource：数据源标签 type 属性：POOLED 使用连接池（MyBatis 内置），UNPOOLED 不使用连接池 property：数据库连接信息标签。 name 属性取值：driver，url，username，password value 属性取值：与 name 对应 1234567891011&lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mydatabase&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;password&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; 引入映射配置文件 mappers：引入映射配置文件标签 mapper：引入映射配置文件子标签 resource：属性指定映射配置文件的名称 url：引用网路路径或者磁盘路径下的 sql 映射文件 class：指定映射配置类 package：批量注册 123456789&lt;mappers&gt; &lt;mapper resource=&quot;com/example/mapper/UserMapper.xml&quot;/&gt; &lt;mapper resource=&quot;com/example/mapper/OrderMapper.xml&quot;/&gt;&lt;/mappers&gt;&lt;!-- 批量注册 --&gt;&lt;mappers&gt; &lt;package name=&quot;com.example.mapper&quot;/&gt;&lt;/mappers&gt; 官方文档：https://mybatis.org/mybatis-3/zh/configuration.html #{}和${} #{}：占位符，传入的内容会作为字符串加上引号，以预编译的方式传入，将 sql 中的 #{} 替换为 ? 号，调用 PreparedStatement 的 set 方法来赋值，有效的防止 SQL 注入，提高系统安全性。 ${}：拼接符，传入的内容会直接替换拼接，不会加上引号，可能存在 sql 注入的安全隐患。 能用 #{} 的地方就用 #{}，不用或少用 ${} 必须使用 ${} 的情况： 表名作参数时，如：SELECT * FROM $&#123;tableName&#125; order by 时，如：SELECT * FROM t_user ORDER BY $&#123;columnName&#125; sql 语句使用 #{}，properties 文件内容获取使用 ${} 日志文件 在日常开发过程中，排查问题时需要输出 MyBatis 真正执行的 SQL 语句、参数、结果等信息，就可以借助 log4j 的功能来实现执行信息的输出。 pom.xml 12345678910&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt; 在核心配置文件根标签内配置 log4j 1234&lt;!--配置LOG4J--&gt;&lt;settings&gt; &lt;setting name=&quot;logImpl&quot; value=&quot;log4j&quot;/&gt;&lt;/settings&gt; 在 src 目录下创建 log4j.properties 123456789101112# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n#输出到日志文件#log4j.appender.file=org.apache.log4j.FileAppender#log4j.appender.file.File=../logs/iask.log#log4j.appender.file.layout=org.apache.log4j.PatternLayout#log4j.appender.file.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; %l %m%n 代码演示 实体类 123456public class Student &#123; private Integer id; private String name; private Integer age; .....&#125; StudentMapper 12345678910111213141516public interface StudentMapper &#123; //查询全部 public abstract List&lt;Student&gt; selectAll(); //根据id查询 public abstract Student selectById(Integer id); //新增数据 public abstract Integer insert(Student stu); //修改数据 public abstract Integer update(Student stu); //删除数据 public abstract Integer delete(Integer id);&#125; config.properties 1234driver=com.mysql.jdbc.Driverurl=jdbc:mysql://192.168.2.184:3306/db1username=rootpassword=123456 MyBatisConfig.xml 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;!--核心根标签--&gt;&lt;configuration&gt; &lt;!--引入数据库连接的配置文件--&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;!--配置LOG4J--&gt; &lt;settings&gt; &lt;setting name=&quot;logImpl&quot; value=&quot;log4j&quot;/&gt; &lt;/settings&gt; &lt;!--起别名--&gt; &lt;typeAliases&gt; &lt;typeAlias type=&quot;bean.Student&quot; alias=&quot;student&quot;/&gt; &lt;!--&lt;package name=&quot;bean&quot;/&gt;--&gt; &lt;/typeAliases&gt; &lt;!--配置数据库环境，可以多个环境，default指定哪个--&gt; &lt;environments default=&quot;mysql&quot;&gt; &lt;!--id属性唯一标识--&gt; &lt;environment id=&quot;mysql&quot;&gt; &lt;!--事务管理，type属性，默认JDBC事务--&gt; &lt;transactionManager type=&quot;JDBC&quot;&gt;&lt;/transactionManager&gt; &lt;!--数据源信息 type属性连接池--&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;!--property获取数据库连接的配置信息--&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!--引入映射配置文件--&gt; &lt;mappers&gt; &lt;!--mapper引入指定的映射配置 resource属性执行的映射配置文件的名称--&gt; &lt;mapper resource=&quot;StudentMapper.xml&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; StudentMapper.xml 123456789101112131415161718192021222324252627&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapperPUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;StudentMapper&quot;&gt; &lt;select id=&quot;selectAll&quot; resultType=&quot;student&quot;&gt; SELECT * FROM student &lt;/select&gt; &lt;select id=&quot;selectById&quot; resultType=&quot;student&quot; parameterType=&quot;int&quot;&gt; SELECT * FROM student WHERE id = #&#123;id&#125; &lt;/select&gt; &lt;insert id=&quot;insert&quot; parameterType=&quot;student&quot;&gt; INSERT INTO student VALUES (#&#123;id&#125;,#&#123;name&#125;,#&#123;age&#125;) &lt;/insert&gt; &lt;update id=&quot;update&quot; parameterType=&quot;student&quot;&gt; UPDATE student SET name = #&#123;name&#125;, age = #&#123;age&#125; WHERE id = #&#123;id&#125; &lt;/update&gt; &lt;delete id=&quot;delete&quot; parameterType=&quot;student&quot;&gt; DELETE FROM student WHERE id = #&#123;id&#125; &lt;/delete&gt;&lt;/mapper&gt; 控制层测试代码：根据 id 查询 123456789101112131415161718192021@Testpublic void selectById() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = ssf.openSession(); //4.执行映射配置文件中的sql语句，并接收结果 Student stu = sqlSession.selectOne(&quot;StudentMapper.selectById&quot;, 3); //5.处理结果 System.out.println(stu); //6.释放资源 sqlSession.close(); is.close();&#125; 控制层测试代码：新增功能 123456789101112131415161718192021@Testpublic void insert() throws Exception&#123; //1.加载核心配置文件 //2.获取SqlSession工厂对象 //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = sqlSessionFactory.openSession(true); //4.执行映射配置文件中的sql语句，并接收结果 Student stu = new Student(5, &quot;周七&quot;, 27); int result = sqlSession.insert(&quot;StudentMapper.insert&quot;, stu); //5.提交事务 //sqlSession.commit(); //6.处理结果 System.out.println(result); //7.释放资源 sqlSession.close(); is.close();&#125; 批量操作 三种方式实现批量操作： 标签属性：这种方式属于全局批量 123&lt;settings&gt; &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;BATCH&quot;/&gt;&lt;/settings&gt; defaultExecutorType：配置默认的执行器 SIMPLE 就是普通的执行器（默认，每次执行都要重新设置参数） REUSE 执行器会重用预处理语句（只预设置一次参数，多次执行） BATCH 执行器不仅重用语句还会执行批量更新（只针对修改操作） SqlSession 会话内批量操作： 1234567891011121314151617181920public void testBatch() throws IOException&#123; SqlSessionFactory sqlSessionFactory = getSqlSessionFactory(); // 可以执行批量操作的sqlSession SqlSession openSession = sqlSessionFactory.openSession(ExecutorType.BATCH); long start = System.currentTimeMillis(); try&#123; EmployeeMapper mapper = openSession.getMapper(EmployeeMapper.class); for (int i = 0; i &lt; 10000; i++) &#123; mapper.addEmp(new Employee(UUID.randomUUID().toString().substring(0, 5), &quot;b&quot;, &quot;1&quot;)); &#125; openSession.commit(); long end = System.currentTimeMillis(); // 批量：（预编译sql一次==&gt;设置参数===&gt;10000次===&gt;执行1次（类似管道）） // 非批量：（预编译sql=设置参数=执行）==&gt;10000 耗时更多 System.out.println(&quot;执行时长：&quot; + (end - start)); &#125;finally&#123; openSession.close(); &#125;&#125; Spring 配置文件方式（applicationContext.xml）： 12345&lt;!--配置一个可以进行批量执行的sqlSession --&gt;&lt;bean id=&quot;sqlSession&quot; class=&quot;org.mybatis.spring.SqlSessionTemplate&quot;&gt; &lt;constructor-arg name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactoryBean&quot;/&gt; &lt;constructor-arg name=&quot;executorType&quot; value=&quot;BATCH&quot;/&gt;&lt;/bean&gt; 12@Autowiredprivate SqlSession sqlSession; 代理开发 代理规则 分层思想：控制层（controller）、业务层（service）、持久层（dao）。 调用流程： 传统方式实现 DAO 层，需要写接口和实现类。采用 Mybatis 的代理开发方式实现 DAO 层的开发，只需要编写 Mapper 接口（相当于 Dao 接口），由 Mybatis 框架根据接口定义创建接口的动态代理对象。 接口开发方式： 定义接口 操作数据库，MyBatis 框架根据接口，通过动态代理的方式生成代理对象，负责数据库的操作 Mapper 接口开发需要遵循以下规范： Mapper.xml 文件中的 namespace 与 DAO 层 mapper 接口的全类名相同 Mapper.xml 文件中的增删改查标签的id属性和 DAO 层 Mapper 接口方法名相同 Mapper.xml 文件中的增删改查标签的 parameterType 属性和 DAO 层 Mapper 接口方法的参数相同 Mapper.xml 文件中的增删改查标签的 resultType 属性和 DAO 层 Mapper 接口方法的返回值相同 实现原理 通过动态代理开发模式，只编写一个接口不写实现类，通过 getMapper() 方法最终获取到 MapperProxy 代理对象，而这个代理对象是 MyBatis 使用了 JDK 的动态代理技术生成的。 动态代理实现类对象在执行方法时最终调用了 MapperMethod.execute() 方法，这个方法中通过 switch case 语句根据操作类型来判断是新增、修改、删除、查询操作，最后一步回到了 MyBatis 最原生的 SqlSession 方式来执行增删改查。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637public Student selectById(Integer id) &#123; Student stu = null; SqlSession sqlSession = null; InputStream is = null; try&#123; //1.加载核心配置文件 is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory s = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 sqlSession = s.openSession(true); //4.获取StudentMapper接口的实现类对象 StudentMapper mapper = sqlSession.getMapper(StudentMapper.class); //5.通过实现类对象调用方法，接收结果 stu = mapper.selectById(id); &#125; catch (Exception e) &#123; e.getMessage(); &#125; finally &#123; //6.释放资源 if(sqlSession != null) &#123; sqlSession.close(); &#125; if(is != null) &#123; try &#123; is.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; //7.返回结果 return stu;&#125; 结果映射 相关标签 resultMap：返回结果映射对象类型，和对应方法的返回值类型保持一致，但是如果返回值是 List 则和其泛型保持一致 result：返回一条记录的 Map，key 是列名，value 是对应的值，用来配置字段和对象属性的映射关系标签，结果映射（和 resultType 二选一） id 属性：唯一标识 type 属性：实体对象类型 autoMapping 属性：结果自动映射 核心配置文件标签： id：配置主键映射关系标签 result：配置非主键映射关系标签 column 属性：表中字段名称 property 属性： 实体对象变量名称 association：配置被包含单个对象的映射关系标签，嵌套封装结果集（多对一、一对一） property 属性：被包含对象的变量名，要进行映射的属性名 javaType 属性：被包含对象的数据类型，要进行映射的属性的类型（Java 中的 Bean 类） select 属性：加载复杂类型属性的映射语句的 ID，会从 column 属性指定的列中检索数据，作为参数传递给目标 select 语句 collection：配置被包含集合对象的映射关系标签，嵌套封装结果集（一对多、多对多） property 属性：被包含集合对象的变量名 ofType 属性：集合中保存的对象数据类型 discriminator：鉴别器，用来判断某列的值，根据得到某列的不同值做出不同自定义的封装行为 自定义封装规则可以将数据库中比较复杂的数据类型映射为 JavaBean 中的属性。 嵌套查询 子查询： 123456public class Blog &#123; private int id; private String msg; private Author author; // set + get&#125; 1234567891011&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot; autoMapping = &quot;true&quot;&gt; &lt;association property=&quot;author&quot; column=&quot;author_id&quot; javaType=&quot;Author&quot; select=&quot;selectAuthor&quot;/&gt;&lt;/resultMap&gt;&lt;select id=&quot;selectBlog&quot; resultMap=&quot;blogResult&quot;&gt; SELECT * FROM BLOG WHERE ID = #&#123;id&#125;&lt;/select&gt;&lt;select id=&quot;selectAuthor&quot; resultType=&quot;Author&quot;&gt; SELECT * FROM AUTHOR WHERE ID = #&#123;id&#125;&lt;/select&gt; 循环引用：通过缓存解决 123456&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot; autoMapping = &quot;true&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;collection property=&quot;comment&quot; ofType=&quot;Comment&quot;&gt; &lt;association property=&quot;blog&quot; javaType=&quot;Blog&quot; resultMap=&quot;blogResult&quot;/&gt;&lt;!--y--&gt; &lt;/collection&gt;&lt;/resultMap 多表查询 一对一 一对一实现： 数据准备 1234567891011121314CREATE TABLE person( id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(20), age INT);INSERT INTO person VALUES (NULL,&#x27;张三&#x27;,23),(NULL,&#x27;李四&#x27;,24),(NULL,&#x27;王五&#x27;,25);CREATE TABLE card( id INT PRIMARY KEY AUTO_INCREMENT, number VARCHAR(30), pid INT, CONSTRAINT cp_fk FOREIGN KEY (pid) REFERENCES person(id));INSERT INTO card VALUES (NULL,&#x27;12345&#x27;,1),(NULL,&#x27;23456&#x27;,2),(NULL,&#x27;34567&#x27;,3); bean 类 123456789101112public class Card &#123; private Integer id; //主键id private String number; //身份证号 private Person p; //所属人的对象 ......&#125;public class Person &#123; private Integer id; //主键id private String name; //人的姓名 private Integer age; //人的年龄&#125; 配置文件 OneToOneMapper.xml，MyBatisConfig.xml 需要引入（可以把 bean 包下起别名） 12345678910111213141516171819202122232425262728&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapperPUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;OneToOneMapper&quot;&gt; &lt;!--配置字段和实体对象属性的映射关系--&gt; &lt;resultMap id=&quot;oneToOne&quot; type=&quot;card&quot;&gt; &lt;!--column 表中字段名称，property 实体对象变量名称--&gt; &lt;id column=&quot;cid&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;number&quot; property=&quot;number&quot; /&gt; &lt;!-- association：配置被包含对象的映射关系 property：被包含对象的变量名 javaType：被包含对象的数据类型 --&gt; &lt;association property=&quot;p&quot; javaType=&quot;bean.Person&quot;&gt; &lt;id column=&quot;pid&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot; /&gt; &lt;result column=&quot;age&quot; property=&quot;age&quot; /&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;oneToOne&quot;&gt; &lt;!--SQL--&gt; SELECT c.id cid,number,pid,NAME,age FROM card c,person p WHERE c.pid=p.id &lt;/select&gt;&lt;/mapper&gt; 核心配置文件 MyBatisConfig.xml 123456&lt;!-- mappers引入映射配置文件 --&gt;&lt;mappers&gt; &lt;mapper resource=&quot;one_to_one/OneToOneMapper.xml&quot;/&gt; &lt;mapper resource=&quot;one_to_many/OneToManyMapper.xml&quot;/&gt; &lt;mapper resource=&quot;many_to_many/ManyToManyMapper.xml&quot;/&gt;&lt;/mappers&gt; 测试类 12345678910111213141516171819202122232425262728public class Test01 &#123; @Test public void selectAll() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = ssf.openSession(true); //4.获取OneToOneMapper接口的实现类对象 OneToOneMapper mapper = sqlSession.getMapper(OneToOneMapper.class); //5.调用实现类的方法，接收结果 List&lt;Card&gt; list = mapper.selectAll(); //6.处理结果 for (Card c : list) &#123; System.out.println(c); &#125; //7.释放资源 sqlSession.close(); is.close(); &#125;&#125; 一对多 一对多实现： 数据准备 1234567891011121314CREATE TABLE classes( id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(20));INSERT INTO classes VALUES (NULL,&#x27;程序一班&#x27;),(NULL,&#x27;程序二班&#x27;)CREATE TABLE student( id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(30), age INT, cid INT, CONSTRAINT cs_fk FOREIGN KEY (cid) REFERENCES classes(id));INSERT INTO student VALUES (NULL,&#x27;张三&#x27;,23,1),(NULL,&#x27;李四&#x27;,24,1),(NULL,&#x27;王五&#x27;,25,2); bean 类 1234567891011public class Classes &#123; private Integer id; //主键id private String name; //班级名称 private List&lt;Student&gt; students; //班级中所有学生对象 ........&#125;public class Student &#123; private Integer id; //主键id private String name; //学生姓名 private Integer age; //学生年龄&#125; 映射配置文件 12345678910111213141516&lt;mapper namespace=&quot;OneToManyMapper&quot;&gt; &lt;resultMap id=&quot;oneToMany&quot; type=&quot;bean.Classes&quot;&gt; &lt;id column=&quot;cid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;cname&quot; property=&quot;name&quot;/&gt; &lt;!--collection：配置被包含的集合对象映射关系--&gt; &lt;collection property=&quot;students&quot; ofType=&quot;bean.Student&quot;&gt; &lt;id column=&quot;sid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;sname&quot; property=&quot;name&quot;/&gt; &lt;result column=&quot;sage&quot; property=&quot;age&quot;/&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;oneToMany&quot;&gt; &lt;!--SQL--&gt; SELECT c.id cid,c.name cname,s.id sid,s.name sname,s.age sage FROM classes c,student s WHERE c.id=s.cid &lt;/select&gt;&lt;/mapper&gt; 代码实现片段 1234567891011121314//4.获取OneToManyMapper接口的实现类对象OneToManyMapper mapper = sqlSession.getMapper(OneToManyMapper.class);//5.调用实现类的方法，接收结果List&lt;Classes&gt; classes = mapper.selectAll();//6.处理结果for (Classes cls : classes) &#123; System.out.println(cls.getId() + &quot;,&quot; + cls.getName()); List&lt;Student&gt; students = cls.getStudents(); for (Student student : students) &#123; System.out.println(&quot;\\t&quot; + student); &#125;&#125; 多对多 学生课程例子，中间表不需要 bean 实体类 数据准备 1234567891011121314CREATE TABLE course( id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(20));INSERT INTO course VALUES (NULL,&#x27;语文&#x27;),(NULL,&#x27;数学&#x27;);CREATE TABLE stu_cr( id INT PRIMARY KEY AUTO_INCREMENT, sid INT, cid INT, CONSTRAINT sc_fk1 FOREIGN KEY (sid) REFERENCES student(id), CONSTRAINT sc_fk2 FOREIGN KEY (cid) REFERENCES course(id));INSERT INTO stu_cr VALUES (NULL,1,1),(NULL,1,2),(NULL,2,1),(NULL,2,2); bean类 12345678910public class Student &#123; private Integer id; //主键id private String name; //学生姓名 private Integer age; //学生年龄 private List&lt;Course&gt; courses; // 学生所选择的课程集合&#125;public class Course &#123; private Integer id; //主键id private String name; //课程名称&#125; 配置文件 123456789101112131415&lt;mapper namespace=&quot;ManyToManyMapper&quot;&gt; &lt;resultMap id=&quot;manyToMany&quot; type=&quot;Bean.Student&quot;&gt; &lt;id column=&quot;sid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;sname&quot; property=&quot;name&quot;/&gt; &lt;result column=&quot;sage&quot; property=&quot;age&quot;/&gt; &lt;collection property=&quot;courses&quot; ofType=&quot;Bean.Course&quot;&gt; &lt;id column=&quot;cid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;cname&quot; property=&quot;name&quot;/&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;manyToMany&quot;&gt; &lt;!--SQL--&gt; SELECT sc.sid,s.name sname,s.age sage,sc.cid,c.name cname FROM student s,course c,stu_cr sc WHERE sc.sid=s.id AND sc.cid=c.id &lt;/select&gt;&lt;/mapper&gt; 鉴别器 需求：如果查询结果是女性，则把部门信息查询出来，否则不查询；如果是男性，把 last_name 这一列的值赋值。 1234567891011121314151617181920&lt;!-- column：指定要判断的列名 javaType：列值对应的java类型--&gt;&lt;discriminator javaType=&quot;string&quot; column=&quot;gender&quot;&gt; &lt;!-- 女生 --&gt; &lt;!-- resultType不可缺少，也可以使用resutlMap --&gt; &lt;case value=&quot;0&quot; resultType=&quot;com.bean.Employee&quot;&gt; &lt;association property=&quot;dept&quot; select=&quot;com.dao.DepartmentMapper.getDeptById&quot; column=&quot;d_id&quot;&gt; &lt;/association&gt; &lt;/case&gt; &lt;!-- 男生 --&gt; &lt;case value=&quot;1&quot; resultType=&quot;com.bean.Employee&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;last_name&quot; property=&quot;lastName&quot;/&gt; &lt;result column=&quot;gender&quot; property=&quot;gender&quot;/&gt; &lt;/case&gt;&lt;/discriminator&gt; 加载方式 两种加载 立即加载：只要调用方法，马上发起查询。 延迟加载：在需要用到数据时才进行加载，不需要用到数据时就不加载数据，延迟加载也称懒加载。 优点： 先从单表查询，需要时再从关联表去关联查询，提高数据库性能，因为查询单表要比关联查询多张表速度要快，节省资源 缺点：只有当需要用到数据时，才会进行数据库查询，这样在大批量数据查询时，查询工作也要消耗时间，所以可能造成用户等待时间变长，造成用户体验下降 核心配置文件： 标签名 描述 默认值 lazyLoadingEnabled 延迟加载的全局开关。当开启时，所有关联对象都会延迟加载，特定关联关系中可通过设置 fetchType 属性来覆盖该项的开关状态。 false aggressiveLazyLoading 开启时，任一方法的调用都会加载该对象的所有延迟加载属性。否则每个延迟加载属性会按需加载（参考 lazyLoadTriggerMethods） false 1234&lt;settings&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;false&quot;/&gt; &lt;/settings&gt; assocation 分布查询：先按照身份 id 查询所属人的 id、然后根据所属人的 id 去查询人的全部信息，这就是分步查询。 映射配置文件 OneToOneMapper.xml一对一映射： column 属性表示给要调用的其它的 select 标签传入的参数 select 属性表示调用其它的 select 标签 fetchType=“lazy” 表示延迟加载（局部配置，只有配置了这个的地方才会延迟加载） 1234567891011121314151617&lt;mapper namespace=&quot;OneToOneMapper&quot;&gt; &lt;!--配置字段和实体对象属性的映射关系--&gt; &lt;resultMap id=&quot;oneToOne&quot; type=&quot;card&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;number&quot; property=&quot;number&quot; /&gt; &lt;association property=&quot;p&quot; javaType=&quot;bean.Person&quot; column=&quot;pid&quot; select=&quot;one_to_one.PersonMapper.findPersonByid&quot; fetchType=&quot;lazy&quot;&gt; &lt;!--需要配置新的映射文件--&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;oneToOne&quot;&gt; SELECT * FROM card &lt;!--查询全部，负责根据条件直接全部加载--&gt; &lt;/select&gt;&lt;/mapper&gt; PersonMapper.xml 12345&lt;mapper namespace=&quot;one_to_one.PersonMapper&quot;&gt; &lt;select id=&quot;findPersonByid&quot; parameterType=&quot;int&quot; resultType=&quot;person&quot;&gt; SELECT * FROM person WHERE id=#&#123;pid&#125; &lt;/select&gt;&lt;/mapper&gt; PersonMapper.java 123public interface PersonMapper &#123; User findPersonByid(int id);&#125; 测试文件 1234567891011121314151617public class Test01 &#123; @Test public void selectAll() throws Exception&#123; InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); SqlSession sqlSession = ssf.openSession(true); OneToOneMapper mapper = sqlSession.getMapper(OneToOneMapper.class); // 调用实现类的方法，接收结果 List&lt;Card&gt; list = mapper.selectAll(); // 不能遍历，遍历就是相当于使用了该数据，需要加载，不遍历就是没有使用。 // 释放资源 sqlSession.close(); is.close(); &#125;&#125; collection 同样在一对多关系配置的 结点中配置延迟加载策略， 结点中也有 select 属性和 column 属性。 映射配置文件 OneToManyMapper.xml一对多映射： column 是用于指定使用哪个字段的值作为条件查询 select 是用于指定查询账户的唯一标识（账户的 dao 全限定类名加上方法名称） 123456789101112131415&lt;mapper namespace=&quot;OneToManyMapper&quot;&gt; &lt;resultMap id=&quot;oneToMany&quot; type=&quot;bean.Classes&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt; &lt;!--collection：配置被包含的集合对象映射关系--&gt; &lt;collection property=&quot;students&quot; ofType=&quot;bean.Student&quot; column=&quot;id&quot; select=&quot;one_to_one.StudentMapper.findStudentByCid&quot;&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;oneToMany&quot;&gt; SELECT * FROM classes &lt;/select&gt;&lt;/mapper&gt; StudentMapper.xml 12345&lt;mapper namespace=&quot;one_to_one.StudentMapper&quot;&gt; &lt;select id=&quot;findPersonByCid&quot; parameterType=&quot;int&quot; resultType=&quot;student&quot;&gt; SELECT * FROM person WHERE cid=#&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 注解开发 单表操作 注解可以简化开发操作，省略映射配置文件的编写。 常用注解： @Select(“查询的 SQL 语句”)：执行查询操作注解 @Insert(“插入的 SQL 语句”)：执行新增操作注解 @Update(“修改的 SQL 语句”)：执行修改操作注解 @Delete(“删除的 SQL 语句”)：执行删除操作注解 参数注解： @Param：当 SQL 语句需要多个（大于1）参数时，用来指定参数的对应规则 核心配置文件配置映射关系： 1234567&lt;mappers&gt; &lt;package name=&quot;使用了注解的Mapper接口所在包&quot;/&gt;&lt;/mappers&gt;&lt;!--或者--&gt;&lt;mappers&gt; &lt;mapper class=&quot;包名.Mapper名&quot;&gt;&lt;/mapper&gt;&lt;/mappers&gt; 基本增删改查： 创建 Mapper 接口 12345678910111213141516171819package mapper;public interface StudentMapper &#123; //查询全部 @Select(&quot;SELECT * FROM student&quot;) public abstract List&lt;Student&gt; selectAll(); //新增数据 @Insert(&quot;INSERT INTO student VALUES (#&#123;id&#125;,#&#123;name&#125;,#&#123;age&#125;)&quot;) public abstract Integer insert(Student student); //修改操作 @Update(&quot;UPDATE student SET name=#&#123;name&#125;,age=#&#123;age&#125; WHERE id=#&#123;id&#125;&quot;) public abstract Integer update(Student student); //删除操作 @Delete(&quot;DELETE FROM student WHERE id=#&#123;id&#125;&quot;) public abstract Integer delete(Integer id);&#125; 修改 MyBatis 的核心配置文件 123&lt;mappers&gt; &lt;package name=&quot;mapper&quot;/&gt;&lt;/mappers&gt; bean类 12345public class Student &#123; private Integer id; private String name; private Integer age;&#125; 测试类 1234567891011121314151617181920212223242526@Testpublic void selectAll() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = ssf.openSession(true); //4.获取StudentMapper接口的实现类对象 StudentMapper mapper = sqlSession.getMapper(StudentMapper.class); //5.调用实现类对象中的方法，接收结果 List&lt;Student&gt; list = mapper.selectAll(); //6.处理结果 for (Student student : list) &#123; System.out.println(student); &#125; //7.释放资源 sqlSession.close(); is.close();&#125; 多表操作 相关注解 实现复杂关系映射之前我们可以在映射文件中通过配置 来实现，使用注解开发后，可以使用 @Results 注解，@Result 注解，@One 注解，@Many 注解组合完成复杂关系的配置。 注解 说明 @Results 代替 标签，注解中使用单个 @Result 注解或者 @Result 集合 使用格式：@Results({ @Result(), @Result() })或@Results({ @Result() }) @Result 代替 和 标签，@Result 中属性介绍： column：数据库的列名 property：封装类的变量名 one：需要使用 @One 注解（@Result(one = @One)） Many：需要使用 @Many 注解（@Result(many= @Many)） @One(一对一) 代替 标签，多表查询的关键，用来指定子查询返回单一对象。 select：指定调用 Mapper 接口中的某个方法 使用格式：@Result(column=“”, property=“”, one=@One(select=“”)) @Many(多对一) 代替标签，多表查询的关键，用来指定子查询返回对象集合。 select：指定调用 Mapper 接口中的某个方法 使用格式：@Result(column=“”, property=“”, many=@Many(select=“”)) 一对一 身份证对人 PersonMapper 接口 12345public interface PersonMapper &#123; //根据id查询 @Select(&quot;SELECT * FROM person WHERE id=#&#123;id&#125;&quot;) public abstract Person selectById(Integer id);&#125; CardMapper接口 12345678910111213141516171819public interface CardMapper &#123; //查询全部 @Select(&quot;SELECT * FROM card&quot;) @Results(&#123; @Result(column = &quot;id&quot;,property = &quot;id&quot;), @Result(column = &quot;number&quot;,property = &quot;number&quot;), @Result( property = &quot;p&quot;, // 被包含对象的变量名 javaType = Person.class, // 被包含对象的实际数据类型 column = &quot;pid&quot;, // 根据查询出的card表中的pid字段来查询person表 /* one、@One 一对一固定写法 select属性：指定调用哪个接口中的哪个方法 */ one = @One(select = &quot;one_to_one.PersonMapper.selectById&quot;) ) &#125;) public abstract List&lt;Card&gt; selectAll();&#125; 测试类（详细代码参考单表操作） 12345678//1.加载核心配置文件//2.获取SqlSession工厂对象//3.通过工厂对象获取SqlSession对象//4.获取StudentMapper接口的实现类对象CardMapper mapper = sqlSession.getMapper(CardMapper.class);//5.调用实现类对象中的方法，接收结果List&lt;Card&gt; list = mapper.selectAll(); 一对多 班级和学生 StudentMapper接口 12345public interface StudentMapper &#123; //根据cid查询student表 cid是外键约束列 @Select(&quot;SELECT * FROM student WHERE cid=#&#123;cid&#125;&quot;) public abstract List&lt;Student&gt; selectByCid(Integer cid);&#125; ClassesMapper接口 123456789101112131415public interface ClassesMapper &#123; //查询全部 @Select(&quot;SELECT * FROM classes&quot;) @Results(&#123; @Result(column = &quot;id&quot;, property = &quot;id&quot;), @Result(column = &quot;name&quot;, property = &quot;name&quot;), @Result( property = &quot;students&quot;, //被包含对象的变量名 javaType = List.class, //被包含对象的实际数据类型 column = &quot;id&quot;, //根据id字段查询student表 many = @Many(select = &quot;one_to_many.StudentMapper.selectByCid&quot;) ) &#125;) public abstract List&lt;Classes&gt; selectAll();&#125; 测试类 1234//4.获取StudentMapper接口的实现类对象ClassesMapper mapper = sqlSession.getMapper(ClassesMapper.class);//5.调用实现类对象中的方法，接收结果List&lt;Classes&gt; classes = mapper.selectAll(); 多对多 学生和课程 SQL 查询语句 12SELECT DISTINCT s.id,s.name,s.age FROM student s,stu_cr sc WHERE sc.sid=s.idSELECT c.id,c.name FROM stu_cr sc,course c WHERE sc.cid=c.id AND sc.sid=#&#123;id&#125; CourseMapper 接口 12345public interface CourseMapper &#123; //根据学生id查询所选课程 @Select(&quot;SELECT c.id,c.name FROM stu_cr sc,course c WHERE sc.cid=c.id AND sc.sid=#&#123;id&#125;&quot;) public abstract List&lt;Course&gt; selectBySid(Integer id);&#125; StudentMapper 接口 12345678910111213141516public interface StudentMapper &#123; //查询全部 @Select(&quot;SELECT DISTINCT s.id,s.name,s.age FROM student s,stu_cr sc WHERE sc.sid=s.id&quot;) @Results(&#123; @Result(column = &quot;id&quot;,property = &quot;id&quot;), @Result(column = &quot;name&quot;,property = &quot;name&quot;), @Result(column = &quot;age&quot;,property = &quot;age&quot;), @Result( property = &quot;courses&quot;, //被包含对象的变量名 javaType = List.class, //被包含对象的实际数据类型 column = &quot;id&quot;, //根据查询出的student表中的id字段查询中间表和课程表 many = @Many(select = &quot;many_to_many.CourseMapper.selectBySid&quot;) ) &#125;) public abstract List&lt;Student&gt; selectAll();&#125; 测试类 1234//4.获取StudentMapper接口的实现类对象StudentMapper mapper = sqlSession.getMapper(StudentMapper.class);//5.调用实现类对象中的方法，接收结果List&lt;Student&gt; students = mapper.selectAll(); 缓存机制 缓存概述 缓存：缓存就是一块内存空间，保存临时数据。 作用：将数据源（数据库或者文件）中的数据读取出来存放到缓存中，再次获取时直接从缓存中获取，可以减少和数据库交互的次数，提升程序的性能。 缓存适用： 适用于缓存的：经常查询但不经常修改的，数据的正确与否对最终结果影响不大的 不适用缓存的：经常改变的数据 , 敏感数据（例如：股市的牌价，银行的汇率，银行卡里面的钱）等等 缓存类别： 一级缓存：SqlSession 级别的缓存，又叫本地会话缓存，自带的（不需要配置），一级缓存的生命周期与 SqlSession 一致。在操作数据库时需要构造 SqlSession 对象，在对象中有一个数据结构（HashMap）用于存储缓存数据，不同的 SqlSession 之间的缓存数据区域是互相不影响的 二级缓存：mapper（namespace）级别的缓存，二级缓存的使用，需要手动开启（需要配置）。多个 SqlSession 去操作同一个 Mapper 的 SQL 可以共用二级缓存，二级缓存是跨 SqlSession 的 开启缓存：配置核心配置文件中 标签 cacheEnabled：true 表示全局性地开启所有映射器配置文件中已配置的任何缓存，默认 true 参考文章：https://www.cnblogs.com/ysocean/p/7342498.html 一级缓存 一级缓存是 SqlSession 级别的缓存。 工作流程： 第一次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，如果没有，从数据库查询用户信息，得到用户信息，将用户信息存储到一级缓存中；第二次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，缓存中有，直接从缓存中获取用户信息。 一级缓存的失效： SqlSession 不同 SqlSession 相同，查询条件不同时（还未缓存该数据） SqlSession 相同，手动清除了一级缓存，调用 sqlSession.clearCache() SqlSession 相同，执行 commit 操作或者执行插入、更新、删除，清空 SqlSession 中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。 Spring 整合 MyBatis 后，一级缓存作用： 未开启事务的情况，每次查询 Spring 都会创建新的 SqlSession，因此一级缓存失效 开启事务的情况，Spring 使用 ThreadLocal 获取当前资源绑定同一个 SqlSession，因此此时一级缓存是有效的 测试一级缓存存在： 123456789101112131415161718192021public void testFirstLevelCache()&#123; //1. 获取sqlSession对象 SqlSession sqlSession = SqlSessionFactoryUtils.openSession(); //2. 通过sqlSession对象获取UserDao接口的代理对象 UserDao userDao1 = sqlSession.getMapper(UserDao.class); //3. 调用UserDao接口的代理对象的findById方法获取信息 User user1 = userDao1.findById(1); System.out.println(user1); //sqlSession.clearCache() 清空缓存 UserDao userDao2 = sqlSession.getMapper(UserDao.class); User user = userDao.findById(1); System.out.println(user2); //4.测试两次结果是否一样 System.out.println(user1 == user2);//true //5. 提交事务关闭资源 SqlSessionFactoryUtils.commitAndClose(sqlSession);&#125; 二级缓存 基本介绍 二级缓存是 mapper 的缓存，只要是同一个命名空间（namespace）的 SqlSession 就共享二级缓存的内容，并且可以操作二级缓存。 作用：作用范围是整个应用，可以跨线程使用，适合缓存一些修改较少的数据。 工作流程：一个会话查询数据，这个数据就会被放在当前会话的一级缓存中，如果会话关闭或提交一级缓存中的数据会保存到二级缓存 二级缓存的基本使用： 在 MyBatisConfig.xml 文件开启二级缓存，cacheEnabled 默认值为 true，所以这一步可以省略不配置 1234&lt;!--配置开启二级缓存--&gt;&lt;settings&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 配置 Mapper 映射文件 标签表示当前这个 mapper 映射将使用二级缓存，区分的标准就看 mapper 的 namespace 值 12345&lt;mapper namespace=&quot;dao.UserDao&quot;&gt; &lt;!--开启user支持二级缓存--&gt; &lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;6000&quot; readOnly=&quot;&quot; size=&quot;1024&quot;/&gt; &lt;cache&gt;&lt;/cache&gt; &lt;!--则表示所有属性使用默认值--&gt;&lt;/mapper&gt; eviction（清除策略）： LRU– 最近最少使用：移除最长时间不被使用的对象，默认 FIFO – 先进先出：按对象进入缓存的顺序来移除它们 SOFT– 软引用：基于垃圾回收器状态和软引用规则移除对象 WEAK– 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象 flushInterval（刷新间隔）：可以设置为任意的正整数， 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新。 size（引用数目）：缓存存放多少元素，默认值是 1024。 readOnly（只读）：可以被设置为 true 或 false。 只读的缓存会给所有调用者返回缓存对象的相同实例，因此这些对象不能被修改，促进了性能提升 可读写的缓存会（通过序列化）返回缓存对象的拷贝， 速度上会慢一些，但是更安全，因此默认值是 false type：指定自定义缓存的全类名，实现 Cache 接口即可。 要进行二级缓存的类必须实现 java.io.Serializable 接口，可以使用序列化方式来保存对象。 1public class User implements Serializable&#123;&#125; 相关属性 select 标签的 useCache 属性映射文件中的 &lt;select&gt; 标签中设置 useCache=&quot;true&quot; 代表当前 statement 要使用二级缓存（默认）注意：如果每次查询都需要最新的数据 sql，要设置成 useCache=false，禁用二级缓存。 123&lt;select id=&quot;findAll&quot; resultType=&quot;user&quot; useCache=&quot;true&quot;&gt; select * from user&lt;/select&gt; 每个增删改标签都有 flushCache 属性，默认为 true，代表在执行增删改之后就会清除一、二级缓存，保证缓存的一致性；而查询标签默认值为 false，所以查询不会清空缓存 localCacheScope：本地缓存作用域， 中的配置项，默认值为 SESSION，当前会话的所有数据保存在会话缓存中，设置为 STATEMENT 禁用一级缓存 源码解析 事务提交二级缓存才生效：DefaultSqlSession 调用 commit() 时会回调 executor.commit()。 CachingExecutor#query()：执行查询方法，查询出的数据会先放入 entriesToAddOnCommit 集合暂存 12345678// 从二缓存中获取数据，获取不到去一级缓存获取List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key);if (list == null) &#123; // 回调 BaseExecutor#query list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); // 将数据放入 entriesToAddOnCommit 集合暂存，此时还没放入二级缓存 tcm.putObject(cache, key, list);&#125; commit()：事务提交，清空一级缓存，放入二级缓存，二级缓存使用 TransactionalCacheManager（tcm）管理 12345public void commit(boolean required) throws SQLException &#123; // 首先调用 BaseExecutor#commit 方法，【清空一级缓存】 delegate.commit(required); tcm.commit();&#125; TransactionalCacheManager#commit：查询出的数据放入二级缓存 123456public void commit() &#123; // 获取所有的缓存事务，挨着进行提交 for (TransactionalCache txCache : transactionalCaches.values()) &#123; txCache.commit(); &#125;&#125; 123456789public void commit() &#123; if (clearOnCommit) &#123; delegate.clear(); &#125; // 将 entriesToAddOnCommit 中的数据放入二级缓存 flushPendingEntries(); // 清空相关集合 reset();&#125; 123456private void flushPendingEntries() &#123; for (Map.Entry&lt;Object, Object&gt; entry : entriesToAddOnCommit.entrySet()) &#123; // 将数据放入二级缓存 delegate.putObject(entry.getKey(), entry.getValue()); &#125;&#125; 增删改操作会清空缓存： update()：CachingExecutor 的更新操作 12345public int update(MappedStatement ms, Object parameterObject) throws SQLException &#123; flushCacheIfRequired(ms); // 回调 BaseExecutor#update 方法，也会清空一级缓存 return delegate.update(ms, parameterObject);&#125; flushCacheIfRequired()：判断是否需要清空二级缓存 12345678private void flushCacheIfRequired(MappedStatement ms) &#123; Cache cache = ms.getCache(); // 判断二级缓存是否存在，然后判断标签的 flushCache 的值，增删改操作的 flushCache 属性默认为 true if (cache != null &amp;&amp; ms.isFlushCacheRequired()) &#123; // 清空二级缓存 tcm.clear(cache); &#125;&#125; 自定义缓存 1&lt;cache type=&quot;com.domain.something.MyCustomCache&quot;/&gt; type 属性指定的类必须实现 org.apache.ibatis.cache.Cache 接口，且提供一个接受 String 参数作为 id 的构造器。 123456789public interface Cache &#123; String getId(); int getSize(); void putObject(Object key, Object value); Object getObject(Object key); boolean hasKey(Object key); Object removeObject(Object key); void clear();&#125; 缓存的配置，只需要在缓存实现中添加公有的 JavaBean 属性，然后通过 cache 元素传递属性值，例如在缓存实现上调用一个名为 setCacheFile(String file) 的方法： 123&lt;cache type=&quot;com.domain.something.MyCustomCache&quot;&gt; &lt;property name=&quot;cacheFile&quot; value=&quot;/tmp/my-custom-cache.tmp&quot;/&gt;&lt;/cache&gt; 可以使用所有简单类型作为 JavaBean 属性的类型，MyBatis 会进行转换。 可以使用占位符（如 $&#123;cache.file&#125;），以便替换成在配置文件属性中定义的值 MyBatis 支持在所有属性设置完毕之后，调用一个初始化方法， 如果想要使用这个特性，可以在自定义缓存类里实现 org.apache.ibatis.builder.InitializingObject 接口。 123public interface InitializingObject &#123; void initialize() throws Exception;&#125; 注意：对缓存的配置（如清除策略、可读或可读写等），不能应用于自定义缓存。 对某一命名空间的语句，只会使用该命名空间的缓存进行缓存或刷新，在多个命名空间中共享相同的缓存配置和实例，可以使用 cache-ref 元素来引用另一个缓存 1&lt;cache-ref namespace=&quot;com.someone.application.data.SomeMapper&quot;/&gt; 构造语句 动态 SQL 基本介绍 动态 SQL 是 MyBatis 强大特性之一，逻辑复杂时，MyBatis 映射配置文件中，SQL 是动态变化的，所以引入动态 SQL 简化拼装 SQL 的操作。 DynamicSQL 包含的标签： if where set choose (when、otherwise) trim foreach 各个标签都可以进行灵活嵌套和组合 OGNL：Object Graphic Navigation Language（对象图导航语言），用于对数据进行访问。 参考文章：https://www.cnblogs.com/ysocean/p/7289529.html where ：条件标签，有动态条件则使用该标签代替 WHERE 关键字，封装查询条件。 作用：如果标签返回的内容是以 AND 或 OR 开头的，标签内会剔除掉。 表结构： if 基本格式： 123&lt;if test=“条件判断”&gt; 查询条件拼接&lt;/if&gt; 我们根据实体类的不同取值，使用不同的 SQL 语句来进行查询。比如在 id 如果不为空时可以根据 id 查询，如果username 不同空时还要加入用户名作为条件，这种情况在我们的多条件组合查询中经常会碰到。 UserMapper.xml 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapperPUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;mapper.UserMapper&quot;&gt; &lt;select id=&quot;selectCondition&quot; resultType=&quot;user&quot; parameterType=&quot;user&quot;&gt; SELECT * FROM user &lt;where&gt; &lt;if test=&quot;id != null &quot;&gt; id = #&#123;id&#125; &lt;/if&gt; &lt;if test=&quot;username != null &quot;&gt; AND username = #&#123;username&#125; &lt;/if&gt; &lt;if test=&quot;sex != null &quot;&gt; AND sex = #&#123;sex&#125; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt;\t&lt;/mapper&gt; MyBatisConfig.xml，引入映射配置文件 1234&lt;mappers&gt; &lt;!--mapper引入指定的映射配置 resource属性执行的映射配置文件的名称--&gt; &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;&lt;/mappers&gt; DAO 层 Mapper 接口 1234public interface UserMapper &#123; //多条件查询 public abstract List&lt;User&gt; selectCondition(Student stu);&#125; 实现类 123456789101112131415161718192021222324252627282930313233public class DynamicTest &#123; @Test public void selectCondition() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = ssf.openSession(true); //4.获取StudentMapper接口的实现类对象 UserMapper mapper = sqlSession.getMapper(UserMapper.class); User user = new User(); user.setId(2); user.setUsername(&quot;李四&quot;); //user.setSex(男); AND 后会自动剔除 //5.调用实现类的方法，接收结果 List&lt;Student&gt; list = mapper.selectCondition(user); //6.处理结果 for (User user : list) &#123; System.out.println(user); &#125; //7.释放资源 sqlSession.close(); is.close(); &#125;&#125; set ：进行更新操作的时候，含有 set 关键词，使用该标签 12345678910111213&lt;!-- 根据 id 更新 user 表的数据 --&gt;&lt;update id=&quot;updateUserById&quot; parameterType=&quot;com.ys.po.User&quot;&gt; UPDATE user u &lt;set&gt; &lt;if test=&quot;username != null and username != &#x27;&#x27;&quot;&gt; u.username = #&#123;username&#125;, &lt;/if&gt; &lt;if test=&quot;sex != null and sex != &#x27;&#x27;&quot;&gt; u.sex = #&#123;sex&#125; &lt;/if&gt; &lt;/set&gt; WHERE id=#&#123;id&#125;&lt;/update&gt; 如果第一个条件 username 为空，那么 sql 语句为：update user u set u.sex=? where id=? 如果第一个条件不为空，那么 sql 语句为：update user u set u.username = ? ,u.sex = ? where id=? choose 假如不想用到所有的查询条件，只要查询条件有一个满足即可，使用 choose 标签可以解决此类问题，类似于 Java 的 switch 语句 标签：， 12345678910111213141516&lt;select id=&quot;selectUserByChoose&quot; resultType=&quot;user&quot; parameterType=&quot;user&quot;&gt; SELECT * FROM user &lt;where&gt; &lt;choose&gt; &lt;when test=&quot;id !=&#x27;&#x27; and id != null&quot;&gt; id=#&#123;id&#125; &lt;/when&gt; &lt;when test=&quot;username !=&#x27;&#x27; and username != null&quot;&gt; AND username=#&#123;username&#125; &lt;/when&gt; &lt;otherwise&gt; AND sex=#&#123;sex&#125; &lt;/otherwise&gt; &lt;/choose&gt; &lt;/where&gt;&lt;/select&gt; 有三个条件，id、username、sex，只能选择一个作为查询条件 如果 id 不为空，那么查询语句为：select * from user where id=? 如果 id 为空，那么看 username 是否为空 如果不为空，那么语句为：select * from user where username=? 如果 username 为空，那么查询语句为select * from user where sex=? trim trim 标记是一个格式化的标记，可以完成 set 或者是 where 标记的功能，自定义字符串截取 prefix：给拼串后的整个字符串加一个前缀，trim 标签体中是整个字符串拼串后的结果 prefixOverrides：去掉整个字符串前面多余的字符 suffix：给拼串后的整个字符串加一个后缀 suffixOverrides：去掉整个字符串后面多余的字符 改写 if + where 语句： 1234567891011&lt;select id=&quot;selectUserByUsernameAndSex&quot; resultType=&quot;user&quot; parameterType=&quot;com.ys.po.User&quot;&gt; SELECT * FROM user &lt;trim prefix=&quot;where&quot; prefixOverrides=&quot;and | or&quot;&gt; &lt;if test=&quot;username != null&quot;&gt; AND username=#&#123;username&#125; &lt;/if&gt; &lt;if test=&quot;sex != null&quot;&gt; AND sex=#&#123;sex&#125; &lt;/if&gt; &lt;/trim&gt;&lt;/select&gt; 改写 if + set 语句： 12345678910111213&lt;!-- 根据 id 更新 user 表的数据 --&gt;&lt;update id=&quot;updateUserById&quot; parameterType=&quot;com.ys.po.User&quot;&gt; UPDATE user u &lt;trim prefix=&quot;set&quot; suffixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;username != null and username != &#x27;&#x27;&quot;&gt; u.username = #&#123;username&#125;, &lt;/if&gt; &lt;if test=&quot;sex != null and sex != &#x27;&#x27;&quot;&gt; u.sex = #&#123;sex&#125;, &lt;/if&gt; &lt;/trim&gt; WHERE id=#&#123;id&#125;&lt;/update&gt; foreach 基本格式： 1234&lt;foreach&gt;：循环遍历标签。适用于多个参数或者的关系。 &lt;foreach collection=“”open=“”close=“”item=“”separator=“”&gt; 获取参数 &lt;/foreach&gt; 属性： collection：参数容器类型， (list-集合， array-数组) open：开始的 SQL 语句 close：结束的 SQL 语句 item：参数变量名 separator：分隔符 需求：循环执行 sql 的拼接操作，SELECT * FROM user WHERE id IN (1,2,5) UserMapper.xml片段 12345678&lt;select id=&quot;selectByIds&quot; resultType=&quot;user&quot; parameterType=&quot;list&quot;&gt; SELECT * FROM student &lt;where&gt; &lt;foreach collection=&quot;list&quot; open=&quot;id IN(&quot; close=&quot;)&quot; item=&quot;id&quot; separator=&quot;,&quot;&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/where&gt;&lt;/select&gt; 测试代码片段 1234567891011//4.获取StudentMapper接口的实现类对象UserMapper mapper = sqlSession.getMapper(UserMapper.class);List&lt;Integer&gt; ids = new ArrayList&lt;&gt;();Collections.addAll(list, 1, 2);//5.调用实现类的方法，接收结果List&lt;User&gt; list = mapper.selectByIds(ids);for (User user : list) &#123; System.out.println(user);&#125; SQL片段 将一些重复性的 SQL 语句进行抽取，以达到复用的效果。 格式： 12&lt;sql id=“片段唯一标识”&gt;抽取的SQL语句&lt;/sql&gt; &lt;!--抽取标签--&gt;&lt;include refid=“片段唯一标识”/&gt; &lt;!--引入标签--&gt; 使用： 12345678910&lt;sql id=&quot;select&quot;&gt;SELECT * FROM user&lt;/sql&gt;&lt;select id=&quot;selectByIds&quot; resultType=&quot;user&quot; parameterType=&quot;list&quot;&gt; &lt;include refid=&quot;select&quot;/&gt; &lt;where&gt; &lt;foreach collection=&quot;list&quot; open=&quot;id IN(&quot; close=&quot;)&quot; item=&quot;id&quot; separator=&quot;,&quot;&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/where&gt;&lt;/select&gt; 逆向工程 MyBatis 逆向工程，可以针对单表自动生成 MyBatis 执行所需要的代码（mapper.java、mapper.xml、pojo…）。 generatorConfig.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE generatorConfigurationPUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot;&quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt;&lt;generatorConfiguration&gt; &lt;context id=&quot;testTables&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;commentGenerator&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name=&quot;suppressAllComments&quot; value=&quot;true&quot; /&gt; &lt;/commentGenerator&gt; &lt;!--数据库连接的信息：驱动类、连接地址、用户名、密码 --&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql://localhost:3306/mybatisrelation&quot; userId=&quot;root&quot; password=&quot;root&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- 默认false，把JDBC DECIMAL 和 NUMERIC 类型解析为 Integer，为 true时把JDBC DECIMAL和NUMERIC类型解析为java.math.BigDecimal --&gt; &lt;javaTypeResolver&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot; /&gt; &lt;/javaTypeResolver&gt; &lt;!-- targetProject:生成PO类的位置！！ --&gt; &lt;javaModelGenerator targetPackage=&quot;com.ys.po&quot; targetProject=&quot;.\\src&quot;&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot; /&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot; /&gt; &lt;/javaModelGenerator&gt; &lt;!-- targetProject:mapper映射文件生成的位置！！ --&gt; &lt;sqlMapGenerator targetPackage=&quot;com.ys.mapper&quot; targetProject=&quot;.\\src&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot; /&gt; &lt;/sqlMapGenerator&gt; &lt;!-- targetPackage：mapper接口生成的位置，重要！！ --&gt; &lt;javaClientGenerator type=&quot;XMLMAPPER&quot; targetPackage=&quot;com.ys.mapper&quot; targetProject=&quot;.\\src&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot; /&gt; &lt;/javaClientGenerator&gt; &lt;!-- 指定数据库表，要生成哪些表，就写哪些表，要和数据库中对应，不能写错！ --&gt; &lt;table tableName=&quot;items&quot;&gt;&lt;/table&gt; &lt;table tableName=&quot;orders&quot;&gt;&lt;/table&gt; &lt;table tableName=&quot;orderdetail&quot;&gt;&lt;/table&gt; &lt;table tableName=&quot;user&quot;&gt;&lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 生成代码： 1234567891011121314public void testGenerator() throws Exception&#123; List&lt;String&gt; warnings = new ArrayList&lt;String&gt;(); boolean overwrite = true; //指向逆向工程配置文件 File configFile = new File(GeneratorTest.class. getResource(&quot;/generatorConfig.xml&quot;).getFile()); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = cp.parseConfiguration(configFile); DefaultShellCallback callback = new DefaultShellCallback(overwrite); MyBatisGenerator myBatisGenerator = new MyBatisGenerator(config, callback, warnings); myBatisGenerator.generate(null);&#125; 参考文章：https://www.cnblogs.com/ysocean/p/7360409.html 构建 SQL 基础语法 MyBatis 提供了 org.apache.ibatis.jdbc.SQL功能类，专门用于构建 SQL 语句。 方法 说明 SELECT(String… columns) 根据字段拼接查询语句 FROM(String… tables) 根据表名拼接语句 WHERE(String… conditions) 根据条件拼接语句 INSERT_INTO(String tableName) 根据表名拼接新增语句 INTO_VALUES(String… values) 根据值拼接新增语句 UPDATE(String table) 根据表名拼接修改语句 DELETE_FROM(String table) 根据表名拼接删除语句 增删改查注解： @SelectProvider：生成查询用的 SQL 语句 @InsertProvider：生成新增用的 SQL 语句 @UpdateProvider：生成修改用的 SQL 语句注解 @DeleteProvider：生成删除用的 SQL 语句注解。 type 属性：生成 SQL 语句功能类对象 method 属性：指定调用方法 基本操作 MyBatisConfig.xml 配置 1234&lt;!-- mappers引入映射配置文件 --&gt;&lt;mappers&gt; &lt;package name=&quot;mapper&quot;/&gt;&lt;/mappers&gt; Mapper 类 123456789101112131415161718public interface StudentMapper &#123; //查询全部 @SelectProvider(type = ReturnSql.class, method = &quot;getSelectAll&quot;) public abstract List&lt;Student&gt; selectAll(); //新增数据 @InsertProvider(type = ReturnSql.class, method = &quot;getInsert&quot;) public abstract Integer insert(Student student); //修改操作 @UpdateProvider(type = ReturnSql.class, method = &quot;getUpdate&quot;) public abstract Integer update(Student student); //删除操作 @DeleteProvider(type = ReturnSql.class, method = &quot;getDelete&quot;) public abstract Integer delete(Integer id);&#125; ReturnSQL 类 123456789101112131415161718192021222324252627282930313233343536373839404142public class ReturnSql &#123; //定义方法，返回查询的sql语句 public String getSelectAll() &#123; return new SQL() &#123; &#123; SELECT(&quot;*&quot;); FROM(&quot;student&quot;); &#125; &#125;.toString(); &#125; //定义方法，返回新增的sql语句 public String getInsert(Student stu) &#123; return new SQL() &#123; &#123; INSERT_INTO(&quot;student&quot;); INTO_VALUES(&quot;#&#123;id&#125;,#&#123;name&#125;,#&#123;age&#125;&quot;); &#125; &#125;.toString(); &#125; //定义方法，返回修改的sql语句 public String getUpdate(Student stu) &#123; return new SQL() &#123; &#123; UPDATE(&quot;student&quot;); SET(&quot;name=#&#123;name&#125;&quot;,&quot;age=#&#123;age&#125;&quot;); WHERE(&quot;id=#&#123;id&#125;&quot;); &#125; &#125;.toString(); &#125; //定义方法，返回删除的sql语句 public String getDelete(Integer id) &#123; return new SQL() &#123; &#123; DELETE_FROM(&quot;student&quot;); WHERE(&quot;id=#&#123;id&#125;&quot;); &#125; &#125;.toString(); &#125;&#125; 功能测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class SqlTest &#123; @Test //查询全部 public void selectAll() throws Exception&#123; //1.加载核心配置文件 InputStream is = Resources.getResourceAsStream(&quot;MyBatisConfig.xml&quot;); //2.获取SqlSession工厂对象 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(is); //3.通过工厂对象获取SqlSession对象 SqlSession sqlSession = sqlSessionFactory.openSession(true); //4.获取StudentMapper接口的实现类对象 StudentMapper mapper = sqlSession.getMapper(StudentMapper.class); //5.调用实现类对象中的方法，接收结果 List&lt;Student&gt; list = mapper.selectAll(); //6.处理结果 for (Student student : list) &#123; System.out.println(student); &#125; //7.释放资源 sqlSession.close(); is.close(); &#125; @Test //新增 public void insert() throws Exception&#123; //1 2 3 4获取StudentMapper接口的实现类对象 StudentMapper mapper = sqlSession.getMapper(StudentMapper.class); //5.调用实现类对象中的方法，接收结果 -&gt;6 7 Student stu = new Student(4,&quot;赵六&quot;,26); Integer result = mapper.insert(stu); &#125; @Test //修改 public void update() throws Exception&#123; //1 2 3 4 5调用实现类对象中的方法，接收结果 -&gt;6 7 Student stu = new Student(4,&quot;赵六wq&quot;,36); Integer result = mapper.update(stu); &#125; @Test //删除 public void delete() throws Exception&#123; //1 2 3 4 5 6 7 Integer result = mapper.delete(4); &#125;&#125; 运行原理 运行机制 MyBatis 运行过程： 加载配置文件：首先加载 MyBatis 的全局配置文件，通过 XPath 方式解析 XML 配置文件，填充到 Configuration 对象中。 解析映射器配置：解析映射器配置的映射文件，并构建 MappedStatement 对象填充至 Configuration。 创建 SqlSession：创建一个 DefaultSqlSession 对象，根据参数创建指定类型的 Executor，二级缓存默认开启。 获取 Mapper 代理对象：通过 DefaultSqlSession 的 getMapper() 方法获取 Mapper 接口的代理对象 MapperProxy。 执行 SQL 语句： MapperProxy.invoke() 方法执行代理方法，通过 MapperMethod 判断执行的是增删改查中的哪个方法。 查询方法调用sqlSession.selectOne()，从 Configuration 中获取执行者对象 MappedStatement，然后 Executor 调用 executor.query 开始执行查询方法。 执行 SQL 查询过程： Executor.query()方法开始执行查询，首先尝试从二级缓存查询，然后从一级缓存查询，最后去数据库查询并放入一级缓存。 在查询过程中，涉及到 Configuration 对象的配置，创建 StatementHandler、ParameterHandler 和 ResultSetHandler 对象，获取 JDBC 原生的 Connection 数据库连接对象，创建 Statement 执行者对象，设置预编译参数，并执行 SQL 查询操作。 处理查询结果： 结果集由 ResultSetHandler 处理，映射成 Java 对象，并存入 ResultHandler 对象中。 结果集处理完成后，可能存在多个结果集的情况，最终将结果放入一级（本地）缓存并返回结果集。 总结四大对象： StatementHandler：执行 SQL 语句的对象。 ParameterHandler：设置预编译参数的对象。 ResultSetHandler：处理结果集的对象。 Executor：执行器，真正进行 Java 与数据库交互的对象。 插件使用 插件原理 实现原理：插件是按照插件配置顺序创建层层包装对象，执行目标方法的之后，按照逆向顺序执行（栈）。在四大对象创建时： 每个创建出来的对象不是直接返回的，而是 interceptorChain.pluginAll(parameterHandler) 获取到所有 Interceptor（插件需要实现的接口），调用 interceptor.plugin(target)返回 target 包装后的对象 插件机制可以使用插件为目标对象创建一个代理对象，代理对象可以拦截到四大对象的每一个执行 123456789101112131415161718192021222324252627282930313233343536373839404142@Intercepts( &#123; @Signature(type=StatementHandler.class,method=&quot;parameterize&quot;,args=java.sql.Statement.class) &#125;)public class MyFirstPlugin implements Interceptor&#123; //intercept：拦截目标对象的目标方法的执行 @Override public Object intercept(Invocation invocation) throws Throwable &#123; System.out.println(&quot;MyFirstPlugin...intercept:&quot; + invocation.getMethod()); //动态的改变一下sql运行的参数：以前1号员工，实际从数据库查询11号员工 Object target = invocation.getTarget(); System.out.println(&quot;当前拦截到的对象：&quot; + target); //拿到：StatementHandler==&gt;ParameterHandler===&gt;parameterObject //拿到target的元数据 MetaObject metaObject = SystemMetaObject.forObject(target); Object value = metaObject.getValue(&quot;parameterHandler.parameterObject&quot;); System.out.println(&quot;sql语句用的参数是：&quot; + value); //修改完sql语句要用的参数 metaObject.setValue(&quot;parameterHandler.parameterObject&quot;, 11); //执行目标方法 Object proceed = invocation.proceed(); //返回执行后的返回值 return proceed; &#125; // plugin：包装目标对象的，为目标对象创建一个代理对象 @Override public Object plugin(Object target) &#123; //可以借助 Plugin 的 wrap 方法来使用当前 Interceptor 包装我们目标对象 System.out.println(&quot;MyFirstPlugin...plugin:mybatis将要包装的对象&quot; + target); Object wrap = Plugin.wrap(target, this); //返回为当前target创建的动态代理 return wrap; &#125; // setProperties：将插件注册时的property属性设置进来 @Override public void setProperties(Properties properties) &#123; System.out.println(&quot;插件配置的信息：&quot; + properties); &#125;&#125; 核心配置文件： 1234567&lt;!--plugins：注册插件 --&gt;&lt;plugins&gt; &lt;plugin interceptor=&quot;mybatis.dao.MyFirstPlugin&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;/&gt; &lt;/plugin&gt;&lt;/plugins&gt; 分页插件 分页插件可以将很多条结果进行分页显示。如果当前在第一页，则没有上一页。如果当前在最后一页，则没有下一页，需要明确当前是第几页，这一页中显示多少条结果。 MyBatis 是不带分页功能的，如果想实现分页功能，需要手动编写 LIMIT 语句，不同的数据库实现分页的 SQL 语句也是不同，手写分页 成本较高 PageHelper：第三方分页助手，将复杂的分页操作进行封装，从而让分页功能变得非常简单 分页操作 开发步骤： 导入 PageHelper 的 Maven 坐标 在 MyBatis 核心配置文件中配置 PageHelper 插件注意：分页助手的插件配置在通用 Mapper 之前 与 MySQL 分页查询页数计算公式不同 static &lt;E&gt; Page&lt;E&gt; startPage(int pageNum, int pageSize)：pageNum第几页，pageSize页面大小。 1234567&lt;plugins&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageInterceptor&quot;&gt; &lt;!-- 指定方言 --&gt; &lt;property name=&quot;dialect&quot; value=&quot;mysql&quot;/&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;mappers&gt;.........&lt;/mappers&gt; 123456789@Testpublic void selectAll() &#123; //第一页：显示2条数据 PageHelper.startPage(1,2); List&lt;Student&gt; students = sqlSession.selectList(&quot;StudentMapper.selectAll&quot;); for (Student student : students) &#123; System.out.println(student); &#125;&#125; 参数获取 PageInfo 是 MyBatis 分页插件 PageHelper 中的一个实用工具类，用于处理分页查询结果的相关信息，如当前页码、每页显示数量、总记录数等。 构造方法： PageInfo 提供了多个构造方法，可以根据不同的参数情况创建 PageInfo 对象。主要的构造方法通常包括传入当前页码、每页显示数量、总记录数等信息。 PageInfo()：无参构造方法 PageInfo(List list)：传入查询结果列表的构造方法 PageInfo(List list, int navigatePages)：传入查询结果列表和导航页码数量的构造方法 PageInfo(List list, int navigatePages, int navigatePages)：传入查询结果列表、导航页码数量和连续显示页码数量的构造方法 PageInfo相关API： getPageNum()： 返回当前页码。 getPageSize()： 返回每页显示的记录数。 getTotal()： 返回总记录数。 getPages()： 返回总页数。 getList()： 返回当前页的数据列表。 isHasPreviousPage()： 返回是否有上一页。 如果当前页码大于 1，则返回 true；否则返回 false。 isHasNextPage()： 返回是否有下一页。 如果当前页码小于总页数，则返回 true；否则返回 false。 getNavigatePages()： 返回导航页码数量。 导航页码数量是指在页面上显示的连续页码的数量。 getNavigatepageNums()： 返回一个数组，包含了当前页前后连续的导航页码。 这个数组的长度由 getNavigatePages() 决定。 setNavigatePages(int navigatePages)： 设置导航页码数量。 可以通过这个方法来改变导航页码的数量。 setList(List list)： 设置当前页的数据列表。 可以通过这个方法来重新设置当前页的数据列表。 navigateFirstPage()： 导航到第一页。 将当前页码设置为 1。 navigateLastPage()： 导航到最后一页。 将当前页码设置为总页数。 navigateToPage(int pageNum)： 导航到指定页码。 将当前页码设置为指定的页码。 代码演示： 123456789101112131415161718192021222324252627282930313233343536373839404142import com.github.pagehelper.PageHelper;import com.github.pagehelper.PageInfo;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import java.io.InputStream;import java.util.List;public class Main &#123; public static void main(String[] args) &#123; // 读取 MyBatis 配置文件 String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Main.class.getClassLoader().getResourceAsStream(resource); // 创建 SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); // 创建 SqlSession try (SqlSession sqlSession = sqlSessionFactory.openSession()) &#123; // 设置分页参数，这里使用 PageHelper PageHelper.startPage(1, 10); // 第一页，每页显示 10 条数据 // 执行查询 List&lt;User&gt; userList = sqlSession.selectList(&quot;UserMapper.selectUsers&quot;); // 封装查询结果为 PageInfo 对象 PageInfo&lt;User&gt; pageInfo = new PageInfo&lt;&gt;(userList); // 输出分页信息 System.out.println(&quot;当前页码：&quot; + pageInfo.getPageNum()); System.out.println(&quot;每页显示数量：&quot; + pageInfo.getPageSize()); System.out.println(&quot;总记录数：&quot; + pageInfo.getTotal()); System.out.println(&quot;总页数：&quot; + pageInfo.getPages()); // 输出查询结果 for (User user : pageInfo.getList()) &#123; System.out.println(user); &#125; &#125; &#125;&#125; 手写 Mybatis 自定义框架，取名为 Godbatis。 dom4j解析XML文件 可参考老杜在 Mybatis 前面讲解的dom4j解析XML文件的视频。 模块名：parse-xml-by-dom4j（普通的Java Maven模块） 第一步：引入dom4j的依赖 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.group&lt;/groupId&gt; &lt;artifactId&gt;parse-xml-by-dom4j&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!--dom4j依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.dom4j&lt;/groupId&gt; &lt;artifactId&gt;dom4j&lt;/artifactId&gt; &lt;version&gt;2.1.3&lt;/version&gt; &lt;/dependency&gt; &lt;!--jaxen依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;jaxen&lt;/groupId&gt; &lt;artifactId&gt;jaxen&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--junit依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.13.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;17&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;17&lt;/maven.compiler.target&gt; &lt;/properties&gt;&lt;/project&gt; 第二步：编写配置文件godbatis-config.xml 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;configuration&gt; &lt;environments default=&quot;dev&quot;&gt; &lt;environment id=&quot;dev&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/powernode&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;mappers&gt; &lt;mapper resource=&quot;sqlmapper.xml&quot;/&gt; &lt;/mappers&gt; &lt;/environments&gt;&lt;/configuration&gt; 第三步：解析godbatis-config.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.powernode.dom4j;import org.dom4j.Document;import org.dom4j.Element;import org.dom4j.Node;import org.dom4j.io.SAXReader;import org.junit.Test;import java.util.HashMap;import java.util.List;import java.util.Map;/** * 使用dom4j解析XML文件 */public class ParseXMLByDom4j &#123; @Test public void testGodBatisConfig() throws Exception&#123; // 读取xml，获取document对象 SAXReader saxReader = new SAXReader(); Document document = saxReader.read(Thread.currentThread().getContextClassLoader().getResourceAsStream(&quot;godbatis-config.xml&quot;)); // 获取&lt;environments&gt;标签的default属性的值 Element environmentsElt = (Element)document.selectSingleNode(&quot;/configuration/environments&quot;); String defaultId = environmentsElt.attributeValue(&quot;default&quot;); System.out.println(defaultId); // 获取environment标签 Element environmentElt = (Element)document.selectSingleNode(&quot;/configuration/environments/environment[@id=&#x27;&quot; + defaultId + &quot;&#x27;]&quot;); // 获取事务管理器类型 Element transactionManager = environmentElt.element(&quot;transactionManager&quot;); String transactionManagerType = transactionManager.attributeValue(&quot;type&quot;); System.out.println(transactionManagerType); // 获取数据源类型 Element dataSource = environmentElt.element(&quot;dataSource&quot;); String dataSourceType = dataSource.attributeValue(&quot;type&quot;); System.out.println(dataSourceType); // 将数据源信息封装到Map集合 Map&lt;String,String&gt; dataSourceMap = new HashMap&lt;&gt;(); dataSource.elements().forEach(propertyElt -&gt; &#123; dataSourceMap.put(propertyElt.attributeValue(&quot;name&quot;), propertyElt.attributeValue(&quot;value&quot;)); &#125;); dataSourceMap.forEach((k, v) -&gt; System.out.println(k + &quot;:&quot; + v)); // 获取sqlmapper.xml文件的路径 Element mappersElt = (Element) document.selectSingleNode(&quot;/configuration/environments/mappers&quot;); mappersElt.elements().forEach(mapper -&gt; &#123; System.out.println(mapper.attributeValue(&quot;resource&quot;)); &#125;); &#125;&#125; 执行结果： 第四步：编写配置文件sqlmapper.xml 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;mapper namespace=&quot;car&quot;&gt; &lt;insert id=&quot;insertCar&quot;&gt; insert into t_car(id,car_num,brand,guide_price,produce_time,car_type) values(null,#&#123;carNum&#125;,#&#123;brand&#125;,#&#123;guidePrice&#125;,#&#123;produceTime&#125;,#&#123;carType&#125;) &lt;/insert&gt; &lt;select id=&quot;selectCarByCarNum&quot; resultType=&quot;com.powernode.mybatis.pojo.Car&quot;&gt; select id,car_num carNum,brand,guide_price guidePrice,produce_time produceTime,car_type carType from t_car where car_num = #&#123;carNum&#125; &lt;/select&gt;&lt;/mapper&gt; 第五步：解析sqlmapper.xml 1234567891011121314151617181920212223242526272829@Testpublic void testSqlMapper() throws Exception&#123; // 读取xml，获取document对象 SAXReader saxReader = new SAXReader(); Document document = saxReader.read(Thread.currentThread().getContextClassLoader().getResourceAsStream(&quot;sqlmapper.xml&quot;)); // 获取namespace Element mapperElt = (Element) document.selectSingleNode(&quot;/mapper&quot;); String namespace = mapperElt.attributeValue(&quot;namespace&quot;); System.out.println(namespace); // 获取sql id mapperElt.elements().forEach(statementElt -&gt; &#123; // 标签名 String name = statementElt.getName(); System.out.println(&quot;name:&quot; + name); // 如果是select标签，还要获取它的resultType if (&quot;select&quot;.equals(name)) &#123; String resultType = statementElt.attributeValue(&quot;resultType&quot;); System.out.println(&quot;resultType:&quot; + resultType); &#125; // sql id String id = statementElt.attributeValue(&quot;id&quot;); System.out.println(&quot;sqlId:&quot; + id); // sql语句 String sql = statementElt.getTextTrim(); System.out.println(&quot;sql:&quot; + sql); &#125;);&#125; 执行结果： GodBatis 第一步：IDEA中创建模块 模块：godbatis（创建普通的Java Maven模块，打包方式jar），引入相关依赖 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.god&lt;/groupId&gt; &lt;artifactId&gt;godbatis&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!--dom4j依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.dom4j&lt;/groupId&gt; &lt;artifactId&gt;dom4j&lt;/artifactId&gt; &lt;version&gt;2.1.3&lt;/version&gt; &lt;/dependency&gt; &lt;!--jaxen依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;jaxen&lt;/groupId&gt; &lt;artifactId&gt;jaxen&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--junit依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.13.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;17&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;17&lt;/maven.compiler.target&gt; &lt;/properties&gt;&lt;/project&gt; 第二步：资源工具类，方便获取指向配置文件的输入流 123456789101112131415161718192021package org.god.core;import java.io.InputStream;/** * 资源工具类 * @author 老杜 * @version 1.0 * @since 1.0 */public class Resources &#123; /** * 从类路径中获取配置文件的输入流 * @param config * @return 输入流，该输入流指向类路径中的配置文件 */ public static InputStream getResourcesAsStream(String config)&#123; return Thread.currentThread().getContextClassLoader().getResourceAsStream(config); &#125;&#125; 第三步：定义SqlSessionFactoryBuilder类 提供一个无参数构造方法，再提供一个build方法，该build方法要返回SqlSessionFactory对象 12345678910111213141516171819202122232425262728293031323334package org.god.core;import java.io.InputStream;/** * SqlSessionFactory对象构建器 * @author 老杜 * @version 1.0 * @since 1.0 */public class SqlSessionFactoryBuilder &#123; /** * 创建构建器对象 */ public SqlSessionFactoryBuilder() &#123; &#125; /** * 获取SqlSessionFactory对象 * 该方法主要功能是：读取godbatis核心配置文件，并构建SqlSessionFactory对象 * @param inputStream 指向核心配置文件的输入流 * @return SqlSessionFactory对象 */ public SqlSessionFactory build(InputStream inputStream)&#123; // 解析配置文件，创建数据源对象 // 解析配置文件，创建事务管理器对象 // 解析配置文件，获取所有的SQL映射对象 // 将以上信息封装到SqlSessionFactory对象中 // 返回 return null; &#125;&#125; 第四步：分析SqlSessionFactory类中有哪些属性 事务管理器 GodJDBCTransaction SQL映射对象集合 Map&lt;String, GodMappedStatement&gt; 第五步：定义GodJDBCTransaction 事务管理器最好是定义一个接口，然后每一个具体的事务管理器都实现这个接口。 1234567891011121314151617181920212223242526272829303132333435363738package org.god.core;import java.sql.Connection;/** * 事务管理器接口 * @author 老杜 * @version 1.0 * @since 1.0 */public interface TransactionManager &#123; /** * 提交事务 */ void commit(); /** * 回滚事务 */ void rollback(); /** * 关闭事务 */ void close(); /** * 开启连接 */ void openConnection(); /** * 获取连接对象 * @return 连接对象 */ Connection getConnection();&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package org.god.core;import javax.sql.DataSource;import java.sql.Connection;import java.sql.SQLException;/** * 事务管理器 * @author 老杜 * @version 1.0 * @since 1.0 */public class GodJDBCTransaction implements TransactionManager &#123; /** * 连接对象，控制事务时需要 */ private Connection conn; /** * 数据源对象 */ private DataSource dataSource; /** * 自动提交标志： * true表示自动提交 * false表示不自动提交 */ private boolean autoCommit; /** * 构造事务管理器对象 * @param autoCommit */ public GodJDBCTransaction(DataSource dataSource, boolean autoCommit) &#123; this.dataSource = dataSource; this.autoCommit = autoCommit; &#125; /** * 提交事务 */ public void commit()&#123; try &#123; conn.commit(); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; /** * 回滚事务 */ public void rollback()&#123; try &#123; conn.rollback(); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; @Override public void close() &#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; @Override public void openConnection() &#123; try &#123; this.conn = dataSource.getConnection(); this.conn.setAutoCommit(this.autoCommit); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; @Override public Connection getConnection() &#123; return conn; &#125;&#125; 第六步：事务管理器中需要数据源，定义GodUNPOOLEDDataSource 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package org.god.core;import java.io.PrintWriter;import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.sql.SQLFeatureNotSupportedException;import java.util.logging.Logger;/** * 数据源实现类，不使用连接池 * @author 老杜 * @version 1.0 * @since 1.0 */public class GodUNPOOLEDDataSource implements javax.sql.DataSource&#123; private String url; private String username; private String password; public GodUNPOOLEDDataSource(String driver, String url, String username, String password) &#123; try &#123; // 注册驱动 Class.forName(driver); &#125; catch (ClassNotFoundException e) &#123; throw new RuntimeException(e); &#125; this.url = url; this.username = username; this.password = password; &#125; @Override public Connection getConnection() throws SQLException &#123; return DriverManager.getConnection(url, username, password); &#125; @Override public Connection getConnection(String username, String password) throws SQLException &#123; return null; &#125; @Override public PrintWriter getLogWriter() throws SQLException &#123; return null; &#125; @Override public void setLogWriter(PrintWriter out) throws SQLException &#123; &#125; @Override public void setLoginTimeout(int seconds) throws SQLException &#123; &#125; @Override public int getLoginTimeout() throws SQLException &#123; return 0; &#125; @Override public Logger getParentLogger() throws SQLFeatureNotSupportedException &#123; return null; &#125; @Override public &lt;T&gt; T unwrap(Class&lt;T&gt; iface) throws SQLException &#123; return null; &#125; @Override public boolean isWrapperFor(Class&lt;?&gt; iface) throws SQLException &#123; return false; &#125;&#125; 第七步：定义GodMappedStatement 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package org.god.core;/** * SQL映射实体类 * @author 老杜 * @version 1.0 * @since 1.0 */public class GodMappedStatement &#123; private String sqlId; private String resultType; private String sql; private String parameterType; private String sqlType; @Override public String toString() &#123; return &quot;GodMappedStatement&#123;&quot; + &quot;sqlId=&#x27;&quot; + sqlId + &#x27;\\&#x27;&#x27; + &quot;, resultType=&#x27;&quot; + resultType + &#x27;\\&#x27;&#x27; + &quot;, sql=&#x27;&quot; + sql + &#x27;\\&#x27;&#x27; + &quot;, parameterType=&#x27;&quot; + parameterType + &#x27;\\&#x27;&#x27; + &quot;, sqlType=&#x27;&quot; + sqlType + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125; public String getSqlId() &#123; return sqlId; &#125; public void setSqlId(String sqlId) &#123; this.sqlId = sqlId; &#125; public String getResultType() &#123; return resultType; &#125; public void setResultType(String resultType) &#123; this.resultType = resultType; &#125; public String getSql() &#123; return sql; &#125; public void setSql(String sql) &#123; this.sql = sql; &#125; public String getParameterType() &#123; return parameterType; &#125; public void setParameterType(String parameterType) &#123; this.parameterType = parameterType; &#125; public String getSqlType() &#123; return sqlType; &#125; public void setSqlType(String sqlType) &#123; this.sqlType = sqlType; &#125; public GodMappedStatement(String sqlId, String resultType, String sql, String parameterType, String sqlType) &#123; this.sqlId = sqlId; this.resultType = resultType; this.sql = sql; this.parameterType = parameterType; this.sqlType = sqlType; &#125;&#125; 第八步：完善SqlSessionFactory类 123456789101112131415161718192021222324252627282930313233343536373839package org.god.core;import javax.sql.DataSource;import java.util.List;import java.util.Map;/** * SqlSession工厂对象，使用SqlSessionFactory可以获取会话对象 * @author 老杜 * @version 1.0 * @since 1.0 */public class SqlSessionFactory &#123; private TransactionManager transactionManager; private Map&lt;String, GodMappedStatement&gt; mappedStatements; public SqlSessionFactory(TransactionManager transactionManager, Map&lt;String, GodMappedStatement&gt; mappedStatements) &#123; this.transactionManager = transactionManager; this.mappedStatements = mappedStatements; &#125; public TransactionManager getTransactionManager() &#123; return transactionManager; &#125; public void setTransactionManager(TransactionManager transactionManager) &#123; this.transactionManager = transactionManager; &#125; public Map&lt;String, GodMappedStatement&gt; getMappedStatements() &#123; return mappedStatements; &#125; public void setMappedStatements(Map&lt;String, GodMappedStatement&gt; mappedStatements) &#123; this.mappedStatements = mappedStatements; &#125;&#125; 第九步：完善SqlSessionFactoryBuilder中的build方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117package org.god.core;import org.dom4j.Document;import org.dom4j.DocumentException;import org.dom4j.Element;import org.dom4j.io.SAXReader;import javax.sql.DataSource;import java.io.InputStream;import java.util.HashMap;import java.util.Map;/** * SqlSessionFactory对象构建器 * * @author 老杜 * @version 1.0 * @since 1.0 */public class SqlSessionFactoryBuilder &#123; /** * 创建构建器对象 */ public SqlSessionFactoryBuilder() &#123; &#125; /** * 获取SqlSessionFactory对象 * 该方法主要功能是：读取godbatis核心配置文件，并构建SqlSessionFactory对象 * * @param inputStream 指向核心配置文件的输入流 * @return SqlSessionFactory对象 */ public SqlSessionFactory build(InputStream inputStream) throws DocumentException &#123; SAXReader saxReader = new SAXReader(); Document document = saxReader.read(inputStream); Element environmentsElt = (Element) document.selectSingleNode(&quot;/configuration/environments&quot;); String defaultEnv = environmentsElt.attributeValue(&quot;default&quot;); Element environmentElt = (Element) document.selectSingleNode(&quot;/configuration/environments/environment[@id=&#x27;&quot; + defaultEnv + &quot;&#x27;]&quot;); // 解析配置文件，创建数据源对象 Element dataSourceElt = environmentElt.element(&quot;dataSource&quot;); DataSource dataSource = getDataSource(dataSourceElt); // 解析配置文件，创建事务管理器对象 Element transactionManagerElt = environmentElt.element(&quot;transactionManager&quot;); TransactionManager transactionManager = getTransactionManager(transactionManagerElt, dataSource); // 解析配置文件，获取所有的SQL映射对象 Element mappers = environmentsElt.element(&quot;mappers&quot;); Map&lt;String, GodMappedStatement&gt; mappedStatements = getMappedStatements(mappers); // 将以上信息封装到SqlSessionFactory对象中 SqlSessionFactory sqlSessionFactory = new SqlSessionFactory(transactionManager, mappedStatements); // 返回 return sqlSessionFactory; &#125; private Map&lt;String, GodMappedStatement&gt; getMappedStatements(Element mappers) &#123; Map&lt;String, GodMappedStatement&gt; mappedStatements = new HashMap&lt;&gt;(); mappers.elements().forEach(mapperElt -&gt; &#123; try &#123; String resource = mapperElt.attributeValue(&quot;resource&quot;); SAXReader saxReader = new SAXReader(); Document document = saxReader.read(Resources.getResourcesAsStream(resource)); Element mapper = (Element) document.selectSingleNode(&quot;/mapper&quot;); String namespace = mapper.attributeValue(&quot;namespace&quot;); mapper.elements().forEach(sqlMapper -&gt; &#123; String sqlId = sqlMapper.attributeValue(&quot;id&quot;); String sql = sqlMapper.getTextTrim(); String parameterType = sqlMapper.attributeValue(&quot;parameterType&quot;); String resultType = sqlMapper.attributeValue(&quot;resultType&quot;); String sqlType = sqlMapper.getName().toLowerCase(); // 封装GodMappedStatement对象 GodMappedStatement godMappedStatement = new GodMappedStatement(sqlId, resultType, sql, parameterType, sqlType); mappedStatements.put(namespace + &quot;.&quot; + sqlId, godMappedStatement); &#125;); &#125; catch (DocumentException e) &#123; throw new RuntimeException(e); &#125; &#125;); return mappedStatements; &#125; private TransactionManager getTransactionManager(Element transactionManagerElt, DataSource dataSource) &#123; String type = transactionManagerElt.attributeValue(&quot;type&quot;).toUpperCase(); TransactionManager transactionManager = null; if (&quot;JDBC&quot;.equals(type)) &#123; // 使用JDBC事务 transactionManager = new GodJDBCTransaction(dataSource, false); &#125; else if (&quot;MANAGED&quot;.equals(type)) &#123; // 事务管理器是交给JEE容器的 &#125; return transactionManager; &#125; private DataSource getDataSource(Element dataSourceElt) &#123; // 获取所有数据源的属性配置 Map&lt;String, String&gt; dataSourceMap = new HashMap&lt;&gt;(); dataSourceElt.elements().forEach(propertyElt -&gt; &#123; dataSourceMap.put(propertyElt.attributeValue(&quot;name&quot;), propertyElt.attributeValue(&quot;value&quot;)); &#125;); String dataSourceType = dataSourceElt.attributeValue(&quot;type&quot;).toUpperCase(); DataSource dataSource = null; if (&quot;POOLED&quot;.equals(dataSourceType)) &#123; &#125; else if (&quot;UNPOOLED&quot;.equals(dataSourceType)) &#123; dataSource = new GodUNPOOLEDDataSource(dataSourceMap.get(&quot;driver&quot;), dataSourceMap.get(&quot;url&quot;), dataSourceMap.get(&quot;username&quot;), dataSourceMap.get(&quot;password&quot;)); &#125; else if (&quot;JNDI&quot;.equals(dataSourceType)) &#123; &#125; return dataSource; &#125;&#125; 第十步：在SqlSessionFactory中添加openSession方法 12345public SqlSession openSession()&#123; transactionManager.openConnection(); SqlSession sqlSession = new SqlSession(transactionManager, mappedStatements); return sqlSession;&#125; 第十一步：编写SqlSession类中commit rollback close方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445package org.god.core;import java.sql.SQLException;import java.util.Map;/** * 数据库会话对象 * @author 老杜 * @version 1.0 * @since 1.0 */public class SqlSession &#123; private TransactionManager transactionManager; private Map&lt;String, GodMappedStatement&gt; mappedStatements; public SqlSession(TransactionManager transactionManager, Map&lt;String, GodMappedStatement&gt; mappedStatements) &#123; this.transactionManager = transactionManager; this.mappedStatements = mappedStatements; &#125; public void commit()&#123; try &#123; transactionManager.getConnection().commit(); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; public void rollback()&#123; try &#123; transactionManager.getConnection().rollback(); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; public void close()&#123; try &#123; transactionManager.getConnection().close(); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 第十二步：编写SqlSession类中的insert方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 插入数据 * * @param sqlId 要执行的sqlId * @param obj 插入的数据 * @return */public int insert(String sqlId, Object obj) &#123; GodMappedStatement godMappedStatement = mappedStatements.get(sqlId); Connection connection = transactionManager.getConnection(); // 获取sql语句 // insert into t_car(id,car_num,brand,guide_price,produce_time,car_type) values(null,#&#123;carNum&#125;,#&#123;brand&#125;,#&#123;guidePrice&#125;,#&#123;produceTime&#125;,#&#123;carType&#125;) String godbatisSql = godMappedStatement.getSql(); // insert into t_car(id,car_num,brand,guide_price,produce_time,car_type) values(null,?,?,?,?,?) String sql = godbatisSql.replaceAll(&quot;#\\\\&#123;[a-zA-Z0-9_\\\\$]*&#125;&quot;, &quot;?&quot;); // 重点一步 Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); int index = 1; while (godbatisSql.indexOf(&quot;#&quot;) &gt;= 0) &#123; int beginIndex = godbatisSql.indexOf(&quot;#&quot;) + 2; int endIndex = godbatisSql.indexOf(&quot;&#125;&quot;); map.put(index++, godbatisSql.substring(beginIndex, endIndex).trim()); godbatisSql = godbatisSql.substring(endIndex + 1); &#125; final PreparedStatement ps; try &#123; ps = connection.prepareStatement(sql); // 给?赋值 map.forEach((k, v) -&gt; &#123; try &#123; // 获取java实体类的get方法名 String getMethodName = &quot;get&quot; + v.toUpperCase().charAt(0) + v.substring(1); Method getMethod = obj.getClass().getDeclaredMethod(getMethodName); ps.setString(k, getMethod.invoke(obj).toString()); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;); int count = ps.executeUpdate(); ps.close(); return count; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125;&#125; 第十三步：编写SqlSession类中的selectOne方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 查询一个对象 * @param sqlId * @param parameterObj * @return */public Object selectOne(String sqlId, Object parameterObj)&#123; GodMappedStatement godMappedStatement = mappedStatements.get(sqlId); Connection connection = transactionManager.getConnection(); // 获取sql语句 String godbatisSql = godMappedStatement.getSql(); String sql = godbatisSql.replaceAll(&quot;#\\\\&#123;[a-zA-Z0-9_\\\\$]*&#125;&quot;, &quot;?&quot;); // 执行sql PreparedStatement ps = null; ResultSet rs = null; Object obj = null; try &#123; ps = connection.prepareStatement(sql); ps.setString(1, parameterObj.toString()); rs = ps.executeQuery(); if (rs.next()) &#123; // 将结果集封装对象，通过反射 String resultType = godMappedStatement.getResultType(); Class&lt;?&gt; aClass = Class.forName(resultType); Constructor&lt;?&gt; con = aClass.getDeclaredConstructor(); obj = con.newInstance(); // 给对象obj属性赋值 ResultSetMetaData rsmd = rs.getMetaData(); int columnCount = rsmd.getColumnCount(); for (int i = 1; i &lt;= columnCount; i++) &#123; String columnName = rsmd.getColumnName(i); String setMethodName = &quot;set&quot; + columnName.toUpperCase().charAt(0) + columnName.substring(1); Method setMethod = aClass.getDeclaredMethod(setMethodName, aClass.getDeclaredField(columnName).getType()); setMethod.invoke(obj, rs.getString(columnName)); &#125; &#125; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; finally &#123; if (rs != null) &#123; try &#123; rs.close(); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; try &#123; ps.close(); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; return obj;&#125;","tags":["Mybatis"],"categories":["学习笔记"]},{"title":"MySQL--存储引擎","path":"/2023/05/21/MySQL--存储引擎/","content":"基本介绍 对比其他数据库，MySQL 的架构可以在不同场景应用并发挥良好作用，主要体现在存储引擎，插件式的存储引擎架构将查询处理和其他的系统任务以及数据的存储提取分离，可以针对不同的存储需求可以选择最优的存储引擎。 存储引擎的介绍： MySQL 数据库使用不同的机制存取表文件 , 机制的差别在于不同的存储方式、索引技巧、锁定水平等不同的功能和能力，在 MySQL 中，将这些不同的技术及配套的功能称为存储引擎 Oracle、SqlServer 等数据库只有一种存储引擎，MySQL 提供了插件式的存储引擎架构，所以 MySQL 存在多种存储引擎 , 就会让数据库采取了不同的处理数据的方式和扩展功能 在关系型数据库中数据的存储是以表的形式存进行，所以存储引擎也称为表类型（存储和操作此表的类型） 通过选择不同的引擎，能够获取最佳的方案, 也能够获得额外的速度或者功能，提高程序的整体效果。 MySQL 支持的存储引擎： MySQL 支持的引擎包括：InnoDB、MyISAM、MEMORY、Archive、Federate、CSV、BLACKHOLE 等 MySQL5.5 之前的默认存储引擎是 MyISAM，5.5 之后就改为了 InnoDB 引擎对比 MyISAM 特点：不支持事务和外键，读取速度快，节约资源 应用场景：适用于读多写少的场景，对事务的完整性要求不高，比如一些数仓、离线数据、支付宝的年度总结之类的场景，业务进行只读操作，查询起来会更快 存储方式： 每个 MyISAM 在磁盘上存储成 3 个文件，其文件名都和表名相同，拓展名不同 表的定义保存在 .frm 文件，表数据保存在 .MYD (MYData) 文件中，索引保存在 .MYI (MYIndex) 文件中 InnoDB (MySQL5.5 版本后默认的存储引擎) 特点：支持事务和外键操作，支持并发控制。对比 MyISAM 的存储引擎，InnoDB 写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引 应用场景：对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，读写频繁的操作 存储方式： 使用共享表空间存储， 这种方式创建的表的表结构保存在 .frm 文件中， 数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path 定义的表空间中，可以是多个文件 使用多表空间存储，创建的表的表结构存在 .frm 文件中，每个表的数据和索引单独保存在 .ibd 中 逻辑存储结构： 表空间 : InnoDB存储引擎逻辑结构的最高层，ibd 文件其实就是表空间文件，在表空间中可以 包含多个Segment段。 段 : 表空间是由各个段组成的， 常见的段有数据段（Leaf node segment）、索引段（Non-leaf segment）、回滚段（Rollback segment）等。数据段就是 B+树的叶子节点，索引段就是 B+树的非叶子节点。 InnoDB中对于段的管理，都是引擎自身完成，不需要人为对其控制，一个段中包含多个区。 区 : 区是表空间的单元结构，每个区的大小为1M。 默认情况下， InnoDB存储引擎页大小为 16KB， 即一个区中一共有64个连续的页。 页 : 页是组成区的最小单元，页也是InnoDB 存储引擎磁盘管理的最小单元，每个页的大小默认为 16KB。为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。 行 : InnoDB 存储引擎是面向行的，也就是说数据是按行进行存放的，在每一行中除了定义表时所指定的字段以外，还包含两个隐藏字段 1. Trx_id：每次对某条记录进行改动时，都会把对应的事务 id 赋值给 trx_id 隐藏列 2. Roll_pointer：每次对某条记录进行改动时，都会把旧的版本写入 undo 日志中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息 MEMORY 特点：每个 MEMORY 表实际对应一个磁盘文件 ，该文件中只存储表的结构，表数据保存在内存中，且默认使用 HASH 索引，所以数据默认就是无序的，但是在需要快速定位记录可以提供更快的访问，服务一旦关闭，表中的数据就会丢失，存储不安全 应用场景：缓存型存储引擎，通常用于更新不太频繁的小表，用以快速得到访问结果 存储方式：表结构保存在 .frm 中 MERGE 特点： 是一组 MyISAM 表的组合，这些 MyISAM 表必须结构完全相同，通过将不同的表分布在多个磁盘上 MERGE 表本身并没有存储数据，对 MERGE 类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部的 MyISAM 表进行的 应用场景：将一系列等同的 MyISAM 表以逻辑方式组合在一起，并作为一个对象引用他们，适合做数据仓库 操作方式： 插入操作是通过INSERT_METHOD 子句定义插入的表，使用 FIRST 或 LAST 值使得插入操作被相应地作用在第一或者最后一个表上；不定义这个子句或者定义为 NO，表示不能对 MERGE 表执行插入操作 对 MERGE 表进行 DROP 操作，但是这个操作只是删除 MERGE 表的定义，对内部的表是没有任何影响的 123456789CREATE TABLE order_1()ENGINE = MyISAM DEFAULT CHARSET=utf8;CREATE TABLE order_2()ENGINE = MyISAM DEFAULT CHARSET=utf8;CREATE TABLE order_all( -- 结构与MyISAM表相同)ENGINE = MERGE UNION = (order_1,order_2) INSERT_METHOD=LAST DEFAULT CHARSET=utf8; 特性 MyISAM InnoDB MEMORY 存储限制 有（平台对文件系统大小的限制） 64TB 有（平台的内存限制） 事务安全 不支持 支持 不支持 锁机制 表锁 表锁/行锁 表锁 外键 不支持 支持 不支持 B+Tree 索引 支持 支持 支持 哈希索引 不支持 不支持 支持 全文索引 支持 支持 不支持 集群索引 不支持 支持 不支持 数据索引 不支持 支持 支持 数据缓存 不支持 支持 N/A 索引缓存 支持 支持 N/A 数据可压缩 支持 不支持 不支持 空间使用 低 高 N/A 内存使用 低 高 中等 批量插入速度 高 低 高 只读场景 MyISAM 比 InnoDB 更快： 底层存储结构有差别，MyISAM 是非聚簇索引，叶子节点保存的是数据的具体地址，不用回表查询 InnoDB 每次查询需要维护 MVCC 版本状态，保证并发状态下的读写冲突问题 引擎操作 查询数据库支持的存储引擎 12SHOW ENGINES;SHOW VARIABLES LIKE &#x27;%storage_engine%&#x27;; -- 查看Mysql数据库默认的存储引擎 查询某个数据库中所有数据表的存储引擎 1SHOW TABLE STATUS FROM 数据库名称; 查询某个数据库中某个数据表的存储引擎 1SHOW TABLE STATUS FROM 数据库名称 WHERE NAME = &#x27;数据表名称&#x27;; 创建数据表，指定存储引擎 1234CREATE TABLE 表名( 列名,数据类型, ...)ENGINE = 引擎名称; 修改数据表的存储引擎 1ALTER TABLE 表名 ENGINE = 引擎名称; 常考面试题 问： InnoDB引擎与MyISAM引擎的区别 答： InnoDB引擎, 支持事务, 而MyISAM不支持。 InnoDB引擎, 支持行锁和表锁, 而MyISAM仅支持表锁, 不支持行锁。 InnoDB引擎, 支持外键, 而MyISAM是不支持的。","tags":["MySQL"],"categories":["学习笔记"]},{"title":"MySQL--多表操作","path":"/2023/05/16/MySQL--多表操作/","content":"约束分类 约束介绍 约束：对表中的数据进行限定，保证数据的正确性、有效性、完整性。 约束的分类： 约束 说明 PRIMARY KEY 主键约束 PRIMARY KEY AUTO_INCREMENT 主键、自动增长 UNIQUE 唯一约束 NOT NULL 非空约束 FOREIGN KEY 外键约束 FOREIGN KEY ON UPDATE CASCADE 外键级联更新 FOREIGN KEY ON DELETE CASCADE 外键级联删除 主键约束 主键约束特点： 主键约束默认包含非空和唯一两个功能 一张表只能有一个主键 主键一般用于表中数据的唯一标识 建表时添加主键约束 12345CREATE TABLE 表名( 列名 数据类型 PRIMARY KEY, 列名 数据类型, ...); 删除主键约束 1ALTER TABLE 表名 DROP PRIMARY KEY; 建表后单独添加主键约束 1ALTER TABLE 表名 MODIFY 列名 数据类型 PRIMARY KEY; 例如 1234567891011-- 创建student表CREATE TABLE student( id INT PRIMARY KEY -- 给id添加主键约束);-- 添加数据INSERT INTO student VALUES (1),(2);-- 主键默认唯一，添加重复数据，会报错INSERT INTO student VALUES (2);-- 主键默认非空，不能添加null的数据INSERT INTO student VALUES (NULL); 主键自增 主键自增约束可以为空，并自动增长。删除某条数据不影响自增的下一个数值，依然按照前一个值自增。 建表时添加主键自增约束 12345CREATE TABLE 表名( 列名 数据类型 PRIMARY KEY AUTO_INCREMENT, 列名 数据类型, ...); 删除主键自增约束 1ALTER TABLE 表名 MODIFY 列名 数据类型; 建表后单独添加主键自增约束 1ALTER TABLE 表名 MODIFY 列名 数据类型 AUTO_INCREMENT; 例如 123456789-- 创建student2表CREATE TABLE student2( id INT PRIMARY KEY AUTO_INCREMENT -- 给id添加主键自增约束);-- 添加数据INSERT INTO student2 VALUES (1),(2);-- 添加null值，会自动增长INSERT INTO student2 VALUES (NULL),(NULL);-- 3，4 唯一约束 唯一约束：约束不能有重复的数据。 建表时添加唯一约束 12345CREATE TABLE 表名( 列名 数据类型 UNIQUE, 列名 数据类型, ...); 删除唯一约束 1ALTER TABLE 表名 DROP INDEX 列名; 建表后单独添加唯一约束 1ALTER TABLE 表名 MODIFY 列名 数据类型 UNIQUE; 非空约束 非空约束：该列的值不能为空。 建表时添加非空约束 12345CREATE TABLE 表名( 列名 数据类型 NOT NULL, 列名 数据类型, ...); 删除非空约束 1ALTER TABLE 表名 MODIFY 列名 数据类型; 建表后单独添加非空约束 1ALTER TABLE 表名 MODIFY 列名 数据类型 NOT NULL; 外键约束 外键约束：让表和表之间产生关系，从而保证数据的准确性。 建表时添加外键约束 12345CREATE TABLE 表名( 列名 数据类型 约束, ... CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名)); 删除外键约束 1ALTER TABLE 表名 DROP FOREIGN KEY 外键名; 建表后单独添加外键约束 1ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名); 例如 123456789101112131415161718192021222324-- 创建user用户表CREATE TABLE USER( id INT PRIMARY KEY AUTO_INCREMENT, -- id name VARCHAR(20) NOT NULL -- 姓名);-- 添加用户数据INSERT INTO USER VALUES (NULL,&#x27;张三&#x27;),(NULL,&#x27;李四&#x27;),(NULL,&#x27;王五&#x27;);-- 创建orderlist订单表CREATE TABLE orderlist( id INT PRIMARY KEY AUTO_INCREMENT, -- id number VARCHAR(20) NOT NULL, -- 订单编号 uid INT, -- 订单所属用户 CONSTRAINT ou_fk1 FOREIGN KEY (uid) REFERENCES USER(id) -- 添加外键约束);-- 添加订单数据INSERT INTO orderlist VALUES (NULL,&#x27;hm001&#x27;,1),(NULL,&#x27;hm002&#x27;,1),(NULL,&#x27;hm003&#x27;,2),(NULL,&#x27;hm004&#x27;,2),(NULL,&#x27;hm005&#x27;,3),(NULL,&#x27;hm006&#x27;,3);-- 添加一个订单，但是没有所属用户。无法添加INSERT INTO orderlist VALUES (NULL,&#x27;hm007&#x27;,8);-- 删除王五这个用户，但是订单表中王五还有很多个订单呢。无法删除DELETE FROM USER WHERE NAME=&#x27;王五&#x27;; 外键级联 级联操作：当把主表中的数据进行删除或更新时，从表中有关联的数据的相应操作，包括 RESTRICT、CASCADE、SET NULL 和 NO ACTION。 RESTRICT 和 NO ACTION相同， 是指限制在子表有关联记录的情况下， 父表不能更新 CASCADE 表示父表在更新或者删除时，更新或者删除子表对应的记录 SET NULL 则表示父表在更新或者删除的时候，子表的对应字段被SET NULL 级联操作： 添加级联更新 1ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名) ON UPDATE [CASCADE | RESTRICT | SET NULL]; 添加级联删除 1ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名) ON DELETE CASCADE; 同时添加级联更新和级联删除 1ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名) ON UPDATE CASCADE ON DELETE CASCADE; 多表设计 一对一 多表：有多张数据表，而表与表之间有一定的关联关系，通过外键约束实现，分为一对一、一对多、多对多三类。 举例：人和身份证。 实现原则：在任意一个表建立外键，去关联另外一个表的主键。 1234567891011121314151617-- 创建person表CREATE TABLE person( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id NAME VARCHAR(20) -- 姓名);-- 添加数据INSERT INTO person VALUES (NULL,&#x27;张三&#x27;),(NULL,&#x27;李四&#x27;);-- 创建card表CREATE TABLE card( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id number VARCHAR(20) UNIQUE NOT NULL,\t-- 身份证号 pid INT UNIQUE, -- 外键列 CONSTRAINT cp_fk1 FOREIGN KEY (pid) REFERENCES person(id));-- 添加数据INSERT INTO card VALUES (NULL,&#x27;12345&#x27;,1),(NULL,&#x27;56789&#x27;,2); 一对多 举例：用户和订单、商品分类和商品。 实现原则：在多的一方，建立外键约束，来关联一的一方主键。 1234567891011121314151617-- 创建user表CREATE TABLE USER( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id NAME VARCHAR(20) -- 姓名);-- 添加数据INSERT INTO USER VALUES (NULL,&#x27;张三&#x27;),(NULL,&#x27;李四&#x27;);-- 创建orderlist表CREATE TABLE orderlist( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id number VARCHAR(20), -- 订单编号 uid INT, -- 外键列 CONSTRAINT ou_fk1 FOREIGN KEY (uid) REFERENCES USER(id));-- 添加数据INSERT INTO orderlist VALUES (NULL,&#x27;hm001&#x27;,1),(NULL,&#x27;hm002&#x27;,1),(NULL,&#x27;hm003&#x27;,2),(NULL,&#x27;hm004&#x27;,2); 多对多 举例：学生和课程。一个学生可以选择多个课程，一个课程也可以被多个学生选择。 实现原则：借助第三张表中间表，中间表至少包含两个列，这两个列作为中间表的外键，分别关联两张表的主键。 1234567891011121314151617181920212223242526-- 创建student表CREATE TABLE student( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id NAME VARCHAR(20) -- 学生姓名);-- 添加数据INSERT INTO student VALUES (NULL,&#x27;张三&#x27;),(NULL,&#x27;李四&#x27;);-- 创建course表CREATE TABLE course( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id NAME VARCHAR(10) -- 课程名称);-- 添加数据INSERT INTO course VALUES (NULL,&#x27;语文&#x27;),(NULL,&#x27;数学&#x27;);-- 创建中间表CREATE TABLE stu_course( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 主键id sid INT, -- 用于和student表中的id进行外键关联 cid INT, -- 用于和course表中的id进行外键关联 CONSTRAINT sc_fk1 FOREIGN KEY (sid) REFERENCES student(id), -- 添加外键约束 CONSTRAINT sc_fk2 FOREIGN KEY (cid) REFERENCES course(id) -- 添加外键约束);-- 添加数据INSERT INTO stu_course VALUES (NULL,1,1),(NULL,1,2),(NULL,2,1),(NULL,2,2); 连接查询 内外连接 内连接 连接查询的是两张表有交集的部分数据，两张表分为驱动表和被驱动表，如果结果集中的每条记录都是两个表相互匹配的组合，则称这样的结果集为笛卡尔积。 内连接查询，若驱动表中的记录在被驱动表中找不到匹配的记录时，则该记录不会加到最后的结果集。 显式内连接： 1SELECT 列名 FROM 表名1 [INNER] JOIN 表名2 ON 条件; 隐式内连接：内连接中 WHERE 子句和 ON 子句是等价的 1SELECT 列名 FROM 表名1,表名2 WHERE 条件; STRAIGHT_JOIN与 JOIN 类似，只不过左表始终在右表之前读取，只适用于内连接。 外连接 外连接查询，若驱动表中的记录在被驱动表中找不到匹配的记录时，则该记录也会加到最后的结果集，只是对于被驱动表中不匹配过滤条件的记录，各个字段使用 NULL 填充。 应用实例：查学生成绩，也想展示出缺考的人的成绩。 左外连接：选择左侧的表为驱动表，查询左表的全部数据，和左右两张表有交集部分的数据 1SELECT 列名 FROM 表名1 LEFT [OUTER] JOIN 表名2 ON 条件; 右外连接：选择右侧的表为驱动表，查询右表的全部数据，和左右两张表有交集部分的数据 1SELECT 列名 FROM 表名1 RIGHT [OUTER] JOIN 表名2 ON 条件; 关联查询 自关联查询：同一张表中有数据关联，可以多次查询这同一个表。 数据准备 123456789-- 创建员工表CREATE TABLE employee( id INT PRIMARY KEY AUTO_INCREMENT,\t-- 员工编号 NAME VARCHAR(20), -- 员工姓名 mgr INT, -- 上级编号 salary DOUBLE -- 员工工资);-- 添加数据INSERT INTO employee VALUES (1001,&#x27;孙悟空&#x27;,1005,9000.00),..,(1009,&#x27;宋江&#x27;,NULL,16000.00); 数据查询 12345678910111213141516171819-- 查询所有员工的姓名及其直接上级的姓名，没有上级的员工也需要查询/*分析\t员工信息 employee表\t条件：employee.mgr = employee.id\t查询左表的全部数据，和左右两张表有交集部分数据，左外连接*/SELECT\te1.id,\te1.name,\te1.mgr,\te2.id,\te2.nameFROM\temployee e1LEFT OUTER JOIN\temployee e2ON\te1.mgr = e2.id; 查询结果 12345678910id name mgr id name1001\t孙悟空 1005\t1005\t唐僧1002\t猪八戒 1005\t1005\t唐僧1003\t沙和尚 1005\t1005\t唐僧1004\t小白龙 1005\t1005\t唐僧1005\t唐僧 NULL NULL NULL1006\t武松 1009 1009 宋江1007\t李逵 1009 1009 宋江1008\t林冲 1009 1009 宋江1009\t宋江 NULL NULL NULL 连接原理 Index Nested-Loop Join 算法：查询驱动表得到数据集，然后根据数据集中的每一条记录的关联字段再分别到被驱动表中查找匹配（走索引），所以驱动表只需要访问一次，被驱动表要访问多次。 MySQL 将查询驱动表后得到的记录成为驱动表的扇出，连接查询的成本：单次访问驱动表的成本 + 扇出值 * 单次访问被驱动表的成本，优化器会选择成本最小的表连接顺序（确定谁是驱动表，谁是被驱动表）生成执行计划，进行连接查询，优化方式： 减少驱动表的扇出（让数据量小的表来做驱动表） 降低访问被驱动表的成本 说明：STRAIGHT_JOIN 是查一条驱动表，然后根据关联字段去查被驱动表，要访问多次驱动表，所以需要优化为 INL 算法。 Block Nested-Loop Join 算法：一种空间换时间的优化方式，基于块的循环连接，执行连接查询前申请一块固定大小的内存作为连接缓冲区 Join Buffer，先把若干条驱动表中的扇出暂存在缓冲区，每一条被驱动表中的记录一次性的与 Buffer 中多条记录进行匹配（扫描全部数据，一条一条的匹配），因为是在内存中完成，所以速度快，并且降低了 I/O 成本。 Join Buffer 可以通过参数 join_buffer_size 进行配置，默认大小是 256 KB。 在成本分析时，对于很多张表的连接查询，连接顺序有非常多，MySQL 如果挨着进行遍历计算成本，会消耗很多资源。 提前结束某种连接顺序的成本评估：维护一个全局变量记录当前成本最小的连接方式，如果一种顺序只计算了一部分就已经超过了最小成本，可以提前结束计算 系统变量 optimizer_search_depth：如果连接表的个数小于该变量，就继续穷举分析每一种连接数量，反之只对数量与 depth 值相同的表进行分析，该值越大成本分析的越精确 系统变量 optimizer_prune_level：控制启发式规则的启用，这些规则就是根据以往经验指定的，不满足规则的连接顺序不分析成本 连接优化 BKA Batched Key Access 算法是对 NLJ 算法的优化，在读取被驱动表的记录时使用顺序 IO，Extra 信息中会有 Batched Key Access 信息。 使用 BKA 的表的 JOIN 过程如下： 连接驱动表将满足条件的记录放入 Join Buffer，并将两表连接的字段放入一个 DYNAMIC_ARRAY ranges 中 在进行表的过接过程中，会将 ranges 相关的信息传入 Buffer 中，进行被驱动表主建的查找及排序操作 调用步骤 2 中产生的有序主建，顺序读取被驱动表的数据 当缓冲区的数据被读完后，会重复进行步骤 2、3，直到记录被读取完 使用 BKA 优化需要设进行设置： 1SET optimizer_switch=&#x27;mrr=on,mrr_cost_based=off,batched_key_access=on&#x27;; 说明：前两个参数的作用是启用 MRR，因为 BKA 算法的优化要依赖于 MRR（系统优化 → 内存优化 → Read 详解） BNL 问题 BNL 即 Block Nested-Loop Join 算法，由于要访问多次被驱动表，会产生两个问题： Join 语句多次扫描一个冷表，并且语句执行时间小于 1 秒，就会在再次扫描冷表时，把冷表的数据页移到 LRU 链表头部，导致热数据被淘汰，影响业务的正常运行这种情况冷表的数据量要小于整个 Buffer Pool 的 old 区域，能够完全放入 old 区，才会再次被读时加到 young，否则读取下一段时就已经把上一段淘汰 Join 语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页很可能在 1 秒之内就被淘汰，就会导致 MySQL 实例的 Buffer Pool 在这段时间内 young 区域的数据页没有被合理地淘汰 大表 Join 操作虽然对 IO 有影响，但是在语句执行结束后对 IO 的影响随之结束。但是对 Buffer Pool 的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。 优化 将 BNL 算法转成 BKA 算法，优化方向： 在被驱动表上建索引，这样就可以根据索引进行顺序 IO 使用临时表，在临时表上建立索引，将被驱动表和临时表进行连接查询 驱动表 t1，被驱动表 t2，使用临时表的工作流程： 把表 t1 中满足条件的数据放在临时表 tmp_t 中 给临时表 tmp_t 的关联字段加上索引，使用 BKA 算法 让表 t2 和 tmp_t 做 Join 操作（临时表是被驱动表） 补充：MySQL 8.0 支持 hash join，join_buffer 维护的不再是一个无序数组，而是一个哈希表，查询效率更高，执行效率比临时表更高。 嵌套（子）查询 查询分类 查询语句中嵌套了查询语句，将嵌套查询称为子查询，FROM 子句后面的子查询的结果集称为派生表。 根据结果分类： 结果是单行单列：可以将查询的结果作为另一条语句的查询条件，使用运算符判断 1SELECT 列名 FROM 表名 WHERE 列名=(SELECT 列名/聚合函数(列名) FROM 表名 [WHERE 条件]); 结果是多行单列：可以作为条件，使用运算符 IN 或 NOT IN 进行判断 1SELECT 列名 FROM 表名 WHERE 列名 [NOT] IN (SELECT 列名 FROM 表名 [WHERE 条件]); 结果是多行多列：查询的结果可以作为一张虚拟表参与查询 12345678910SELECT 列名 FROM 表名 [别名],(SELECT 列名 FROM 表名 [WHERE 条件]) [别名] [WHERE 条件];-- 查询订单表orderlist中id大于4的订单信息和所属用户USER信息SELECT * FROM USER u,\t(SELECT * FROM orderlist WHERE id&gt;4) o WHERE u.id=o.uid; 相关性分类： 不相关子查询：子查询不依赖外层查询的值，可以单独运行出结果 相关子查询：子查询的执行需要依赖外层查询的值 查询优化 不相关子查询的结果集会被写入一个临时表，并且在写入时去重，该过程称为物化，存储结果集的临时表称为物化表。 系统变量 tmp_table_size 或者 max_heap_table_size 为表的最值。 小于系统变量时，内存中可以保存，会为建立基于内存的 MEMORY 存储引擎的临时表，并建立哈希索引 大于任意一个系统变量时，物化表会使用基于磁盘的 InnoDB 存储引擎来保存结果集中的记录，索引类型为 B+ 树 物化后，嵌套查询就相当于外层查询的表和物化表进行内连接查询，然后经过优化器选择成本最小的表连接顺序执行查询。 子查询物化会产生建立临时表的成本，但是将子查询转化为连接查询可以充分发挥优化器的作用，所以引入：半连接。 t1 和 t2 表进行半连接，对于 t1 表中的某条记录，只需要关心在 t2 表中是否存在，而不需要关心有多少条记录与之匹配，最终结果集只保留 t1 的记录 半连接只是执行子查询的一种方式，MySQL 并没有提供面向用户的半连接语法 联合查询 UNION 是取这两个子查询结果的并集，并进行去重，同时进行默认规则的排序（union 是行加起来，join 是列加起来）。 UNION ALL 是对两个结果集进行并集操作不进行去重，不进行排序。 1(select 1000 as f) union (select id from t1 order by id desc limit 2); #t1表中包含id 为 1-1000 的数据 语句的执行流程： 创建一个内存临时表，这个临时表只有一个整型字段 f，并且 f 是主键字段 执行第一个子查询，得到 1000 这个值，并存入临时表中 执行第二个子查询，拿到第一行 id=1000，试图插入临时表中，但由于 1000 这个值已经存在于临时表了，违反了唯一性约束，所以插入失败，然后继续执行 取到第二行 id=999，插入临时表成功 从临时表中按行取出数据，返回结果并删除临时表，结果中包含两行数据分别是 1000 和 999","tags":["MySQL"],"categories":["学习笔记"]},{"title":"MySQL--单表操作","path":"/2023/05/15/MySQL--单表操作/","content":"SQL SQL Structured Query Language：结构化查询语言 定义了操作所有关系型数据库的规则，每种数据库操作的方式可能会存在不一样的地方，称为“方言” SQL 通用语法 SQL 语句可以单行或多行书写，以分号结尾。 可使用空格和缩进来增强语句的可读性。 MySQL 数据库的 SQL 语句不区分大小写，关键字建议使用大写。 数据库的注释： 单行注释：-- 注释内容 #注释内容（MySQL 特有） 多行注释：/* 注释内容 */ SQL 分类 DDL（Data Definition Language）数据定义语言 用来定义数据库对象：数据库，表，列等。关键字：create、drop,、alter 等 DML（Data Manipulation Language）数据操作语言 用来对数据库中表的数据进行增删改。关键字：insert、delete、update 等 DQL（Data Query Language）数据查询语言 用来查询数据库中表的记录(数据)。关键字：select、where 等 DCL（Data Control Language）数据控制语言 用来定义数据库的访问权限和安全级别，及创建用户。关键字：grant， revoke等 DDL 数据库 R(Retrieve)：查询 查询所有数据库： 1SHOW DATABASES; 查询某个数据库的创建语句 123SHOW CREATE DATABASE 数据库名称; -- 标准语法SHOW CREATE DATABASE mysql; -- 查看mysql数据库的创建格式 C(Create)：创建 创建数据库 1CREATE DATABASE 数据库名称;-- 标准语法 创建数据库（判断，如果不存在则创建） 1CREATE DATABASE IF NOT EXISTS 数据库名称; 创建数据库，并指定字符集 1CREATE DATABASE 数据库名称 CHARACTER SET 字符集名称; 例如：创建db4数据库、如果不存在则创建，指定字符集为gbk 12345-- 创建db4数据库、如果不存在则创建，指定字符集为gbkCREATE DATABASE IF NOT EXISTS db4 CHARACTER SET gbk;-- 查看db4数据库的字符集SHOW CREATE DATABASE db4; U(Update)：修改 修改数据库的字符集 1ALTER DATABASE 数据库名称 CHARACTER SET 字符集名称; 常用字符集： 1234567--查询所有支持的字符集SHOW CHARSET;--查看所有支持的校对规则SHOW COLLATION;-- 字符集: utf8,latinI,GBK,,GBK是utf8的子集-- 校对规则: ci 大小定不敏感，cs或bin大小写敏感 D(Delete)：删除 删除数据库： 1DROP DATABASE 数据库名称; 删除数据库(判断，如果存在则删除)： 1DROP DATABASE IF EXISTS 数据库名称; 使用数据库： 查询当前正在使用的数据库名称 1SELECT DATABASE(); 使用数据库 1USE 数据库名称； -- 标准语法 数据表 R(Retrieve)：查询 查询数据库中所有的数据表 1SHOW TABLES;-- 查询库中所有的表 查询表结构 1DESC 表名; 查询表字符集 1SHOW TABLE STATUS FROM 库名 LIKE &#x27;表名&#x27;; C(Create)：创建 创建数据表 1234567CREATE TABLE 表名( 列名1 数据类型1, 列名2 数据类型2, .... 列名n 数据类型n);-- 注意：最后一列，不需要加逗号 复制表 123CREATE TABLE 表名 LIKE 被复制的表名; -- 标准语法CREATE TABLE product2 LIKE product; -- 复制product表到product2表 数据类型 数据类型 说明 INT 整数类型 DOUBLE 小数类型 DATE 日期，只包含年月日：yyyy-MM-dd DATETIME 日期，包含年月日时分秒：yyyy-MM-dd HH:mm:ss TIMESTAMP 时间戳类型，包含年月日时分秒：yyyy-MM-dd HH:mm:ss 如果不给这个字段赋值或赋值为 NULL，则默认使用当前的系统时间 CHAR 字符串，定长类型 VARCHAR 字符串，变长类型 name varchar(20) 代表姓名最大 20 个字符：zhangsan 8 个字符，张三 2 个字符 INT(n)：n 代表位数 3：int（9）显示结果为 000000010 3：int（3）显示结果为 010 varchar(n)：n 表示的是字符数 例如： 1234567891011-- 使用db3数据库USE db3;-- 创建一个product商品表CREATE TABLE product( id INT, -- 商品编号 NAME VARCHAR(30),\t-- 商品名称 price DOUBLE, -- 商品价格 stock INT, -- 商品库存 insert_time DATE -- 上架时间); U(Update)：修改 修改表名 1ALTER TABLE 表名 RENAME TO 新的表名; 修改表的字符集 1ALTER TABLE 表名 CHARACTER SET 字符集名称; 添加一列 1ALTER TABLE 表名 ADD 列名 数据类型; 修改列数据类型 1ALTER TABLE 表名 MODIFY 列名 新数据类型; 修改列名称和数据类型 1ALTER TABLE 表名 CHANGE 列名 新列名 新数据类型; 删除列 1ALTER TABLE 表名 DROP 列名; D(Delete)：删除 删除数据表 1DROP TABLE 表名; 删除数据表(判断，如果存在则删除) 1DROP TABLE IF EXISTS 表名; DML INSERT 新增表数据 新增格式 1：给指定列添加数据 1INSERT INTO 表名(列名1,列名2...) VALUES (值1,值2...); 新增格式 2：默认给全部列添加数据 1INSERT INTO 表名 VALUES (值1,值2,值3,...); 新增格式 3：批量添加数据 12345-- 给指定列批量添加数据INSERT INTO 表名(列名1,列名2,...) VALUES (值1,值2,...),(值1,值2,...)...;-- 默认给所有列批量添加数据 INSERT INTO 表名 VALUES (值1,值2,值3,...),(值1,值2,值3,...)...; 字符串拼接 1CONCAT(string1,string2,&#x27;&#x27;,...) 注意事项 列名和值的数量以及数据类型要对应 除了数字类型，其他数据类型的数据都需要加引号(单引双引都可以，推荐单引) UPDATE 修改表数据语法 标准语法 1UPDATE 表名 SET 列名1 = 值1,列名2 = 值2,... [where 条件]; 修改电视的价格为1800、库存为36 12UPDATE product SET price=1800,stock=36 WHERE NAME=&#x27;电视&#x27;;SELECT * FROM product;-- 查看所有商品信息 注意事项 修改语句中必须加条件 如果不加条件，则将所有数据都修改 DELETE 删除表数据语法 1DELETE FROM 表名 [WHERE 条件]; 注意事项 删除语句中必须加条件 如果不加条件，则将所有数据删除 DQL 查询语法 数据库查询遵循条件在前的原则。 12345678910111213141516SELECT DISTINCT\t&lt;select list&gt;FROM\t&lt;left_table&gt; &lt;join_type&gt;JOIN\t&lt;right_table&gt; ON &lt;join_condition&gt;\t-- 连接查询在多表查询部分详解WHERE\t&lt;where_condition&gt;GROUP BY\t&lt;group_by_list&gt;HAVING\t&lt;having_condition&gt;ORDER BY\t&lt;order_by_condition&gt;LIMIT\t&lt;limit_params&gt; 执行顺序 1234567891011121314151617FROM\t&lt;left_table&gt;ON &lt;join_condition&gt;&lt;join_type&gt; JOIN\t&lt;right_table&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;HAVING &lt;having_condition&gt;SELECT DISTINCT &lt;select list&gt;ORDER BY\t&lt;order_by_condition&gt;LIMIT &lt;limit_params&gt; 查询全部 查询全部的表数据 12345-- 标准语法SELECT * FROM 表名;-- 查询product表所有数据(常用)SELECT * FROM product; 查询指定字段的表数据 1SELECT 列名1,列名2,... FROM 表名; 去除重复查询：只有值全部重复的才可以去除，需要创建临时表辅助查询 1SELECT DISTINCT 列名1,列名2,... FROM 表名; 计算列的值（四则运算） 123456SELECT 列名1 运算符(+ - * /) 列名2 FROM 表名;/*如果某一列值为null，可以进行替换\tifnull(表达式1,表达式2)\t表达式1：想替换的列\t表达式2：想替换的值*/ 例如： 12345-- 查询商品名称和库存，库存数量在原有基础上加10SELECT NAME,stock+10 FROM product;-- 查询商品名称和库存，库存数量在原有基础上加10。进行null值判断SELECT NAME,IFNULL(stock,0)+10 FROM product; 起别名 1SELECT 列名1,列名2,... AS 别名 FROM 表名; 例如： 123-- 查询商品名称和库存，库存数量在原有基础上加10。进行null值判断，起别名为getSum,AS可以省略。SELECT NAME,IFNULL(stock,0)+10 AS getsum FROM product;SELECT NAME,IFNULL(stock,0)+10 getsum FROM product; 函数查询 聚合函数 聚合函数：将一列数据作为一个整体，进行纵向的计算。 聚合函数语法 1SELECT 函数名(列名) FROM 表名 [WHERE 条件] 聚合函数分类 函数名 功能 COUNT(列名) 统计数量（一般选用不为 null 的列） MAX(列名) 最大值 MIN(列名) 最小值 SUM(列名) 求和 AVG(列名) 平均值（会忽略 null 行） 例如 1234567891011121314151617-- 计算product表中总记录条数 7SELECT COUNT(*) FROM product;-- 获取最高价格SELECT MAX(price) FROM product;-- 获取最高价格的商品名称SELECT NAME,price FROM product WHERE price = (SELECT MAX(price) FROM product);-- 获取最低库存SELECT MIN(stock) FROM product;-- 获取最低库存的商品名称SELECT NAME,stock FROM product WHERE stock = (SELECT MIN(stock) FROM product);-- 获取总库存数量SELECT SUM(stock) FROM product;-- 获取品牌为小米的平均商品价格SELECT AVG(price) FROM product WHERE brand=&#x27;小米&#x27;; 文本函数 CONCAT()：用于连接两个字段。 12SELECT CONCAT(TRIM(col1), &#x27;(&#x27;, TRIM(col2), &#x27;)&#x27;) AS concat_col FROM mytable-- 许多数据库会使用空格把一个值填充为列宽，连接的结果出现一些不必要的空格，使用TRIM()可以去除首尾空格 函数名称 作 用 LENGTH 计算字符串长度函数，返回字符串的字节长度 CONCAT 合并字符串函数，返回结果为连接参数产生的字符串，参数可以使一个或多个 INSERT 替换字符串函数 LOWER 将字符串中的字母转换为小写 UPPER 将字符串中的字母转换为大写 LEFT 从左侧字截取符串，返回字符串左边的若干个字符 RIGHT 从右侧字截取符串，返回字符串右边的若干个字符 TRIM 删除字符串左右两侧的空格 REPLACE 字符串替换函数，返回替换后的新字符串 SUBSTRING 截取字符串，返回从指定位置开始的指定长度的字符换 REVERSE 字符串反转（逆序）函数，返回与原始字符串顺序相反的字符串 数字函数 函数名称 作 用 ABS 求绝对值 SQRT 求二次方根 MOD 求余数 CEIL 和 CEILING 两个函数功能相同，都是返回不小于参数的最小整数，即向上取整 FLOOR 向下取整，返回值转化为一个BIGINT RAND 生成一个0~1之间的随机数，传入整数参数是，用来产生重复序列 ROUND 对所传参数进行四舍五入 SIGN 返回参数的符号 POW 和 POWER 两个函数的功能相同，都是所传参数的次方的结果值 SIN 求正弦值 ASIN 求反正弦值，与函数 SIN 互为反函数 COS 求余弦值 ACOS 求反余弦值，与函数 COS 互为反函数 TAN 求正切值 ATAN 求反正切值，与函数 TAN 互为反函数 COT 求余切值 日期函数 函数名称 作 用 CURDATE 和 CURRENT_DATE 两个函数作用相同，返回当前系统的日期值 CURTIME 和 CURRENT_TIME 两个函数作用相同，返回当前系统的时间值 NOW 和 SYSDATE 两个函数作用相同，返回当前系统的日期和时间值 MONTH 获取指定日期中的月份 MONTHNAME 获取指定日期中的月份英文名称 DAYNAME 获取指定曰期对应的星期几的英文名称 DAYOFWEEK 获取指定日期对应的一周的索引位置值 WEEK 获取指定日期是一年中的第几周，返回值的范围是否为 0〜52 或 1〜53 DAYOFYEAR 获取指定曰期是一年中的第几天，返回值范围是1~366 DAYOFMONTH 获取指定日期是一个月中是第几天，返回值范围是1~31 YEAR 获取年份，返回值范围是 1970〜2069 TIME_TO_SEC 将时间参数转换为秒数 SEC_TO_TIME 将秒数转换为时间，与TIME_TO_SEC 互为反函数 DATE_ADD 和 ADDDATE 两个函数功能相同，都是向日期添加指定的时间间隔 DATE_SUB 和 SUBDATE 两个函数功能相同，都是向日期减去指定的时间间隔 ADDTIME 时间加法运算，在原始时间上添加指定的时间 SUBTIME 时间减法运算，在原始时间上减去指定的时间 DATEDIFF 获取两个日期之间间隔，返回参数 1 减去参数 2 的值 DATE_FORMAT 格式化指定的日期，根据参数返回指定格式的值 WEEKDAY 获取指定日期在一周内的对应的工作日索引 条件查询 WHERE 条件查询语法 1SELECT 列名 FROM 表名 WHERE 条件; 条件分类 符号 功能 &gt; 大于 &lt; 小于 &gt;= 大于等于 &lt;= 小于等于 = 等于 &lt;&gt; 或 != 不等于 BETWEEN … AND … 在某个范围之内(都包含) IN(…) 多选一 LIKE 模糊查询：_单个任意字符、%任意个字符、[] 匹配集合内的字符 LIKE ‘[^AB]%’ ：不以 A 和 B 开头的任意文本 IS NULL 是NULL IS NOT NULL 不是NULL AND 或 &amp;&amp; 并且 OR 或 || 或者 NOT 或 ! 非，不是 UNION 对两个结果集进行并集操作并进行去重，同时进行默认规则的排序 UNION ALL 对两个结果集进行并集操作不进行去重，不进行排序 例如： 123456789101112131415161718192021222324252627282930-- 查询库存大于20的商品信息SELECT * FROM product WHERE stock &gt; 20;-- 查询品牌为华为的商品信息SELECT * FROM product WHERE brand=&#x27;华为&#x27;;-- 查询金额在4000 ~ 6000之间的商品信息SELECT * FROM product WHERE price &gt;= 4000 AND price &lt;= 6000;SELECT * FROM product WHERE price BETWEEN 4000 AND 6000;-- 查询库存为14、30、23的商品信息SELECT * FROM product WHERE stock=14 OR stock=30 OR stock=23;SELECT * FROM product WHERE stock IN(14,30,23);-- 查询库存为null的商品信息SELECT * FROM product WHERE stock IS NULL;-- 查询库存不为null的商品信息SELECT * FROM product WHERE stock IS NOT NULL;-- 查询名称以&#x27;小米&#x27;为开头的商品信息SELECT * FROM product WHERE NAME LIKE &#x27;小米%&#x27;;-- 查询名称第二个字是&#x27;为&#x27;的商品信息SELECT * FROM product WHERE NAME LIKE &#x27;_为%&#x27;;-- 查询名称为四个字符的商品信息 4个下划线SELECT * FROM product WHERE NAME LIKE &#x27;____&#x27;;-- 查询名称中包含电脑的商品信息SELECT * FROM product WHERE NAME LIKE &#x27;%电脑%&#x27;; 正则查询 REGEXP 正则表达式（Regular Expression）是指一个用来描述或者匹配一系列符合某个句法规则的字符串的单个字符串。 123SELECT * FROM emp WHERE name REGEXP &#x27;^T&#x27;;\t-- 匹配以T开头的name值SELECT * FROM emp WHERE name REGEXP &#x27;2$&#x27;;\t-- 匹配以2结尾的name值SELECT * FROM emp WHERE name REGEXP &#x27;[uvw]&#x27;;-- 匹配包含 uvw 的name值 符号 含义 ^ 在字符串开始处进行匹配 $ 在字符串末尾处进行匹配 . 匹配任意单个字符, 包括换行符 […] 匹配出括号内的任意字符 [^…] 匹配不出括号内的任意字符 a* 匹配零个或者多个a(包括空串) a+ 匹配一个或者多个a(不包括空串) a? 匹配零个或者一个a a1|a2 匹配a1或a2 a(m) 匹配m个a a(m,) 至少匹配m个a a(m,n) 匹配m个a 到 n个a a(,n) 匹配0到n个a (…) 将模式元素组成单一元素 排序查询 ORDER BY 排序查询语法 1SELECT 列名 FROM 表名 [WHERE 条件] ORDER BY 列名1 排序方式1,列名2 排序方式2; 排序方式 12ASC:升序DESC:降序 注意：多个排序条件，当前边的条件值一样时，才会判断第二条件。 例如 12345678-- 按照库存升序排序SELECT * FROM product ORDER BY stock ASC;-- 查询名称中包含手机的商品信息。按照金额降序排序SELECT * FROM product WHERE NAME LIKE &#x27;%手机%&#x27; ORDER BY price DESC;-- 按照金额升序排序，如果金额相同，按照库存降序排列SELECT * FROM product ORDER BY price ASC,stock DESC; 分组查询 GROUP BY 分组查询会进行去重 分组查询语法 1SELECT 列名 FROM 表名 [WHERE 条件] GROUP BY 分组列名 [HAVING 分组后条件过滤] [ORDER BY 排序列名 排序方式]; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。 分组规定： GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前 NULL 的行会单独分为一组 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型 例如 1234567891011-- 按照品牌分组，获取每组商品的总金额SELECT brand,SUM(price) FROM product GROUP BY brand;-- 对金额大于4000元的商品，按照品牌分组,获取每组商品的总金额SELECT brand,SUM(price) FROM product WHERE price &gt; 4000 GROUP BY brand;-- 对金额大于4000元的商品，按照品牌分组，获取每组商品的总金额，只显示总金额大于7000元的SELECT brand,SUM(price) AS getSum FROM product WHERE price &gt; 4000 GROUP BY brand HAVING getSum &gt; 7000;-- 对金额大于4000元的商品，按照品牌分组，获取每组商品的总金额，只显示总金额大于7000元的、并按照总金额的降序排列SELECT brand,SUM(price) AS getSum FROM product WHERE price &gt; 4000 GROUP BY brand HAVING getSum &gt; 7000 ORDER BY getSum DESC; 分页查询 LIMIT 分页查询语法 1SELECT 列名 FROM 表名 [WHERE 条件] GROUP BY 分组列名 [HAVING 分组后条件过滤] [ORDER BY 排序列名 排序方式] LIMIT 开始索引,查询条数; 公式：开始索引 = (当前页码-1) * 每页显示的条数 例如 1234SELECT * FROM product LIMIT 0,2; -- 第一页 开始索引=(1-1) * 2SELECT * FROM product LIMIT 2,2; -- 第二页 开始索引=(2-1) * 2SELECT * FROM product LIMIT 4,2; -- 第三页 开始索引=(3-1) * 2SELECT * FROM product LIMIT 6,2; -- 第四页 开始索引=(4-1) * 2 WHERE 和 HAVING 的区别 执行时机不同：WHERE 时分组之前进行条件限定，不满足WHERE条件则不参与分组，而HAVING是分组后对结果进行过滤。 可判断的条件不同：WHERE 不能对聚合函数进行判断，HAVING 可以。 执行顺序：WHERE &gt; 聚合函数 &gt; HAVING","tags":["MySQL"],"categories":["学习笔记"]},{"title":"MySQL--入门","path":"/2023/05/11/MySQL--入门/","content":"简介 数据库 数据库：DataBase，简称 DB，存储和管理数据的仓库。 数据库的优势： 可以持久化存储数据 方便存储和管理数据 使用了统一的方式操作数据库 SQL 数据库、数据表、数据的关系介绍： 数据库 用于存储和管理数据的仓库 一个库中可以包含多个数据表 数据表 数据库最重要的组成部分之一 由纵向的列和横向的行组成（类似 excel 表格） 可以指定列名、数据类型、约束等 一个表中可以存储多条数据 数据：想要永久化存储的数据 MySQL MySQL 数据库是一个最流行的关系型数据库管理系统之一，关系型数据库是将数据保存在不同的数据表中，而且表与表之间可以有关联关系，提高了灵活性。MySQL 所使用的 SQL 语句是用于访问数据库最常用的标准化语言。 缺点：数据存储在磁盘中，导致读写性能差，而且数据关系复杂，扩展性差。 MySQL 配置： 修改 MySQL 默认字符集：安装 MySQL 之后第一件事就是修改字符集编码 123456789vim /etc/mysql/my.cnf添加如下内容：[mysqld]character-set-server=utf8collation-server=utf8_general_ci[client]default-character-set=utf8 登录 MySQL： 123mysql -u root -p 敲回车，输入密码初始密码查看：cat /var/log/mysqld.log在root@localhost: 后面的就是初始密码 启动 MySQL 服务： 1systemctl start/restart mysql 查看默认字符集命令： 1SHOW VARIABLES LIKE &#x27;char%&#x27;; 修改MySQL登录密码： 1234set global validate_password_policy=0;set global validate_password_length=1; set password=password(&#x27;密码&#x27;); 授予远程连接权限（MySQL 内输入）： 1234-- 授权grant all privileges on *.* to &#x27;root&#x27; @&#x27;%&#x27; identified by &#x27;密码&#x27;;-- 刷新flush privileges; 修改 MySQL 绑定 IP： 1234cd /etc/mysql/mysql.conf.dsudo chmod 666 mysqld.cnf vim mysqld.cnf # bind-address = 127.0.0.1注释该行 关闭 Linux 防火墙 12systemctl stop firewalld.service# 放行3306端口 体系架构 MySQL 整体架构 体系结构详解： 第一层：网络连接层 一些客户端和链接服务，包含本地 Socket 通信和大多数基于客户端/服务端工具实现的 TCP/IP 通信，主要完成一些类似于连接处理、授权认证、及相关的安全方案 在该层上引入了连接池 Connection Pool 的概念，管理缓冲用户连接，线程处理等需要缓存的需求 在该层上实现基于 SSL 的安全链接，服务器也会为安全接入的每个客户端验证它所具有的操作权限 第二层：核心服务层 查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，所有的内置函数（日期、数学、加密函数等） Management Serveices &amp; Utilities：系统管理和控制工具，备份、安全、复制、集群等 SQL Interface：接受用户的 SQL 命令，并且返回用户需要查询的结果 Parser：SQL 语句分析器 Optimizer：查询优化器 Caches &amp; Buffers：查询缓存，服务器会查询内部的缓存，如果缓存空间足够大，可以在大量读操作的环境中提升系统性能 所有跨存储引擎的功能在这一层实现，如存储过程、触发器、视图等 在该层服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询顺序，是否利用索引等， 最后生成相应的执行操作 MySQL 中服务器层不管理事务，事务是由存储引擎实现的 第三层：存储引擎层 Pluggable Storage Engines：存储引擎接口，MySQL 区别于其他数据库的重要特点就是其存储引擎的架构模式是插件式的（存储引擎是基于表的，而不是数据库） 存储引擎真正的负责了 MySQL 中数据的存储和提取，服务器通过 API 和存储引擎进行通信 不同的存储引擎具有不同的功能，共用一个 Server 层，可以根据开发的需要，来选取合适的存储引擎 第四层：系统文件层 数据存储层，主要是将数据存储在文件系统之上，并完成与存储引擎的交互 File System：文件系统，保存配置文件、数据文件、日志文件、错误文件、二进制文件等 SQL 执行流程 整体的执行流程： 建立连接 连接器 池化技术：对于访问数据库来说，建立连接的代价是比较昂贵的，因为每个连接对应一个用来交互的线程，频繁的创建关闭连接比较耗费资源，有必要建立数据库连接池，以提高访问的性能 连接建立 TCP 以后需要做权限验证，验证成功后可以进行执行 SQL。如果这时管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限，只有再新建的连接才会使用新的权限设置。 MySQL 服务器可以同时和多个客户端进行交互，所以要保证每个连接会话的隔离性。 权限信息 grant 语句会同时修改数据表和内存，判断权限的时候使用的是内存数据。 flush privileges 语句本身会用数据表（磁盘）的数据重建一份内存权限数据，所以在权限数据可能存在不一致的情况下使用，这种不一致往往是由于直接用 DML 语句操作系统权限表导致的，所以尽量不要使用这类语句。 连接状态 客户端如果长时间没有操作，连接器就会自动断开，时间是由参数 wait_timeout 控制的，默认值是 8 小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒：Lost connection to MySQL server during query 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接；短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 为了减少连接的创建，推荐使用长连接，但是过多的长连接会造成 OOM，解决方案： 定期断开长连接，使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连 1KILL CONNECTION id MySQL 5.7 版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源，这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态 SHOW PROCESSLIST：查看当前 MySQL 在进行的线程，可以实时地查看 SQL 的执行情况，其中的 Command 列显示为 Sleep 的这一行，就表示现在系统里面有一个空闲连接 参数 含义 ID 用户登录 mysql 时系统分配的 connection_id，可以使用函数 connection_id() 查看 User 显示当前用户，如果不是 root，这个命令就只显示用户权限范围的 sql 语句 Host 显示这个语句是从哪个 ip 的哪个端口上发的，可以用来跟踪出现问题语句的用户 db 显示这个进程目前连接的是哪个数据库 Command 显示当前连接的执行的命令，一般取值为休眠 Sleep、查询 Query、连接 Connect 等 Time 显示这个状态持续的时间，单位是秒 State 显示使用当前连接的 sql 语句的状态，以查询为例，需要经过 copying to tmp table、sorting result、sending data等状态才可以完成 Info 显示执行的 sql 语句，是判断问题语句的一个重要依据 Sending data 状态表示 MySQL 线程开始访问数据行并把结果返回给客户端，而不仅仅只是返回给客户端，是处于执行器过程中的任意阶段。由于在 Sending data 状态下，MySQL 线程需要做大量磁盘读取操作，所以是整个查询中耗时最长的状态。 查询缓存 工作流程 当执行完全相同的 SQL 语句的时候，服务器就会直接从缓存中读取结果，当数据被修改，之前的缓存会失效，修改比较频繁的表不适合做查询缓存。 查询过程： 客户端发送一条查询给服务器 服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果（一般是 K-V 键值对），否则进入下一阶段 分析器进行 SQL 分析，再由优化器生成对应的执行计划 执行器根据优化器生成的执行计划，调用存储引擎的 API 来执行查询 将结果返回给客户端 大多数情况下不建议使用查询缓存，因为查询缓存往往弊大于利 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能费力地把结果存起来，还没使用就被一个更新全清空了，对于更新压力大的数据库来说，查询缓存的命中率会非常低 除非业务就是有一张静态表，很长时间才会更新一次，比如一个系统配置表，那这张表上的查询才适合使用查询缓存 缓存配置 查看当前 MySQL 数据库是否支持查询缓存： 1SHOW VARIABLES LIKE &#x27;have_query_cache&#x27;;\t-- YES 查看当前 MySQL 是否开启了查询缓存： 1SHOW VARIABLES LIKE &#x27;query_cache_type&#x27;;\t-- OFF 参数说明： OFF 或 0：查询缓存功能关闭 ON 或 1：查询缓存功能打开，查询结果符合缓存条件即会缓存，否则不予缓存；可以显式指定 SQL_NO_CACHE 不予缓存 DEMAND 或 2：查询缓存功能按需进行，显式指定 SQL_CACHE 的 SELECT 语句才缓存，其它不予缓存 12SELECT SQL_CACHE id, name FROM customer; -- SQL_CACHE:查询结果可缓存SELECT SQL_NO_CACHE id, name FROM customer;-- SQL_NO_CACHE:不使用查询缓存 查看查询缓存的占用大小： 1SHOW VARIABLES LIKE &#x27;query_cache_size&#x27;;-- 单位是字节 1048576 / 1024 = 1024 = 1KB 查看查询缓存的状态变量： 1SHOW STATUS LIKE &#x27;Qcache%&#x27;; 参数 含义 Qcache_free_blocks 查询缓存中的可用内存块数 Qcache_free_memory 查询缓存的可用内存量 Qcache_hits 查询缓存命中数 Qcache_inserts 添加到查询缓存的查询数 Qcache_lowmen_prunes 由于内存不足而从查询缓存中删除的查询数 Qcache_not_cached 非缓存查询的数量（由于 query_cache_type 设置而无法缓存或未缓存） Qcache_queries_in_cache 查询缓存中注册的查询数 Qcache_total_blocks 查询缓存中的块总数 配置 my.cnf： 1234sudo chmod 666 /etc/mysql/my.cnfvim my.cnf# mysqld中配置缓存query_cache_type=1 重启服务既可生效，执行 SQL 语句进行验证 ，执行一条比较耗时的 SQL 语句，然后再多执行几次，查看后面几次的执行时间；获取通过查看查询缓存的缓存命中数，来判定是否走查询缓存。 缓存失效 查询缓存失效的情况： SQL 语句不一致，要想命中查询缓存，查询的 SQL 语句必须一致，因为缓存中 key 是查询的语句，value 是查询结构 12select count(*) from tb_item;Select count(*) from tb_item;\t-- 不走缓存，首字母不一致 当查询语句中有一些不确定查询时，则不会缓存，比如：now()、current_date()、curdate()、curtime()、rand()、uuid()、user()、database() 123SELECT * FROM tb_item WHERE updatetime &lt; NOW() LIMIT 1;SELECT USER();SELECT DATABASE(); 不使用任何表查询语句： 1SELECT &#x27;A&#x27;; 查询 mysql、information_schema、performance_schema 等系统表时，不走查询缓存： 1SELECT * FROM information_schema.engines; 在跨存储引擎的存储过程、触发器或存储函数的主体内执行的查询，缓存失效 如果表更改，则使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除，包括使用 MERGE 映射到已更改表的表的查询，比如：INSERT、UPDATE、DELETE、ALTER TABLE、DROP TABLE、DROP DATABASE 分析器 没有命中查询缓存，就开始了 SQL 的真正执行，分析器会对 SQL 语句做解析。 1SELECT * FROM t WHERE id = 1; 解析器：处理语法和解析查询，生成一颗对应的解析树。 先做词法分析，输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么代表什么。从输入的 select 这个关键字识别出来这是一个查询语句；把字符串 t 识别成 表名 t，把字符串 id 识别成列 id 然后做语法分析，根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。如果语句不对，就会收到 You have an error in your SQL syntax 的错误提醒 预处理器：进一步检查解析树的合法性，比如数据表和数据列是否存在、别名是否有歧义等。 优化器 成本分析 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 根据搜索条件找出所有可能的使用的索引 成本分析，执行成本由 I/O 成本和 CPU 成本组成，计算全表扫描和使用不同索引执行 SQL 的代价 找到一个最优的执行方案，用最小的代价去执行语句 在数据库里面，扫描行数是影响执行代价的因素之一，扫描的行数越少意味着访问磁盘的次数越少，消耗的 CPU 资源越少，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。 统计数据 MySQL 中保存着两种统计数据： innodb_table_stats 存储了表的统计数据，每一条记录对应着一个表的统计数据 innodb_index_stats 存储了索引的统计数据，每一条记录对应着一个索引的一个统计项的数据 MySQL 在真正执行语句之前，并不能精确地知道满足条件的记录有多少条，只能根据统计信息来估算记录，统计信息就是索引的区分度，一个索引上不同的值的个数（比如性别只能是男女，就是 2 ），称之为基数（cardinality），基数越大说明区分度越好。 通过采样统计来获取基数，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 在 MySQL 中，有两种存储统计数据的方式，可以通过设置参数 innodb_stats_persistent 的值来选择： ON：表示统计信息会持久化存储（默认），采样页数 N 默认为 20，可以通过 innodb_stats_persistent_sample_pages 指定，页数越多统计的数据越准确，但消耗的资源更大 OFF：表示统计信息只存储在内存，采样页数 N 默认为 8，也可以通过系统变量设置（不推荐，每次重新计算浪费资源） 数据表是会持续更新的，两种统计信息的更新方式： 设置 innodb_stats_auto_recalc 为 1，当发生变动的记录数量超过表大小的 10% 时，自动触发重新计算，不过是异步进行 调用 ANALYZE TABLE t 手动更新统计信息，只对信息做重新统计（不是重建表），没有修改数据，这个过程中加了 MDL 读锁并且是同步进行，所以会暂时阻塞系统 EXPLAIN 执行计划在优化器阶段生成，如果 explain 的结果预估的 rows 值跟实际情况差距比较大，可以执行 analyze 命令重新修正信息。 错选索引 采样统计本身是估算数据，或者 SQL 语句中的字段选择有问题时，可能导致 MySQL 没有选择正确的执行索引。 解决方法： 采用 force index 强行选择一个索引 1SELECT * FROM user FORCE INDEX(name) WHERE NAME=&#x27;seazean&#x27;; 可以考虑修改 SQL 语句，引导 MySQL 使用期望的索引 新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引 执行器 开始执行的时候，要先判断一下当前连接对表有没有执行查询的权限，如果没有就会返回没有权限的错误，在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。如果有权限，就打开表继续执行，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 引擎层 Server 层和存储引擎层的交互是以记录为单位的，存储引擎会将单条记录返回给 Server 层做进一步处理，并不是直接返回所有的记录。 工作流程： 首先根据二级索引选择扫描范围，获取第一条符合二级索引条件的记录，进行回表查询，将聚簇索引的记录返回 Server 层，由 Server 判断记录是否符合要求 然后在二级索引上继续扫描下一个符合条件的记录 终止流程 终止语句 终止线程中正在执行的语句： 1KILL QUERY thread_id KILL 不是马上终止的意思，而是告诉执行线程这条语句已经不需要继续执行，可以开始执行停止的逻辑（类似于打断）。因为对表做增删改查操作，会在表上加 MDL 读锁，如果线程被 KILL 时就直接终止，那这个 MDL 读锁就没机会被释放了。 命令 KILL QUERYthread_id_A 的执行流程： 把 session A 的运行状态改成 THD::KILL_QUERY（将变量 killed 赋值为 THD::KILL_QUERY） 给 session A 的执行线程发一个信号，让 session A 来处理这个 THD::KILL_QUERY 状态 会话处于等待状态（锁阻塞），必须满足是一个可以被唤醒的等待，必须有机会去判断线程的状态，如果不满足就会造成 KILL 失败。 典型场景：innodb_thread_concurrency 为 2，代表并发线程上限数设置为 2 session A 执行事务，session B 执行事务，达到线程上限；此时 session C 执行事务会阻塞等待，session D 执行 kill query C 无效 C 的逻辑是每 10 毫秒判断是否可以进入 InnoDB 执行，如果不行就调用 nanosleep 函数进入 sleep 状态，没有去判断线程状态 补充：执行 Ctrl+C 的时候，是 MySQL 客户端另外启动一个连接，然后发送一个 KILL QUERY 命令。 终止连接 断开线程的连接： 1KILL CONNECTION id 断开连接后执行 SHOW PROCESSLIST 命令，如果这条语句的 Command 列显示 Killed，代表线程的状态是 KILL_CONNECTION，说明这个线程有语句正在执行，当前状态是停止语句执行中，终止逻辑耗时较长。 超大事务执行期间被 KILL，这时回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长 大查询回滚，如果查询过程中生成了比较大的临时文件，删除临时文件可能需要等待 IO 资源，导致耗时较长 DDL 命令执行到最后阶段被 KILL，需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久 总结：KILL CONNECTION 本质上只是把客户端的 SQL 连接断开，后面的终止流程还是要走 KILL QUERY。 一个事务被 KILL 之后，持续处于回滚状态，不应该强行重启整个 MySQL 进程，应该等待事务自己执行完成，因为重启后依然继续做回滚操作的逻辑。 常用工具 mysql mysql 不是指 mysql 服务，而是指 mysql 的客户端工具。 1mysql [options] [database] -u --user=name：指定用户名 -p --password[=name]：指定密码 -h --host=name：指定服务器IP或域名 -P --port=#：指定连接端口 -e --execute=name：执行SQL语句并退出，在控制台执行SQL语句，而不用连接到数据库执行 示例： 12mysql -h 127.0.0.1 -P 3306 -u root -pmysql -uroot -p2143 db01 -e &quot;select * from tb_book&quot;; mysqladmin mysqladmin 是一个执行管理操作的客户端程序，用来检查服务器的配置和当前状态、创建并删除数据库等。 通过 mysqladmin --help 指令查看帮助文档 1mysqladmin -uroot -p2143 create &#x27;test01&#x27;; mysqlbinlog 服务器生成的日志文件以二进制格式保存，如果需要检查这些文本，就要使用 mysqlbinlog 日志管理工具。 1mysqlbinlog [options] log-files1 log-files2 ... -d --database=name：指定数据库名称，只列出指定的数据库相关操作 -o --offset=#：忽略掉日志中的前 n 行命令。 -r --result-file=name：将输出的文本格式日志输出到指定文件。 -s --short-form：显示简单格式，省略掉一些信息。 –start-datatime=date1 --stop-datetime=date2：指定日期间隔内的所有日志 –start-position=pos1 --stop-position=pos2：指定位置间隔内的所有日志 mysqldump 命令介绍 mysqldump 客户端工具用来备份数据库或在不同数据库之间进行数据迁移，备份内容包含创建表，及插入表的 SQL 语句。 123mysqldump [options] db_name [tables]mysqldump [options] --database/-B db1 [db2 db3...]mysqldump [options] --all-databases/-A 连接选项： -u --user=name：指定用户名 -p --password[=name]：指定密码 -h --host=name：指定服务器 IP 或域名 -P --port=#：指定连接端口 输出内容选项： –add-drop-database：在每个数据库创建语句前加上 Drop database 语句 –add-drop-table：在每个表创建语句前加上 Drop table 语句 , 默认开启，不开启 (–skip-add-drop-table) -n --no-create-db：不包含数据库的创建语句 -t --no-create-info：不包含数据表的创建语句 -d --no-data：不包含数据 -T, --tab=name：自动生成两个文件：一个 .sql 文件，创建表结构的语句；一个 .txt 文件，数据文件，相当于 select into outfile 示例： 12mysqldump -uroot -p2143 db01 tb_book --add-drop-database --add-drop-table &gt; amysqldump -uroot -p2143 -T /tmp test city 数据备份 命令行方式： 备份命令：mysqldump -u root -p 数据库名称 &gt; 文件保存路径 恢复 登录MySQL数据库：mysql -u root p 删除已经备份的数据库 重新创建与备份数据库名称相同的数据库 使用该数据库 导入文件执行：source 备份文件全路径 图形化界面： 备份 恢复 mysqlimport/source mysqlimport 是客户端数据导入工具，用来导入mysqldump 加 -T 参数后导出的文本文件。 1mysqlimport [options] db_name textfile1 [textfile2...] 示例： 1mysqlimport -uroot -p2143 test /tmp/city.txt 导入 sql 文件，可以使用 MySQL 中的 source 指令 : 1source 文件全路径 mysqlshow mysqlshow 客户端对象查找工具，用来很快地查找存在哪些数据库、数据库中的表、表中的列或者索引等统计信息。 1mysqlshow [options] [db_name [table_name [col_name]]] –count：显示数据库及表的统计信息（数据库，表 均可以不指定） -i：显示指定数据库或者指定表的状态信息 示例： 123456#查询每个数据库的表的数量及表中记录的数量mysqlshow -uroot -p1234 --count#查询test库中每个表中的字段书，及行数mysqlshow -uroot -p1234 test --count#查询test库中book表的详细情况mysqlshow -uroot -p1234 test book --count","tags":["MySQL"],"categories":["学习笔记"]},{"title":"免责声明","path":"/about/index.html","content":"尊敬的读者： 欢迎来到我的个人博客。在创建网站的过程中，为丰富网站内容并提供多元化视角，作者在学习的过程中， 部分学习笔记等文章会整合互联网上各种优质资源，不限于 CSDN、OS China、掘金、Github等。 在此声明，一切文章仅为作者学习时笔记的整合和记录，无恶意搬运。 若有侵权，请立即联系作者删除。 如果您有任何疑问或者建议，欢迎您随时与我联系。邮箱：3086660647@qq.com 谢谢！ JIA"},{"title":"更多","path":"/more/index.html","content":"没有更多"},{"title":"友链","path":"/friends/index.html","content":"还没挂友链"}]